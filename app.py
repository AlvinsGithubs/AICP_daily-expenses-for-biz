# 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# --- ì„¤ì¹˜ ì•ˆë‚´ ---
# 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
#    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
#
# 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
import math  # NaN ì²´í¬ìš©
import streamlit as st
import pandas as pd
import json
import os
import re
import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
import openai
from dotenv import load_dotenv

import io
from datetime import datetime, timedelta
import time
import random
import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from collections import Counter
from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
from typing import Any, Dict, List, Optional, Set, Tuple
from sqlalchemy import text


import altair as alt  # [ì‹ ê·œ 2] ê³ ê¸‰ ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pydeck as pdk  # [ì‹ ê·œ 2] ê³ ê¸‰ 3D ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬

# [ì‹ ê·œ 2(ì§€ë„)] ê´€ë ¨ ì„í¬íŠ¸
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderUnavailable

# --- [v20.0 DBì—°ë™] Streamlit Connection ì´ˆê¸°í™” ---
# Supabase (Postgres) DBì— ì—°ê²°í•©ë‹ˆë‹¤.
# ì´ ì—°ê²°ì€ Streamlitì˜ Secretsì—ì„œ "connections.supabase_db" ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
conn = st.connection("supabase_db", type="sql", dialect="postgresql")


from datetime import date, datetime

def _json_default(obj):
    """report_dataë¥¼ JSONìœ¼ë¡œ ì €ì¥í•  ë•Œ, ë‚ ì§œ/ì‹œê°„ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜."""
    if isinstance(obj, (date, datetime)):
        return obj.isoformat()
    return obj  # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ ë™ì‘ (ì—ëŸ¬ ë‚˜ë©´ ê·¸ë•Œ í™•ì¸)

# --- [v20.0 DBì—°ë™] data_sources.menu_cache.py ê¸°ëŠ¥ ëŒ€ì²´ ---
# ì´ì œ 'menu_cache.py' íŒŒì¼ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
def load_cached_menu_prices(
    city: str, 
    country: str, 
    neighborhood: Optional[str]
) -> List[Dict[str, Any]]:
    """DBì—ì„œ íŠ¹ì • ìœ„ì¹˜ì˜ ë©”ë‰´ ê°€ê²© ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    city_lower = city.lower()
    country_lower = country.lower()
    
    query_base = "SELECT * FROM menu_cache WHERE LOWER(city) = :city AND LOWER(country) = :country"
    params = {"city": city_lower, "country": country_lower}

    if neighborhood:
        # 1ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°
        query_neighborhood = query_base + " AND LOWER(neighborhood) = :neighborhood"
        params_neighborhood = params.copy()
        params_neighborhood["neighborhood"] = neighborhood.lower().strip()
        df_neighborhood = conn.query(query_neighborhood, params=params_neighborhood)
        if not df_neighborhood.empty:
            return df_neighborhood.to_dict('records')

    # 2ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ë¹„ì–´ìˆëŠ” 'ë„ì‹œ ì „ì²´' ë°ì´í„°
    query_city = query_base + " AND (neighborhood IS NULL OR neighborhood = '')"
    df_city = conn.query(query_city, params=params)
    return df_city.to_dict('records')

def load_all_cache() -> List[Dict[str, Any]]:
    """DBì—ì„œ *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    df = conn.query("SELECT * FROM menu_cache ORDER BY last_updated DESC, id DESC", ttl=5) # 5ì´ˆ ìºì‹œ
    return df.to_dict('records')

def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool:
    """ìƒˆë¡œìš´ ìºì‹œ í•­ëª© 1ê°œë¥¼ DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        new_entry["last_updated"] = datetime.now().date()
        df_new = pd.DataFrame([new_entry])
        # conn.insert(...) ëŒ€ì‹  ì§ì ‘ INSERT
        _bulk_insert_dataframe("menu_cache", df_new)
        return True
    except Exception as e:
        st.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool:
    """(ì‚­ì œ ì‹œ ì‚¬ìš©) *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
    try:
        # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
        with conn.session as s:
            s.execute(text("DELETE FROM menu_cache"))
            s.commit()
            
        # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
        if all_samples:
            df_new = pd.DataFrame(all_samples)
            df_new = df_new.drop(columns=["id", "created_at"], errors='ignore')
            _bulk_insert_dataframe("menu_cache", df_new)

        return True
    except Exception as e:
        st.error(f"DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False


MENU_CACHE_ENABLED = True # DBë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•­ìƒ í™œì„±í™”
# --- [v20.0 DBì—°ë™] ë ---

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- [ê³µí†µ] Admin Access Code ì„¤ì • (ì „ì—­) ---
ACCESS_CODE_KEY = "admin_access_code_valid"
ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE")  # .envì—ì„œ ë¡œë“œ

# Maximum number of AI calls per analysis
NUM_AI_CALLS = 10
# --- Weight configuration (sum should remain 1.0) ---
DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
_WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


def weight_config_path() -> str:
    return os.path.join(DATA_DIR, "weight_config.json")



def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
    """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
    try:
        un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
    except (TypeError, ValueError):
        un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
    try:
        ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
    except (TypeError, ValueError):
        ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

    total = un_raw + ai_raw
    if total <= 0:
        return dict(DEFAULT_WEIGHT_CONFIG)

    un_norm = max(0.0, min(1.0, un_raw / total))
    ai_norm = max(0.0, min(1.0, ai_raw / total))

    total_norm = un_norm + ai_norm
    if total_norm == 0:
        return dict(DEFAULT_WEIGHT_CONFIG)
    return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# --- [v20.0 DBì—°ë™] Weight Config í•¨ìˆ˜ (DB ê¸°ë°˜) ---
def _get_config_from_db(config_key: str, default_config: Dict[str, Any]) -> Dict[str, Any]:
    """DBì˜ app_config í…Œì´ë¸”ì—ì„œ ì„¤ì •ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
    try:
        result = conn.query(
            "SELECT config_value FROM app_config WHERE config_key = :key",
            params={"key": config_key},
            ttl=10 # 10ì´ˆ ìºì‹œ
        )
        if result.empty:
            return default_config
        
        db_value = result.iloc[0]["config_value"]
        if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
            return json.loads(db_value)
        return db_value
    except Exception:
        return default_config

def _save_config_to_db(config_key: str, config_value: Dict[str, Any]) -> None:
    """DBì˜ app_config í…Œì´ë¸”ì— ì„¤ì •ì„ ì €ì¥(ì—…ë°ì´íŠ¸)í•©ë‹ˆë‹¤."""
    try:
        # PostgreSQLì˜ JSONB íƒ€ì…ì„ ìœ„í•´ json.dumps ì‚¬ìš©
        config_json = json.dumps(config_value) 
        
        # 'UPSERT' (Update or Insert) ì¿¼ë¦¬
        with conn.session as s:
            s.execute(text(
                f"""
                INSERT INTO app_config (config_key, config_value, last_updated)
                VALUES (:key, :value, :now)
                ON CONFLICT (config_key) 
                DO UPDATE SET config_value = :value, last_updated = :now
                """
            ), params={"key": config_key, "value": config_json, "now": datetime.now()})
            s.commit()
    except Exception as e:
        st.error(f"DB ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")

def get_weight_config() -> Dict[str, float]:
    """DBì—ì„œ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    config = _get_config_from_db("weight_config", DEFAULT_WEIGHT_CONFIG)
    normalized = _normalize_weight_config(config)
    
    # st.session_stateì—ë„ ì €ì¥ (ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜ì„± ìœ ì§€)
    try:
        st.session_state["weight_config"] = normalized
    except Exception:
        pass
    return normalized

def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
    """DBì™€ ì„¸ì…˜ì˜ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    config = {"un_weight": un_weight, "ai_weight": ai_weight}
    normalized = _normalize_weight_config(config)
    _save_config_to_db("weight_config", normalized)
    
    try:
        st.session_state["weight_config"] = normalized
    except Exception:
        pass
    return normalized
# --- [v20.0 DBì—°ë™] ë ---
def load_weight_config() -> Dict[str, float]:
    """
    (Backwards compatibility)
    ì˜ˆì „ íŒŒì¼ ê¸°ë°˜ í•¨ìˆ˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì“°ë˜,
    ì‹¤ì œ êµ¬í˜„ì€ DB ê¸°ë°˜ get_weight_config()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    return get_weight_config()



# ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
    """Return markdown-friendly bullets for cached menu/reference entries."""
    lines_out: List[str] = []
    if not menu_samples:
        return lines_out

    for sample in menu_samples[:max_items]:
        if not isinstance(sample, dict):
            continue

        name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

        url = None
        for key in ("url", "link", "source_url", "href"):
            value = sample.get(key)
            if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
                url = value
                break

        details: List[str] = []
        price = sample.get("price")
        if isinstance(price, (int, float)):
            currency = sample.get("currency") or "USD"
            details.append(f"{currency} {price}")
        elif isinstance(price, str) and price.strip():
            details.append(price.strip())

        category = sample.get("category")
        if category:
            details.append(str(category))

        last_updated = sample.get("last_updated")
        if last_updated:
            details.append(f"updated {last_updated}")

        detail_text = ", ".join(details)
        label = f"[{name}]({url})" if url else name

        if detail_text:
            lines_out.append(f"{label} - {detail_text}")
        else:
            lines_out.append(label)

    return lines_out


_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
    "show_un_basis": True,
    "show_ai_estimate": True,
    "show_weighted_result": True,
    "show_ai_market_detail": True,
    "show_provenance": True,
    "show_menu_samples": True,
}
EMPLOYEE_SECTION_LABELS = [
    ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
    ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
    ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
    ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
    ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
    ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
]
_UI_SETTINGS_CACHE: Dict[str, Any] = {}


CARD_STYLES = {
    "primary": {
        # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
        "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
        "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
        "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
        "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
    },
    "secondary": {
        # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
        "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
        "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
        "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
        "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
    },
    "muted": {
        # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
        "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
        "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
        "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
        "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
    },
}


def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
    style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
    # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
    caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
    card_html = f"""
    <div style="{style['container']}">
        <div style="{style['title']}">{title}</div>
        <div style="{style['value']}">{value}</div>
        {caption_html}
    </div>
    """
    st.markdown(card_html, unsafe_allow_html=True)


def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
    style = CARD_STYLES["primary"]
    card_html = f"""
    <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
        <div style="{style['title']}">Estimated Total Per Diem ({level_label})</div>
        <div style="{style['value']}">$ {total:,}</div>
        <div style="{style['caption']}">
            <span style='font-size:0.95rem;opacity:0.8;'>Calculation</span><br/>
            $ {daily:,} Ã— {days} days Ã— {term_label} (Ã—{multiplier:.2f})
        </div>
    </div>
    """
    st.markdown(card_html, unsafe_allow_html=True)


def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
    normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
    if isinstance(sections, dict):
        for key in normalized:
            normalized[key] = bool(sections.get(key, normalized[key]))
    return normalized

def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
    """Ensure UI settings include expected keys with correct types."""
    normalized = dict(DEFAULT_UI_SETTINGS)
    raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
    normalized["show_employee_tab"] = bool(raw_visibility)
    normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
    return normalized

# --- [v20.0 DBì—°ë™] UI Settings í•¨ìˆ˜ (DB ê¸°ë°˜) ---
def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
    """DBì— UI ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    normalized = _normalize_ui_settings(settings)
    _save_config_to_db("ui_settings", normalized)
    global _UI_SETTINGS_CACHE
    _UI_SETTINGS_CACHE = dict(normalized)
    return normalized

def load_ui_settings(force: bool = False) -> Dict[str, Any]:
    """DBì—ì„œ UI ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global _UI_SETTINGS_CACHE
    if _UI_SETTINGS_CACHE and not force:
        return dict(_UI_SETTINGS_CACHE)
    
    config = _get_config_from_db("ui_settings", DEFAULT_UI_SETTINGS)
    normalized = _normalize_ui_settings(config)
    
    _UI_SETTINGS_CACHE = dict(normalized)
    return dict(normalized)
# --- [v20.0 DBì—°ë™] ë ---

JOB_LEVEL_RATIOS = {
    "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6": 1.00,
    "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
}

TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
LONG_TERM_THRESHOLD_DAYS = 30
SHORT_TERM_MULTIPLIER = 1.0
LONG_TERM_MULTIPLIER = 1.05
TRIP_TERM_LABELS = {"Short-term": "Short-term", "Long-term": "Long-term"}


def classify_trip_duration(days: int) -> Tuple[str, float]:
    """Return trip term classification and multiplier based on duration in days."""
    if days >= LONG_TERM_THRESHOLD_DAYS:
        return "Long-term", LONG_TERM_MULTIPLIER
    return "Short-term", SHORT_TERM_MULTIPLIER

DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
    {"region": "North America", "city": "Nassau", "country": "Bahamas"},
    {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
    {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
    {"region": "North America", "city": "Seattle", "country": "USA"},
    {"region": "North America", "city": "Florida", "country": "USA"},
    {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
    {"region": "North America", "city": "Toronto", "country": "Canada"},
    {"region": "Europe", "city": "Valletta", "country": "Malta"},
    {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
    {"region": "Europe", "city": "Dublin", "country": "Ireland"},
    {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
    {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
    {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
    {"region": "Europe", "city": "San Remo", "country": "Italy"},
    {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
    {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
    {"region": "Europe", "city": "Paris", "country": "France"},
    {"region": "Europe", "city": "Provence", "country": "France"},
    {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
    {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
    {"region": "Asia", "city": "Manila", "country": "Philippines"},
    {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
    {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
    {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
    {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
    {"region": "Others", "city": "Sydney", "country": "Australia"},
    {"region": "Others", "city": "Rosario", "country": "Argentina"},
    {"region": "Others", "city": "Marrakech", "country": "Morocco"},
    {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
]


def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
    """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
    entry = dict(entry)
    entry.setdefault("region", "Others")
    entry.setdefault("neighborhood", "")
    entry.setdefault("hotel_cluster", "")
    entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
    return entry


# --- [v20.0 DBì—°ë™] Target Cities í•¨ìˆ˜ (DB ê¸°ë°˜) ---
def load_target_city_entries() -> List[Dict[str, Any]]:
    """DBì—ì„œ ëª¨ë“  ë„ì‹œ ëª©ë¡ì„ ë¡œë“œí•˜ë˜, ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±."""
    df = conn.query("SELECT * FROM target_cities ORDER BY region, country, city", ttl=10)
    if df.empty:
        # DB ë¹„ì–´ ìˆìœ¼ë©´ ì½”ë“œì— ë°•í˜€ ìˆëŠ” DEFAULT_TARGET_CITY_ENTRIES ì‚¬ìš©
        return [normalize_target_entry(e) for e in DEFAULT_TARGET_CITY_ENTRIES]
    return df.to_dict('records')

def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
    """(ìˆ˜ì •/ì‚­ì œ ì‹œ ì‚¬ìš©) ì „ì²´ ë„ì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
    try:
        # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
        with conn.session as s:
            s.execute(text("DELETE FROM target_cities"))
            s.commit()
            
        # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
        if entries:
            df_new = pd.DataFrame(entries)
            df_new = df_new.drop(columns=["id", "created_at"], errors='ignore')

            # ---------- âœ¨ ì—¬ê¸°ë¶€í„° ì¶”ê°€/ìˆ˜ì • ë¶€ë¶„ âœ¨ ----------

            # NaN â†’ None ìœ¼ë¡œ í†µì¼ (íŠ¹íˆ un_dsa_substitute ì»¬ëŸ¼ì˜ NaN ë°©ì§€)
            df_new = df_new.where(pd.notnull(df_new), None)

            # un_dsa_substitute ì»¬ëŸ¼: dict/list â†’ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            if "un_dsa_substitute" in df_new.columns:
                def _normalize_sub(v):
                    # dict/list ëŠ” json.dumps
                    if isinstance(v, (dict, list)):
                        return json.dumps(v)
                    return v  # None ì´ë‚˜ ê¸°ì¡´ ë¬¸ìì—´ì€ ê·¸ëŒ€ë¡œ
                df_new["un_dsa_substitute"] = df_new["un_dsa_substitute"].apply(_normalize_sub)

            # ---------- âœ¨ ì¶”ê°€/ìˆ˜ì • ë âœ¨ ----------

            _bulk_insert_dataframe("target_cities", df_new)
    except Exception as e:
        st.error(f"DB ë„ì‹œ ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")


def _bulk_insert_dataframe(table_name: str, df: pd.DataFrame) -> None:
    """
    Streamlit SQLConnection ì—ëŠ” insert() ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ,
    session + raw SQL ë¡œ DataFrame ì„ ì¼ê´„ insert í•œë‹¤.
    NaN ê°’ì€ ëª¨ë‘ None(NULL)ìœ¼ë¡œ ë³€í™˜í•´ì„œ íƒ€ì… ì˜¤ë¥˜ë¥¼ ë°©ì§€í•œë‹¤.
    """
    if df.empty:
        return

    cols = list(df.columns)
    col_list = ", ".join(cols)
    placeholders = ", ".join([f":{c}" for c in cols])
    insert_sql = text(
        f"INSERT INTO {table_name} ({col_list}) VALUES ({placeholders})"
    )

    with conn.session as s:
        for _, row in df.iterrows():
            raw_dict = row.to_dict()
            clean_dict = {}

            for k, v in raw_dict.items():
                # pandas ê°€ ë„£ì–´ì¤€ NaN(float) â†’ None ìœ¼ë¡œ ë³€í™˜
                if isinstance(v, float) and math.isnan(v):
                    clean_dict[k] = None
                else:
                    clean_dict[k] = v

            s.execute(insert_sql, params=clean_dict)
        s.commit()




TARGET_CITIES_ENTRIES = load_target_city_entries() # ì•± ì‹œì‘ ì‹œ DBì—ì„œ ë¡œë“œ

def get_target_city_entries() -> List[Dict[str, Any]]:
    if "target_cities_entries" in st.session_state:
        return st.session_state["target_cities_entries"]
    # DBì—ì„œ ë¡œë“œí•œ ìµœì‹ ë³¸ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    st.session_state["target_cities_entries"] = load_target_city_entries()
    return st.session_state["target_cities_entries"]

def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
    # 1. DBì— ì˜êµ¬ ì €ì¥
    save_target_city_entries(entries) 
    # 2. í˜„ì¬ ì„¸ì…˜ ìƒíƒœì—ë„ ì¦‰ì‹œ ë°˜ì˜
    st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
# --- [v20.0 DBì—°ë™] ë ---

def auto_fill_all_city_coordinates() -> tuple[int, int]:
    """
    target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
    DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
    í•œêµ­(Seoul/Busan/Incheon/Jeju Island) ë“±ì€ ë³„ì¹­ ë° ìˆ˜ë™ ì¢Œí‘œë¡œ ë³´ì™„.
    """
    try:
        geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
    except Exception as e:
        st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return 0, 0

    current_entries = get_target_city_entries()
    entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

    if not entries_to_update:
        return 0, 0

    success_count = 0
    fail_count = 0

    progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

    with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
        for i, entry in enumerate(entries_to_update):
            city = entry["city"]
            country = entry["country"]

            # 1) country ë³„ì¹­ ì ìš© (ì˜ˆ: "Korea, Republic of" â†’ "South Korea")
            country_for_query = GEOCODING_COUNTRY_ALIASES.get(country, country)

            # 2) (city, country) ì¡°í•©ë³„ ì¿¼ë¦¬ override
            key = (city.lower(), country_for_query.lower())
            if key in GEOCODING_CITY_QUERY_OVERRIDES:
                query = GEOCODING_CITY_QUERY_OVERRIDES[key]
            else:
                query = f"{city}, {country_for_query}"

            try:
                location = geolocator.geocode(query, timeout=5)
                time.sleep(1)  # Nominatim rate limit

                if location:
                    entry["lat"] = float(location.latitude)
                    entry["lon"] = float(location.longitude)
                    st.toast(
                        f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})",
                        icon="ğŸŒ",
                    )
                    success_count += 1
                else:
                    # 3) Nominatim ì—ì„œ ëª» ì°¾ìœ¼ë©´ ìˆ˜ë™ ì¢Œí‘œ fallback ì‹œë„
                    manual = MANUAL_COORDS.get((city, country))
                    if manual:
                        lat, lon = manual
                        entry["lat"] = float(lat)
                        entry["lon"] = float(lon)
                        st.toast(
                            f"âœ… ìˆ˜ë™ ì¢Œí‘œ ì‚¬ìš©: {city}, {country} ({lat:.4f}, {lon:.4f})",
                            icon="ğŸ“Œ",
                        )
                        success_count += 1
                    else:
                        st.toast(
                            f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                            icon="â“",
                        )
                        fail_count += 1

            except (GeocoderTimedOut, GeocoderUnavailable):
                st.toast(
                    f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.",
                    icon="ğŸ”¥",
                )
                fail_count += 1
            except Exception as e:
                st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
                fail_count += 1

            progress_bar.progress(
                (i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}"
            )

    # DB + ì„¸ì…˜ì— ì €ì¥
    set_target_city_entries(current_entries)
    progress_bar.empty()
    return success_count, fail_count




def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
    entries = entries or get_target_city_entries()
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for entry in entries:
        grouped.setdefault(entry.get("region", "Others"), []).append(entry)
    return grouped


def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
    entries = entries or get_target_city_entries()
    return [normalize_target_entry(entry) for entry in entries]


# --- Geocoding ì „ìš© ë³„ì¹­ / ìˆ˜ë™ ì¢Œí‘œ ---

GEOCODING_COUNTRY_ALIASES = {
    # UN í‘œê¸° â†’ ì‹¤ì œ ì§€ì˜¤ì½”ë”©ìš© í‘œê¸°
    "Korea, Republic of": "South Korea",
}

# íŠ¹ì • (city, country) ì¡°í•©ì— ëŒ€í•´, Nominatimì— ë˜ì§ˆ ì¿¼ë¦¬ë¥¼ ê°•ì œë¡œ ì§€ì •
GEOCODING_CITY_QUERY_OVERRIDES = {
    ("seoul", "south korea"): "Seoul, South Korea",
    ("busan", "south korea"): "Busan, South Korea",
    ("incheon", "south korea"): "Incheon, South Korea",
    ("jeju island", "south korea"): "Jeju-do, South Korea",  # ì œì£¼
}

# ê·¸ë˜ë„ ëª» ì°¾ì•˜ì„ ë•Œ ì‚¬ìš©í•  ìˆ˜ë™ ì¢Œí‘œ (ë§ˆì§€ë§‰ ì•ˆì „ë§)
MANUAL_COORDS = {
    ("Seoul", "Korea, Republic of"): (37.5665, 126.9780),
    ("Busan", "Korea, Republic of"): (35.1796, 129.0756),
    ("Incheon", "Korea, Republic of"): (37.4563, 126.7052),
    ("Jeju Island", "Korea, Republic of"): (33.4996, 126.5312),
}


# ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
CITY_ALIASES = {
    "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
    "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
    "kuala lumpur": "kuala lumpur"
}

# --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

SEASON_BANDS = [
    {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
    {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
    {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
    {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
]

CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
    ("las vegas", "usa"): [
        {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
        {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
    ],
    ("seoul", "korea, republic of"): [
        {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
        {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
    ],
    ("barcelona", "spain"): [
        {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
    ],
}


def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
    key = (city.lower(), country.lower())
    for entry in get_target_city_entries():
        if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
            return {
                "neighborhood": entry.get("neighborhood"),
                "hotel_cluster": entry.get("hotel_cluster"),
            }
    return {"neighborhood": None, "hotel_cluster": None}


def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
    """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
    month = datetime.now().month
    city_key = (city.lower(), country.lower())
    overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
    for override in overrides:
        if month in override["months"]:
            return {
                "label": override["label"],
                "factor": override["factor"],
                "source": "city_override",
            }

    for band in SEASON_BANDS:
        if month in band["months"]:
            return {
                "label": band["label"],
                "factor": band["factor"],
                "source": "global_profile",
            }

    return {"label": "Standard", "factor": 1.0, "source": "default"}


# --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
    """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
    if not totals:
        return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

    sorted_totals = sorted(totals)
    if len(sorted_totals) >= 4:
        try:
            q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
        except (ValueError, StatisticsError):  # type: ignore[name-defined]
            filtered = sorted_totals
    else:
        filtered = sorted_totals

    if not filtered:
        filtered = sorted_totals

    removed_values: List[int] = []
    filtered_counter = Counter(filtered)
    for value in sorted_totals:
        if filtered_counter[value]:
            filtered_counter[value] -= 1
        else:
            removed_values.append(value)

    computed_mean = mean(filtered) if filtered else None
    
    # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
    variation_coeff = None
    if filtered and computed_mean and computed_mean > 0:
        if len(filtered) > 1:
            try:
                computed_stdev = stdev(filtered)
                variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
            except StatisticsError:
                variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
        else:
            variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

    return {
        "used_values": filtered,
        "removed_values": removed_values,
        "mean_raw": computed_mean,
        "mean": round(computed_mean) if computed_mean is not None else None,
        "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
    }

# --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
def get_dynamic_weights(
    variation_coeff: Optional[float], 
    admin_weights: Dict[str, float]
) -> Dict[str, Any]:
    """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
    # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
    base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
    if variation_coeff is None:
        # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
        return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
    if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
        # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
        dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
        source = f"High AI Consistency (VC: {variation_coeff:.2f})"
    elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
        # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
        dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
        source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
    else:
        # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
        dynamic_ai_weight = base_ai_weight
        source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

    final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
    final_un_weight = 1.0 - final_ai_weight
    
    return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

def parse_pdf_to_text(uploaded_file):
    uploaded_file.seek(0)
    doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
    full_text = ""
    for page_num in range(4, len(doc)):
        full_text += doc[page_num].get_text("text") + "\n\n"
    return full_text

# --- [v20.0 DBì—°ë™] Report History í•¨ìˆ˜ (DB ê¸°ë°˜) ---
def get_history_files() -> List[str]:
    """DBì—ì„œ ê³¼ê±° ë³´ê³ ì„œ 'ì´ë¦„' ëª©ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    df = conn.query("SELECT name FROM analysis_reports ORDER BY created_at DESC", ttl=10) # 10ì´ˆ ìºì‹œ
    return df['name'].tolist()

from sqlalchemy import text

def save_report_data(data):
    """ë¶„ì„ ê²°ê³¼ë¥¼ DB(JSONB)ì— ì €ì¥í•©ë‹ˆë‹¤."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"report_{timestamp}.json"

    try:
        report_json = json.dumps(data)

        with conn.session as s:
            s.execute(
                text("""
                    INSERT INTO analysis_reports (name, report_data)
                    VALUES (:name, :report_data)
                """),
                {"name": filename, "report_data": report_json},
            )
            s.commit()

    except Exception as e:
        st.error(f"DB ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")



def load_report_data(filename):
    """DBì—ì„œ íŠ¹ì • ë³´ê³ ì„œ(JSON)ë¥¼ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        result = conn.query(
            "SELECT report_data FROM analysis_reports WHERE name = :name",
            params={"name": filename},
            ttl=3600 # 1ì‹œê°„ ìºì‹œ
        )
        if result.empty:
            return None
        
        db_value = result.iloc[0]["report_data"]
        
        if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
            data = json.loads(db_value)
        else: # ì´ë¯¸ dict/listë¡œ ë°˜í™˜ëœ ê²½ìš°
            data = db_value
            
        return _sanitize_report_data(data)
    except Exception as e:
        st.error(f"DB ë³´ê³ ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
# --- [v20.0 DBì—°ë™] ë ---


def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(data, dict):
        return data
    cities = data.get("cities")
    if isinstance(cities, list):
        for city in cities:
            if isinstance(city, dict):
                city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
    return data




def build_tsv_conversion_prompt():
    return """
[Task]
Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
[Guidelines]
1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
[Output Format]
Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
Country	Area	First 60 Days US$	Room as % of DSA
USA (US Dollar)	Washington D.C.	403	57
"""


def call_openai_for_tsv_conversion(pdf_chunk, api_key):
    client = openai.OpenAI(api_key=api_key)
    system_prompt = build_tsv_conversion_prompt()
    user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
    try:
        response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
        tsv_content = response.choices[0].message.content
        if "```" in tsv_content:
            tsv_content = tsv_content.split('```')[1].strip()
            if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
        return tsv_content
    except Exception as e:
        st.error(f"OpenAI API request failed: {e}")
        return None

def process_tsv_data(tsv_content):
    try:
        df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
        df['Country'] = df['Country'].ffill()
        df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
        df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
        df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
        df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
        df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
        df = df.astype({'TotalDSA': int, 'RoomPct': int})
    except Exception as e:
        st.error(f"TSV processing error: {e}")
        return None

    all_target_cities = get_all_target_cities()
    final_cities_data = []
    for target in all_target_cities:
        city_data = {
            "city": target["city"],
            "country_display": target["country"],
            "notes": "",
            "neighborhood": target.get("neighborhood"),
            "hotel_cluster": target.get("hotel_cluster"),
            "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
        }
        found_row = None

        # --- [ìˆ˜ì •] un_dsa_substitute ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ ---
        sub_raw = target.get("un_dsa_substitute")

        sub_value = None
        if isinstance(sub_raw, dict):
            sub_value = sub_raw
        elif isinstance(sub_raw, str) and sub_raw.strip():
            # JSON ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ëŒ€ë¹„
            try:
                sub_value = json.loads(sub_raw)
            except Exception:
                sub_value = None

        if (
            isinstance(sub_value, dict)
            and sub_value.get("city")
            and sub_value.get("country")
        ):
            # ìœ íš¨í•œ ëŒ€ì²´ ë„ì‹œ ì •ë³´ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©
            search_target = sub_value
            is_substitute = True
        else:
            # ì—†ìœ¼ë©´ ì›ë˜ ë„ì‹œ ì‚¬ìš©
            search_target = target
            is_substitute = False
        # --- [ìˆ˜ì • ë] ---

        country_df = df[
            df["Country"].str.contains(search_target["country"], case=False, na=False)
        ]
        if not country_df.empty:
            target_city_lower = search_target["city"].lower()
            target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
            exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
            non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
            if not non_special_rate.empty:
                found_row = non_special_rate.iloc[0]
                city_data["notes"] = "Exact city match"
            elif not exact_match.empty:
                found_row = exact_match.iloc[0]
                city_data["notes"] = "Exact city match (special rate possible)"
            if found_row is None:
                elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
                if not elsewhere_match.empty:
                    found_row = elsewhere_match.iloc[0]
                    city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
        if is_substitute and found_row is not None:
            city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
        if found_row is not None:
            total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
            if 0 < total_dsa and 0 <= room_pct <= 100:
                per_diem = round(total_dsa * (1 - room_pct / 100))
                city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
            else: city_data["un"] = {"status": "not_found"}
        else:
            city_data["un"] = {"status": "not_found"}
            if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
        city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
        final_cities_data.append(city_data)
    return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
async def get_market_data_from_ai_async(
    client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
    city: str,
    country: str,
    source_name: str = "",
    context: Optional[Dict[str, Optional[str]]] = None,
    season_context: Optional[Dict[str, Any]] = None,
    menu_samples: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
    context = context or {}
    season_context = season_context or {}
    menu_samples = menu_samples or []

    request_id = random.randint(10000, 99999)
    called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

    # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
    def _build_location_block() -> str:
        lines: List[str] = []
        if context.get("neighborhood"):
            lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
        if context.get("hotel_cluster"):
            lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
        return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

    def _build_menu_block() -> str:
        if not menu_samples:
            return "- No direct venue menu data available; use standard mid-range venues."
        snippets = []
        for sample in menu_samples[:5]:
            vendor = sample.get("vendor") or sample.get("name") or "Venue"
            category = sample.get("category") or "General"
            price = sample.get("price")
            currency = sample.get("currency", "USD")
            last_updated = sample.get("last_updated")
            if price is None:
                continue
            tail = f" (last updated {last_updated})" if last_updated else ""
            snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
        if not snippets:
            return "- No direct venue menu data available; use standard mid-range venues."
        return "Menu price signals:\n" + "\n".join(snippets)

    location_block = _build_location_block()
    menu_block = _build_menu_block()
    season_label = season_context.get("label", "Standard")
    season_factor = season_context.get("factor", 1.0)
    season_source = season_context.get("source", "global_profile")
    # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
    prompt = f"""
You are a corporate travel cost analyst. Request ID: {request_id}.
Location context:
{location_block}
Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
{menu_block}

For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

{{
  "food": {{
    "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
    "value": <integer>
  }},
  "transport": {{
    "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
    "value": <integer>
  }},
  "misc": {{
    "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
    "value": <integer>
  }}
}}
"""

    try:
        # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.4,
        )
        # --- [ìˆ˜ì •] ë ---
        
        raw_content = response.choices[0].message.content
        data = json.loads(raw_content)

        food = data.get("food", {}).get("value")
        transport = data.get("transport", {}).get("value")
        misc = data.get("misc", {}).get("value")

        food_val = food if isinstance(food, int) else 0
        transport_val = transport if isinstance(transport, int) else 0
        misc_val = misc if isinstance(misc, int) else 0

        meta = {
            "source_name": source_name,
            "request_id": request_id,
            "prompt": prompt.strip(),
            "response_raw": raw_content,
            "called_at": called_at,
            "season_context": season_context,
            "location_context": context,
            "menu_samples_used": menu_samples[:5],
        }

        if food_val == 0 and transport_val == 0 and misc_val == 0:
            return {
                "status": "na",
                "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "meta": meta,
            }

        total = food_val + transport_val + misc_val
        notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
        return {
            "food": food_val,
            "transport": transport_val,
            "misc": misc_val,
            "total": total,
            "status": "ok",
            "notes": notes,
            "meta": meta,
        }

    except Exception as e:
        return {
            "status": "na",
            "notes": f"{source_name} AI data extraction failed: {e}",
            "meta": {
                "source_name": source_name,
                "request_id": request_id,
                "prompt": prompt.strip(),
                "called_at": called_at,
                "season_context": season_context,
                "location_context": context,
                "menu_samples_used": menu_samples[:5],
                "error": str(e),
            },
        }
# --- [ê°œì„  1] ë ---

def generate_markdown_report(report_data):
    md = f"# Business Travel Daily Allowance Report\n\n"
    md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
    weights_cfg = load_weight_config()
    md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

    valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
    if valid_allowances:
        md += "## 1. Summary\n\n"
        md += (
            f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
            f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
        )

    md += "## 2. City Details\n\n"
    table_data = []
    all_reference_links: Set[str] = set()
    all_target_cities = get_all_target_cities()
    report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
    for target in all_target_cities:
        city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
        if city_data:
            un_data = city_data.get('un', {})
            ai_summary = city_data.get('ai_summary', {})
            season_context = city_data.get('season_context', {})

            un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
            final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
            delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
            ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
            ai_runs_used = ai_summary.get('successful_runs', 0)
            ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
            removed_totals = ai_summary.get('removed_totals') or []
            reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
            # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
            weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

            for link in reference_links:
                if isinstance(link, str) and link.strip():
                    all_reference_links.add(link.strip())

            row = {
                'City': city_data.get('city', 'N/A'),
                'Country': city_data.get('country_display', 'N/A'),
                'UN-DSA (1 day)': un_val,
                'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
                'AI runs used': f"{ai_runs_used}/{ai_attempts}",
                'Season label': season_context.get('label', 'Standard'),
                'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
                'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
            }
            for j in range(1, NUM_AI_CALLS + 1):
                market_data = city_data.get(f"market_data_{j}", {})
                md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
                row[f"AI run {j}"] = md_val

            row.update({
                'Final allowance': final_val,
                'Delta vs UN (%)': delta,
                'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
                'Notes': city_data.get('notes', ''),
            })
            table_data.append(row)

    df = pd.DataFrame(table_data)
    md += df.to_markdown(index=False)
    md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

    md += (
        "---\n"
        "## 3. Methodology\n\n"
        "1. **Baseline (UN-DSA)**\n"
        "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
        "   - Normalize the data as TSV to align city/country names.\n\n"
        "2. **Market data (AI)**\n"
        "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
        "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
        "3. **Post-processing**\n"
        "   - Remove outliers via the IQR rule and compute averages.\n"
        "   - Apply season factors and blend with UN-DSA using configured weights.\n"
        "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
        "   - Multiply by grade ratios to produce per-level allowances.\n\n"
        "---\n"
        "## 4. Sources\n\n"
        "- UN-DSA Circular (International Civil Service Commission)\n"
        "- Mercer Cost of Living (2025 edition)\n"
        "- Numbeo Cost of Living Index (2025 snapshot)\n"
        "- Expatistan Cost of Living Guide\n"
    )

    return md




# --- Streamlit UI Configuration ---
st.set_page_config(
    layout="wide",
    initial_sidebar_state="collapsed"  # ğŸ‘ˆ ì‚¬ì´ë“œë°” ê¸°ë³¸ ì ‘í˜
)
st.title("NSUS GROUP Per Diem Calculation & Inquiry System")

if 'latest_analysis_result' not in st.session_state:
    st.session_state.latest_analysis_result = None
if 'target_cities_entries' not in st.session_state:
    st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
if 'weight_config' not in st.session_state:
    st.session_state.weight_config = load_weight_config()
else:
    st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

ui_settings = load_ui_settings()
stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
if "employee_tab_visibility" not in st.session_state:
    st.session_state.employee_tab_visibility = stored_employee_tab_visible
employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
if "employee_sections_visibility" not in st.session_state:
    st.session_state.employee_sections_visibility = section_visibility_default
else:
    st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
employee_sections_visibility = st.session_state.employee_sections_visibility

# --- [NEW] Global Admin Access + Tab Layout (v21.1) ---

# .env ì— ADMIN_ACCESS_CODE ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì•± ì¤‘ë‹¨
if not ACCESS_CODE_VALUE:
    st.error(
        "Security Error: 'ADMIN_ACCESS_CODE' is not set in the .env file. "
        "Please stop the app and set the .env file."
    )
    st.stop()

# í˜„ì¬ Admin ë¡œê·¸ì¸ ìƒíƒœ
is_admin = bool(st.session_state.get(ACCESS_CODE_KEY, False))

# 0) ì¢Œì¸¡ ì‚¬ì´ë“œë°”: ë¡œê·¸ì¸ / ë¡œê·¸ì•„ì›ƒ UI (ê¸°ë³¸ ë‹«í˜)
with st.sidebar.expander("ğŸ” Admin Access", expanded=False):
    if not is_admin:
        code_input = st.text_input(
            "Access Code",
            type="password",
            placeholder="Enter admin code",
            key="sidebar_admin_code",
        )
        if st.button("Login", key="sidebar_admin_login_btn"):
            if code_input == ACCESS_CODE_VALUE:
                st.session_state[ACCESS_CODE_KEY] = True
                st.success("Access granted. Admin tabs unlocked.")
                st.rerun()
            else:
                st.error("The Access Code is incorrect.")
    else:
        st.markdown("âœ… **Admin mode is active.**")
        if st.button("Logout", key="sidebar_admin_logout_btn"):
            # ê´€ë¦¬ì ì„¸ì…˜ í•´ì œ
            st.session_state[ACCESS_CODE_KEY] = False
            # ë¡œê·¸ì•„ì›ƒ ì‹œ ì§ì›ìš© íƒ­ì€ í•­ìƒ ë³´ì´ë„ë¡(Per Diem Inquiryë§Œ ë‚¨ë„ë¡)
            st.session_state["employee_tab_visibility"] = True
            st.success("You have been logged out.")
            st.rerun()

# 1) ìƒë‹¨ íƒ­ ë ˆì´ì•„ì›ƒ
top_left, _ = st.columns([6, 2])

# 2) ë©”ì¸ ì˜ì—­: íƒ­ ë ˆì´ì•„ì›ƒ
#   - employee_tab_visible, is_admin ì€ ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
tab_definitions = []

# (1) ì§ì›ìš© íƒ­ â€“ ê¸°ë³¸ ì²« íƒ­
if employee_tab_visible:
    tab_definitions.append("ğŸ’µ Per Diem Inquiry (Employee)")

# (2) Admin íƒ­ â€“ ë¡œê·¸ì¸ ìƒíƒœì—ì„œë§Œ í‘œì‹œ
if is_admin:
    tab_definitions.append("ğŸ“ˆ Report Analysis (Admin)")
    tab_definitions.append("ğŸ› ï¸ System Settings (Admin)")
    tab_definitions.append("ğŸ“Š Executive Dashboard (Admin)")

# íƒ­ì´ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
if not tab_definitions:
    tab_definitions.append("ğŸ”’ Admin Login")

tabs = st.tabs(tab_definitions)

# íƒ­ ì¸ë±ìŠ¤ ë§¤í•‘
employee_tab = admin_analysis_tab = admin_config_tab = dashboard_tab = None
idx = 0

if employee_tab_visible:
    employee_tab = tabs[idx]
    idx += 1

if is_admin:
    admin_analysis_tab = tabs[idx]; idx += 1
    admin_config_tab   = tabs[idx]; idx += 1
    dashboard_tab      = tabs[idx]; idx += 1

# ì´ ì•„ë˜ë¶€í„°ëŠ” employee_tab / admin_analysis_tab / admin_config_tab / dashboard_tab ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©


# ì´ ì•„ë˜ë¶€í„°ëŠ” employee_tab / admin_analysis_tab / admin_config_tab / dashboard_tab ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©


# --- [End of modification] ---
if dashboard_tab is not None:
    with dashboard_tab:
        st.header("Global Cost Dashboard")
        st.info("Visualizes the global business trip cost status based on the latest report data.")

        try:
            alt.theme.enable("streamlit")
        except Exception:
            pass 

        history_files = get_history_files()
        if not history_files:
            st.warning("No data to display. Please run AI analysis at least once in the 'Report Analysis' tab.")
        else:
            latest_report_file = history_files[0]
            st.subheader(f"Reference Report: `{latest_report_file}`")
          
            report_data = load_report_data(latest_report_file)
            if not report_data or 'cities' not in report_data:
                st.error("Failed to load data.")
            else:
                # --- ì¢Œí‘œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼ (System Settings ì™€ ë™ì¼ ê¸°ëŠ¥) ---
                if st.button("ì¢Œí‘œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°", key="reload_coords_from_dashboard"):
                    success_count, fail_count = auto_fill_all_city_coordinates()
                    if success_count == 0 and fail_count == 0:
                        st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
                    else:
                        st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")

                # í•­ìƒ ìµœì‹  target_cities ë¥¼ ë‹¤ì‹œ ë¡œë“œ
                config_entries = get_target_city_entries()
                if not config_entries:
                    st.error("Failed to load target cities.")
                else:
                    # 1. Report + Target Cities ë¨¸ì§€
                    df_report = pd.DataFrame(report_data['cities'])
                    df_config = pd.DataFrame(config_entries)

                    df_merged = pd.merge(
                        df_report,
                        df_config,
                        left_on=["city", "country_display"],
                        right_on=["city", "country"],
                        suffixes=("_report", "_config")
                    )

                    # 2. ì§€ë„ìš© ë°ì´í„°: lat/lon ì»¬ëŸ¼ ìë™ íƒì§€ (report / config ë‘˜ ë‹¤ ëŒ€ì‘)
                    lat_col = None
                    lon_col = None

                    if "lat_config" in df_merged.columns and "lon_config" in df_merged.columns:
                        # target_cities ìª½ ì¢Œí‘œë¥¼ ìš°ì„  ì‚¬ìš©
                        lat_col, lon_col = "lat_config", "lon_config"
                    elif "lat_report" in df_merged.columns and "lon_report" in df_merged.columns:
                        # ì˜ˆì „ ë¦¬í¬íŠ¸(JSON)ë§Œ ì¢Œí‘œë¥¼ ê°€ì§„ ê²½ìš°
                        lat_col, lon_col = "lat_report", "lon_report"
                    elif "lat" in df_merged.columns and "lon" in df_merged.columns:
                        # ì¤‘ë³µì´ ì—†ì„ ë•Œ (ë‘˜ ì¤‘ í•œìª½ë§Œ lat/lon ë³´ìœ )
                        lat_col, lon_col = "lat", "lon"

                    # final_allowance ì™€ lat/lon ì´ ëª¨ë‘ ìˆì–´ì•¼ ì§€ë„ í‘œì‹œ ê°€ëŠ¥
                    if lat_col is None or lon_col is None or "final_allowance" not in df_merged.columns:
                        map_data = pd.DataFrame(columns=["city", "country", "lat", "lon", "final_allowance"])
                    else:
                        map_data = df_merged.rename(columns={lat_col: "lat", lon_col: "lon"})[
                            ["city", "country", "lat", "lon", "final_allowance"]
                        ].copy()

                        map_data["lat"] = pd.to_numeric(map_data["lat"], errors="coerce")
                        map_data["lon"] = pd.to_numeric(map_data["lon"], errors="coerce")
                        map_data["final_allowance"] = pd.to_numeric(map_data["final_allowance"], errors="coerce")
                        map_data.dropna(subset=["lat", "lon", "final_allowance"], inplace=True)


                    if map_data.empty:
                        st.warning(
                            "ì´ ë¦¬í¬íŠ¸ì— ë§¤ì¹­ë˜ëŠ” ë„ì‹œ ì¢Œí‘œ(lat/lon)ê°€ ì—†ìŠµë‹ˆë‹¤. "
                            "ìœ„ì˜ 'ì¢Œí‘œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ target_cities ì¢Œí‘œë¥¼ ìë™ ì™„ì„±í•œ ë’¤, "
                            "ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•´ì£¼ì„¸ìš”."
                        )
                        st.caption("No data to display on the map. (Check if coordinates were generated.)")
                    else:
                        # 3. Pydeck ì§€ë„ ë Œë”ë§
                        min_cost = map_data['final_allowance'].min()
                        max_cost = map_data['final_allowance'].max()
                        range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

                        def get_color_and_size(cost):
                            norm_cost = (cost - min_cost) / range_cost
                            r = int(255 * norm_cost)
                            g = int(255 * (1 - norm_cost))
                            b = 0
                            size = 50000 + (norm_cost * 450000)
                            return [r, g, b, 160], size

                        color_size = map_data['final_allowance'].apply(get_color_and_size)
                        map_data['color'] = [item[0] for item in color_size]
                        map_data['size'] = [item[1] for item in color_size]

                        view_state = pdk.ViewState(
                            latitude=map_data['lat'].mean(),
                            longitude=map_data['lon'].mean(),
                            zoom=0.5,
                            pitch=0,
                            bearing=0
                        )

                        layer = pdk.Layer(
                            'ScatterplotLayer',
                            data=map_data,
                            get_position='[lon, lat]',
                            get_color='color',
                            get_radius='size',
                            pickable=True,
                            opacity=0.8,
                            stroked=True,
                            filled=True,
                            radius_scale=0.5,
                            get_line_color=[255, 255, 255, 100],
                            get_line_width=10000,
                        )

                        tooltip = {
                            "html": "<b>{city}, {country}</b><br/>"
                                    "Final Allowance: <b>${final_allowance}</b>",
                            "style": { "color": "white", "backgroundColor": "#1e3c72" }
                        }
                        
                        map_col, legend_col = st.columns([4, 1])

                        with map_col:
                            st.pydeck_chart(
                                pdk.Deck(
                                    layers=[layer],
                                    initial_view_state=view_state,
                                    tooltip=tooltip
                                ),
                                use_container_width=True
                            )

                        with legend_col:
                            st.write("##### Legend (Cost)")
                            st.markdown(f"""
                            <div style="display: flex; align-items: center; margin-bottom: 5px;">
                                <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
                                <span style="margin-left: 10px;">High Cost (~${max_cost:,.0f})</span>
                            </div>
                            <div style="display: flex; align-items: center; margin-bottom: 5px;">
                                <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
                                <span style="margin-left: 10px;">Medium Cost</span>
                            </div>
                            <div style="display: flex; align-items: center;">
                                <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
                                <span style="margin-left: 10px;">Low Cost (~${min_cost:,.0f})</span>
                            </div>
                            """, unsafe_allow_html=True)
                            st.caption("The larger the circle and the redder the color, the higher the cost of the city.")

                    # --- ì´í•˜ ê¸°ì¡´ Top 10 ì°¨íŠ¸ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ ---
                    st.divider()
                    col1, col2 = st.columns(2)
                    
                    if 'final_allowance' in df_merged.columns:
                        with col1:
                            st.write("##### ğŸ’° Top 10 High Cost Cities (AI Final)")
                            top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                            
                            average_cost = df_merged['final_allowance'].mean()
                            top_10_cost_df['average'] = average_cost
                            
                            base_cost = alt.Chart(top_10_cost_df).encode(
                                x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
                                y=alt.Y('final_allowance', title="Final Allowance ($)", axis=alt.Axis(format='$,.0f')),
                                tooltip=[
                                    alt.Tooltip('city', title="City"),
                                    alt.Tooltip('final_allowance', title="Final Allowance ($)", format='$,.0f'),
                                    alt.Tooltip('average', title="Overall Average", format='$,.0f')
                                ]
                            )
                            
                            bars_cost = base_cost.mark_bar().encode()
                            
                            rule_cost = alt.Chart(pd.DataFrame({'average_cost': [average_cost]})).mark_rule(
                                strokeDash=[3, 3]
                            ).encode(
                                y=alt.Y('average_cost', title=''),
                                tooltip=[alt.Tooltip('average_cost', title="Overall Average", format='$,.0f')] 
                            )
                            
                            chart_cost = (bars_cost + rule_cost).properties(
                                background='transparent',
                                title=f"Overall Average: ${average_cost:,.0f}" 
                            ).interactive()
                            st.altair_chart(chart_cost, use_container_width=True)
                        
                        with col2:
                            st.write("##### âš ï¸ Top 10 High Volatility Cities (AI Confidence)")
                            df_report_vc = pd.DataFrame(report_data['cities'])
                            df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
                            df_report_vc.dropna(subset=['vc'], inplace=True)
                            
                            if df_report_vc.empty:
                                st.info("Volatility (VC) data is missing. (AI analysis with the latest version is required)")
                            else:
                                top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                                average_vc = df_report_vc['vc'].mean()
                                top_10_vc_df['average'] = average_vc
                                
                                base_vc = alt.Chart(top_10_vc_df).encode(
                                    x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
                                    y=alt.Y('vc', title="Variation Coefficient (VC)", axis=alt.Axis(format='%')),
                                    tooltip=[
                                        alt.Tooltip('city', title="City"),
                                        alt.Tooltip('vc', title="Variation Coefficient (VC)", format='.2%'),
                                        alt.Tooltip('average', title="Overall Average", format='.2%')
                                    ]
                                )
                                
                                bars_vc = base_vc.mark_bar().encode()
                                
                                rule_vc = alt.Chart(pd.DataFrame({'average_vc': [average_vc]})).mark_rule(
                                    strokeDash=[3, 3]
                                ).encode(
                                    y=alt.Y('average_vc', title=''),
                                    tooltip=[alt.Tooltip('average_vc', title="Overall Average", format='.2%')]
                                )
                                
                                chart_vc = (bars_vc + rule_vc).properties(
                                    background='transparent',
                                    title=f"Overall Average: {average_vc:.2%}"
                                ).interactive()
                                st.altair_chart(chart_vc, use_container_width=True)
                                st.caption("The higher the volatility (VC), the less confident the AI is in its price estimation for the city.")
                    else:
                        st.warning("No 'final_allowance' data to display the chart.")


if employee_tab is not None:
    with employee_tab:
        st.header("Per Diem Inquiry by City")
        history_files = get_history_files()
        if not history_files:
            st.info("Please analyze a PDF in the 'Report Analysis' tab first.")
        else:
            # --- ë¦¬í¬íŠ¸ ì„ íƒ / ë¡œë“œ ---
            if "selected_report_file" not in st.session_state:
                st.session_state["selected_report_file"] = history_files[0]
            if st.session_state["selected_report_file"] not in history_files:
                st.session_state["selected_report_file"] = history_files[0]
            selected_file = st.session_state["selected_report_file"]

            report_data = load_report_data(selected_file)
            if report_data and "cities" in report_data and report_data["cities"]:
                cities_df = pd.DataFrame(report_data["cities"])
                target_entries = get_target_city_entries()
                countries = sorted({entry["country"] for entry in target_entries})

                # --- Country / City ---
                col_country, col_city = st.columns(2)

                with col_country:
                    selectable_countries = [
                        c
                        for c in countries
                        if c in cities_df["country_display"].unique()
                    ]
                    sel_country = st.selectbox(
                        "Country",
                        options=selectable_countries,
                        key=f"country_{selected_file}",
                        index=None,  # ê¸°ë³¸: ë¯¸ì„ íƒ
                        placeholder="Select Country",
                    )

                if sel_country:
                    filtered_cities_all = sorted(
                        {
                            entry["city"]
                            for entry in target_entries
                            if entry["country"] == sel_country
                        }
                    )
                else:
                    filtered_cities_all = []

                with col_city:
                    sel_city = st.selectbox(
                        "City",
                        options=filtered_cities_all,
                        key=f"city_{selected_file}",
                        index=None,
                        placeholder="Select City",
                    )
                    if sel_country and not filtered_cities_all:
                        st.warning(
                            "There are no registered cities for the selected country."
                        )

                # --- Trip dates / Job level ---
                col_start, col_end, col_level = st.columns([1, 1, 1])

                with col_start:
                    trip_start = st.date_input(
                        "Trip Start Date",
                        value=None,
                        key=f"trip_start_{selected_file}",
                    )

                with col_end:
                    trip_end = st.date_input(
                        "Trip End Date",
                        value=None,
                        key=f"trip_end_{selected_file}",
                    )

                with col_level:
                    sel_level = st.selectbox(
                        "Job Level",
                        options=list(JOB_LEVEL_RATIOS.keys()),
                        key=f"l_{selected_file}",
                        index=None,
                        placeholder="Select Job Level",
                    )

                # --- ì•ˆë‚´ ë¬¸êµ¬ + Search ë²„íŠ¼ ---
                msg_col, btn_col = st.columns([3, 1])
                with msg_col:
                    st.markdown("**Please select all of the above fields.**")
                with btn_col:
                    run_search = st.button(
                        "Search",
                        key=f"search_{selected_file}",
                        type="primary",
                        use_container_width=True,
                    )

                # ==========================
                #   Search í´ë¦­ ì´í›„ ë¡œì§
                # ==========================
                if run_search:
                    # 1) í•„ìˆ˜ ì…ë ¥ ì²´í¬
                    if not (
                        sel_country
                        and sel_city
                        and sel_level
                        and trip_start
                        and trip_end
                    ):
                        st.error("Please select all of the above fields.")
                    else:
                        # 2) ë‚ ì§œ ë³€í™˜
                        if isinstance(trip_start, datetime):
                            trip_start = trip_start.date()
                        if isinstance(trip_end, datetime):
                            trip_end = trip_end.date()

                        trip_valid = trip_end >= trip_start
                        if not trip_valid:
                            st.error(
                                "The end date must be on or after the start date."
                            )
                            trip_days = 0
                            trip_term = "Short-term"
                            trip_multiplier = SHORT_TERM_MULTIPLIER
                            trip_term_label = TRIP_TERM_LABELS.get(
                                trip_term, trip_term
                            )
                        else:
                            trip_days = (trip_end - trip_start).days + 1
                            trip_term, trip_multiplier = classify_trip_duration(
                                trip_days
                            )
                            trip_term_label = TRIP_TERM_LABELS.get(
                                trip_term, trip_term
                            )
                            st.caption(
                                f"Auto-classified trip type: {trip_term_label} Â· {trip_days}-day trip"
                            )

                        # 3) Trip length ê¸°ì¤€ìœ¼ë¡œ city ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                        if sel_city:
                            filtered_trip_cities = []
                            for entry in target_entries:
                                if (
                                    entry["country"] != sel_country
                                    or entry["city"] != sel_city
                                ):
                                    continue
                                if trip_valid and trip_term not in entry.get(
                                    "trip_lengths", TRIP_LENGTH_OPTIONS
                                ):
                                    continue
                                filtered_trip_cities.append(entry["city"])
                            if trip_valid and not filtered_trip_cities:
                                st.warning(
                                    "No city data available for this period. "
                                    "Adjust the trip type to 'Short-term' or check city settings."
                                )
                                sel_city = None

                        # 4) ëª¨ë“  ì¡°ê±´ì´ ìœ íš¨í•  ë•Œë§Œ ê³„ì‚°/í‘œì‹œ
                        if (
                            trip_valid
                            and sel_city
                            and sel_level
                            and trip_days is not None
                        ):
                            city_row = cities_df[cities_df["city"] == sel_city]
                            if city_row.empty:
                                st.error(
                                    "No analysis data found for the selected city."
                                )
                            else:
                                city_data = city_row.iloc[0].to_dict()
                                final_allowance = city_data.get("final_allowance")
                                st.subheader(f"{sel_country} - {sel_city} Results")

                                level_ratio = JOB_LEVEL_RATIOS.get(
                                    sel_level, 1.0
                                )

                                # --- Summary ì¹´ë“œ (ì´ì•¡ / ì¼ë‹¹) ---
                                if final_allowance:
                                    adjusted_daily_allowance = round(
                                        final_allowance * trip_multiplier
                                    )
                                    level_daily_allowance = round(
                                        adjusted_daily_allowance * level_ratio
                                    )
                                    trip_total_allowance = (
                                        level_daily_allowance * trip_days
                                    )

                                    render_primary_summary(
                                        f"{sel_level.split(' ')[0]}",
                                        trip_total_allowance,
                                        level_daily_allowance,
                                        trip_days,
                                        trip_term_label,
                                        trip_multiplier,
                                    )
                                else:
                                    st.metric(
                                        f"{sel_level.split(' ')[0]} Daily Recommended Per Diem",
                                        "No Amount",
                                    )

                                # =====================
                                #   Basis of Calculation
                                # =====================
                                menu_samples = city_data.get("menu_samples") or []

                                detail_cards_visible = any(
                                    [
                                        employee_sections_visibility[
                                            "show_un_basis"
                                        ],
                                        employee_sections_visibility[
                                            "show_ai_estimate"
                                        ],
                                        employee_sections_visibility[
                                            "show_weighted_result"
                                        ],
                                        employee_sections_visibility[
                                            "show_ai_market_detail"
                                        ],
                                    ]
                                )
                                extra_content_visible = (
                                    employee_sections_visibility[
                                        "show_provenance"
                                    ]
                                    or (
                                        employee_sections_visibility[
                                            "show_menu_samples"
                                        ]
                                        and menu_samples
                                    )
                                )

                                if detail_cards_visible or extra_content_visible:
                                    st.markdown("---")
                                    st.write("**Basis of Calculation (Daily Rate)**")

                                    un_data = city_data.get("un", {})
                                    ai_summary = city_data.get("ai_summary", {})
                                    season_context = city_data.get(
                                        "season_context", {}
                                    )

                                    ai_avg = ai_summary.get(
                                        "season_adjusted_mean_rounded"
                                    )
                                    ai_runs = ai_summary.get(
                                        "successful_runs",
                                        len(
                                            ai_summary.get(
                                                "used_totals", []
                                            )
                                        ),
                                    )
                                    ai_attempts = ai_summary.get(
                                        "attempted_runs", NUM_AI_CALLS
                                    )
                                    removed_totals = (
                                        ai_summary.get("removed_totals")
                                        or []
                                    )
                                    season_label = (
                                        season_context.get("label")
                                        or ai_summary.get(
                                            "season_label", "Standard"
                                        )
                                    )
                                    season_factor = season_context.get(
                                        "factor",
                                        ai_summary.get(
                                            "season_factor", 1.0
                                        ),
                                    )

                                    ai_notes_parts = [
                                        f"Success {ai_runs}/{ai_attempts} runs"
                                    ]
                                    if removed_totals:
                                        ai_notes_parts.append(
                                            f"Outliers {removed_totals}"
                                        )
                                    if season_label:
                                        ai_notes_parts.append(
                                            f"Season {season_label} Ã—{season_factor}"
                                        )
                                    ai_notes = (
                                        " | ".join(ai_notes_parts)
                                        if ai_notes_parts
                                        else "No AI Data"
                                    )

                                    # ë™ì  ê°€ì¤‘ì¹˜ ì •ë³´
                                    weights_info = (
                                        ai_summary.get(
                                            "weighted_average_components", {}
                                        ).get("weights", {})
                                    )
                                    weights_source = weights_info.get(
                                        "source", "N/A"
                                    )
                                    un_weight_pct = (
                                        f"{weights_info.get('un_weight', 0.5):.0%}"
                                    )
                                    ai_weight_pct = (
                                        f"{weights_info.get('ai_weight', 0.5):.0%}"
                                    )
                                    weight_caption = (
                                        f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct})"
                                        f" | Reason: {weights_source}"
                                    )

                                    # --- UN / AI / Weighted (ë ˆë²¨ + ê¸°ê°„ ë°˜ì˜) ---
                                    un_base = None
                                    un_display = None
                                    if un_data.get("status") == "ok" and isinstance(
                                        un_data.get(
                                            "per_diem_excl_lodging"
                                        ),
                                        (int, float),
                                    ):
                                        un_base = un_data[
                                            "per_diem_excl_lodging"
                                        ]  # city ê¸°ì¤€ short-term
                                        un_display = round(
                                            un_base * trip_multiplier * level_ratio
                                        )

                                    ai_display = (
                                        round(
                                            ai_avg * trip_multiplier * level_ratio
                                        )
                                        if ai_avg is not None
                                        else None
                                    )
                                    weighted_display = (
                                        round(
                                            final_allowance
                                            * trip_multiplier
                                            * level_ratio
                                        )
                                        if final_allowance is not None
                                        else None
                                    )

                                    first_row_keys = []
                                    if employee_sections_visibility[
                                        "show_un_basis"
                                    ]:
                                        first_row_keys.append("un")
                                    if employee_sections_visibility[
                                        "show_ai_estimate"
                                    ]:
                                        first_row_keys.append("ai")
                                    if employee_sections_visibility[
                                        "show_weighted_result"
                                    ]:
                                        first_row_keys.append("weighted")

                                    if first_row_keys:
                                        first_row_cols = st.columns(
                                            len(first_row_keys)
                                        )
                                        for key, col in zip(
                                            first_row_keys, first_row_cols
                                        ):
                                            with col:
                                                if key == "un":
                                                    if un_base is not None:
                                                        base_short_level = round(
                                                            un_base * level_ratio
                                                        )
                                                        base_long_level = round(
                                                            un_base
                                                            * trip_multiplier
                                                            * level_ratio
                                                        )
                                                        if (
                                                            trip_term
                                                            == "Long-term"
                                                        ):
                                                            un_caption = (
                                                                f"Base (level-adjusted): "
                                                                f"Short-term $ {base_short_level:,} â†’ "
                                                                f"Long-term $ {base_long_level:,}"
                                                            )
                                                        else:
                                                            un_caption = (
                                                                f"Base (level-adjusted): "
                                                                f"Short-term $ {base_short_level:,}"
                                                            )
                                                    else:
                                                        un_caption = city_data.get(
                                                            "notes", ""
                                                        )

                                                    render_stat_card(
                                                        "UN-DSA Basis",
                                                        f"$ {un_display:,}"
                                                        if un_display
                                                        is not None
                                                        else "N/A",
                                                        un_caption,
                                                        "secondary",
                                                    )

                                                elif key == "ai":
                                                    if ai_avg is not None:
                                                        ai_short_level = round(
                                                            ai_avg * level_ratio
                                                        )
                                                        ai_long_level = round(
                                                            ai_avg
                                                            * trip_multiplier
                                                            * level_ratio
                                                        )
                                                        if (
                                                            trip_term
                                                            == "Long-term"
                                                        ):
                                                            ai_level_caption = (
                                                                f"Short-term $ {ai_short_level:,} â†’ "
                                                                f"Long-term $ {ai_long_level:,}"
                                                            )
                                                        else:
                                                            ai_level_caption = (
                                                                f"Short-term $ {ai_short_level:,}"
                                                            )
                                                    else:
                                                        ai_level_caption = ""

                                                    ai_full_caption = " | ".join(
                                                        part
                                                        for part in [
                                                            ai_notes,
                                                            ai_level_caption,
                                                        ]
                                                        if part
                                                    )

                                                    render_stat_card(
                                                        "AI Market Estimate (Seasonal Adj.)",
                                                        f"$ {ai_display:,}"
                                                        if ai_display
                                                        is not None
                                                        else "N/A",
                                                        ai_full_caption,
                                                        "secondary",
                                                    )

                                                else:  # weighted
                                                    if final_allowance is not None:
                                                        weighted_short_level = round(
                                                            final_allowance
                                                            * level_ratio
                                                        )
                                                        weighted_long_level = round(
                                                            final_allowance
                                                            * trip_multiplier
                                                            * level_ratio
                                                        )
                                                        if (
                                                            trip_term
                                                            == "Long-term"
                                                        ):
                                                            amount_part = (
                                                                f"Short-term $ {weighted_short_level:,} â†’ "
                                                                f"Long-term $ {weighted_long_level:,}"
                                                            )
                                                        else:
                                                            amount_part = (
                                                                f"Short-term $ {weighted_short_level:,}"
                                                            )
                                                        weighted_caption = (
                                                            f"{amount_part} | {weight_caption}"
                                                        )
                                                    else:
                                                        weighted_caption = weight_caption

                                                    render_stat_card(
                                                        "Weighted Average Result",
                                                        f"$ {weighted_display:,}"
                                                        if weighted_display
                                                        is not None
                                                        else "N/A",
                                                        weighted_caption,
                                                        "secondary",
                                                    )

                                    # --- AI ìƒì„¸ Breakdown ---
                                    if employee_sections_visibility[
                                        "show_ai_market_detail"
                                    ]:
                                        st.markdown(
                                            "<br>", unsafe_allow_html=True
                                        )

                                        mean_food = ai_summary.get(
                                            "mean_food", 0
                                        )
                                        mean_trans = ai_summary.get(
                                            "mean_transport", 0
                                        )
                                        mean_misc = ai_summary.get(
                                            "mean_misc", 0
                                        )

                                        food_display = round(
                                            mean_food
                                            * season_factor
                                            * trip_multiplier
                                        )
                                        trans_display = round(
                                            mean_trans
                                            * season_factor
                                            * trip_multiplier
                                        )
                                        misc_display = round(
                                            mean_misc
                                            * season_factor
                                            * trip_multiplier
                                        )

                                        st.write(
                                            "###### AI Estimate Details (Daily Rate)"
                                        )
                                        col_f, col_t, col_m = st.columns(3)
                                        with col_f:
                                            render_stat_card(
                                                "Est. Food",
                                                f"$ {food_display:,}",
                                                f"Short-term base: $ {round(mean_food)}",
                                                "muted",
                                            )
                                        with col_t:
                                            render_stat_card(
                                                "Est. Transport",
                                                f"$ {trans_display:,}",
                                                f"Short-term base: $ {round(mean_trans)}",
                                                "muted",
                                            )
                                        with col_m:
                                            render_stat_card(
                                                "Est. Misc",
                                                f"$ {misc_display:,}",
                                                f"Short-term base: $ {round(mean_misc)}",
                                                "muted",
                                            )

                                    # --- Provenance / Menu samples ---
                                    if employee_sections_visibility[
                                        "show_provenance"
                                    ]:
                                        with st.expander(
                                            "AI provenance & prompts"
                                        ):
                                            provenance_payload = {
                                                "season_context": season_context,
                                                "ai_summary": ai_summary,
                                                "ai_runs": city_data.get(
                                                    "ai_provenance", []
                                                ),
                                                "reference_links": build_reference_link_lines(
                                                    menu_samples, max_items=8
                                                ),
                                                "weights": weights_info,
                                            }
                                            st.json(provenance_payload)

                                    if (
                                        employee_sections_visibility[
                                            "show_menu_samples"
                                        ]
                                        and menu_samples
                                    ):
                                        with st.expander(
                                            "Reference menu samples"
                                        ):
                                            link_lines = build_reference_link_lines(
                                                menu_samples, max_items=8
                                            )
                                            if link_lines:
                                                st.markdown("**Direct links**")
                                                for link_line in link_lines:
                                                    st.markdown(
                                                        f"- {link_line}"
                                                    )
                                                st.markdown("---")
                                            st.table(
                                                pd.DataFrame(menu_samples)
                                            )
                                else:
                                    st.info(
                                        "The administrator has hidden the detailed calculation basis."
                                    )
            else:
                st.error("Failed to load report data.")


# --- Admin: Report Analysis íƒ­ ì „ì²´ ì½”ë“œ ---
if admin_analysis_tab is not None:
    with admin_analysis_tab:
        # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ì‚¬ì´ë“œë°”ì—ì„œ Admin ë¡œê·¸ì¸ ê²€ì¦ì´ ëë‚œ ìƒíƒœë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
        st.subheader("Report Version Management")

        history_files = get_history_files()

        # --- [ì„¹ì…˜ 1] í™œì„± ë¦¬í¬íŠ¸ ë²„ì „ ì„ íƒ ---
        st.subheader("Report Version Management")
        if history_files:
            if "selected_report_file" not in st.session_state:
                st.session_state["selected_report_file"] = history_files[0]
            if st.session_state["selected_report_file"] not in history_files:
                st.session_state["selected_report_file"] = history_files[0]
            default_index = history_files.index(st.session_state["selected_report_file"])
            selected_file = st.selectbox(
                "Select the active report version:",
                history_files,
                index=default_index,
                key="admin_report_file_select",
            )
            st.session_state["selected_report_file"] = selected_file
        else:
            st.info("No reports have been generated.")

        # --- [ì„¹ì…˜ 2] ê³¼ê±° ë¦¬í¬íŠ¸ ë¹„êµ ---
        st.divider()
        st.subheader("Compare Past Reports")
        if len(history_files) < 2:
            st.info("At least 2 reports are required for comparison.")
        else:
            col_a, col_b = st.columns(2)
            with col_a:
                file_a = st.selectbox(
                    "Base Report (A)",
                    history_files,
                    index=1 if len(history_files) > 1 else 0,
                    key="compare_a",
                )
            with col_b:
                file_b = st.selectbox(
                    "Comparison Report (B)",
                    history_files,
                    index=0,
                    key="compare_b",
                )

            if st.button("Compare Reports"):
                if file_a == file_b:
                    st.warning("You must select two different reports.")
                else:
                    with st.spinner("Comparing reports..."):
                        data_a = load_report_data(file_a)
                        data_b = load_report_data(file_b)

                        if (
                            data_a
                            and data_b
                            and "cities" in data_a
                            and "cities" in data_b
                        ):
                            df_a = pd.DataFrame(data_a["cities"])[
                                ["city", "country_display", "final_allowance"]
                            ]
                            df_b = pd.DataFrame(data_b["cities"])[
                                ["city", "country_display", "final_allowance"]
                            ]

                            df_merged = pd.merge(
                                df_a,
                                df_b,
                                on=["city", "country_display"],
                                suffixes=("_A", "_B"),
                            )

                            report_a_label = (
                                file_a.split("report_")[-1].split(".")[0]
                            )
                            report_b_label = (
                                file_b.split("report_")[-1].split(".")[0]
                            )

                            df_merged[f"A ({report_a_label})"] = df_merged[
                                "final_allowance_A"
                            ]
                            df_merged[f"B ({report_b_label})"] = df_merged[
                                "final_allowance_B"
                            ]

                            df_merged["Change ($)"] = (
                                df_merged["final_allowance_B"]
                                - df_merged["final_allowance_A"]
                            )

                            # Prevent division by zero
                            df_merged["Change (%)"] = (
                                df_merged["Change ($)"]
                                / df_merged["final_allowance_A"].replace(
                                    0, pd.NA
                                )
                                * 100
                            )

                            st.dataframe(
                                df_merged[
                                    [
                                        "city",
                                        "country_display",
                                        f"A ({report_a_label})",
                                        f"B ({report_b_label})",
                                        "Change ($)",
                                        "Change (%)",
                                    ]
                                ]
                                .style.format(
                                    {
                                        "Change (%)": "{:,.1f}%",
                                        "Change ($)": "{:,.0f}",
                                    }
                                ),
                                width="stretch",
                            )
                        else:
                            st.error("Failed to load report files.")

        # --- [ì„¹ì…˜ 3] UN-DSA PDF ë¶„ì„ + AI ì‹¤í–‰ ---
        st.divider()
        st.subheader("UN-DSA (PDF) Analysis & AI Execution")
        st.warning(
            f"Note that the AI will be called {NUM_AI_CALLS} times, which will consume time and cost. "
            "(Improvement 1: Async processing for faster speed)"
        )
        uploaded_file = st.file_uploader(
            "Upload UN-DSA PDF file.", type="pdf"
        )

        # --- Async AI analysis ì‹¤í–‰ ë¡œì§ ---
        if uploaded_file and st.button("Run AI Analysis", type="primary"):
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                st.error("Please set OPENAI_API_KEY in the .env file.")
            else:
                st.session_state.latest_analysis_result = None

                async def run_analysis(progress_bar, openai_api_key):
                    # 1) PDF â†’ Text
                    progress_bar.progress(0, text="Extracting PDF text...")
                    full_text = parse_pdf_to_text(uploaded_file)

                    # 2) Text â†’ TSV (OpenAI)
                    CHUNK_SIZE = 15000
                    text_chunks = [
                        full_text[i : i + CHUNK_SIZE]
                        for i in range(0, len(full_text), CHUNK_SIZE)
                    ]
                    all_tsv_lines = []
                    analysis_failed = False

                    for i, chunk in enumerate(text_chunks):
                        progress_bar.progress(
                            i / (len(text_chunks) + 1),
                            text=f"AI PDF->TSV converting... ({i+1}/{len(text_chunks)})",
                        )
                        chunk_tsv = call_openai_for_tsv_conversion(
                            chunk, openai_api_key
                        )
                        if chunk_tsv:
                            lines = chunk_tsv.strip().split("\n")
                            if not all_tsv_lines:
                                all_tsv_lines.extend(lines)
                            else:
                                all_tsv_lines.extend(lines[1:])
                        else:
                            analysis_failed = True
                            break

                    if analysis_failed:
                        st.error("Failed to convert PDF->TSV.")
                        progress_bar.empty()
                        return

                    processed_data = process_tsv_data(
                        "\n".join(all_tsv_lines)
                    )
                    if not processed_data:
                        st.error("Failed to process TSV data.")
                        progress_bar.empty()
                        return

                    # 3) ë¦¬í¬íŠ¸ ë‚´ ë„ì‹œ ì¢Œí‘œ(lat/lon) ìƒì„± í›„ ì €ì¥ (ì§€ë„ìš©)
                    try:
                        geolocator = Nominatim(
                            user_agent=f"aicp_report_map_{random.randint(1000,9999)}"
                        )
                    except Exception as e:
                        st.warning(f"ì§€ë„ ì¢Œí‘œìš© geopy ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    else:
                        with st.spinner(
                            "ì§€ë„ìš© ë„ì‹œ ì¢Œí‘œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
                        ):
                            for city in processed_data.get("cities", []):
                                if city.get("lat") and city.get("lon"):
                                    continue

                                city_name = city.get("city")
                                country_name = city.get("country_display")
                                if not city_name or not country_name:
                                    continue

                                query = f"{city_name}, {country_name}"
                                try:
                                    location = geolocator.geocode(
                                        query, timeout=5
                                    )
                                    time.sleep(1)  # Nominatim rate limit
                                    if location:
                                        city["lat"] = float(
                                            location.latitude
                                        )
                                        city["lon"] = float(
                                            location.longitude
                                        )
                                except Exception:
                                    # ì¢Œí‘œë¥¼ ëª» ì°¾ì•„ë„ ì „ì²´ ë¶„ì„ì€ ê³„ì† ì§„í–‰
                                    continue

                    # 4) Async OpenAI Client ìƒì„±
                    client = openai.AsyncOpenAI(api_key=openai_api_key)

                    total_cities = len(processed_data["cities"])
                    all_tasks = []  # [ [City1-10runs], [City2-10runs], ... ]

                    # 5) ë„ì‹œë³„ AI í˜¸ì¶œ Task ì¤€ë¹„
                    for city_data in processed_data["cities"]:
                        city_name, country_name = (
                            city_data["city"],
                            city_data["country_display"],
                        )
                        city_context = {
                            "neighborhood": city_data.get("neighborhood"),
                            "hotel_cluster": city_data.get("hotel_cluster"),
                        }
                        season_context = city_data.get(
                            "season_context"
                        ) or get_current_season_info(
                            city_name, country_name
                        )
                        menu_samples = load_cached_menu_prices(
                            city_name,
                            country_name,
                            city_context.get("neighborhood"),
                        )

                        city_data["menu_samples"] = menu_samples
                        city_data["reference_links"] = (
                            build_reference_link_lines(
                                menu_samples, max_items=8
                            )
                        )

                        city_tasks = []
                        for j in range(1, NUM_AI_CALLS + 1):
                            task = get_market_data_from_ai_async(
                                client,
                                city_name,
                                country_name,
                                f"Run {j}",
                                context=city_context,
                                season_context=season_context,
                                menu_samples=menu_samples,
                            )
                            city_tasks.append(task)

                        all_tasks.append(city_tasks)

                    # 6) ë„ì‹œë³„ë¡œ 10ê°œ Task ë™ì‹œ ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬
                    city_index = 0
                    for city_tasks in all_tasks:
                        city_data = processed_data["cities"][city_index]
                        city_name = city_data["city"]
                        progress_text = (
                            f"Calculating AI estimates... "
                            f"({city_index+1}/{total_cities}) {city_name}"
                        )
                        progress_bar.progress(
                            (city_index + 1) / max(total_cities, 1),
                            text=progress_text,
                        )

                        try:
                            market_results = await asyncio.gather(
                                *city_tasks
                            )
                        except Exception as e:
                            st.error(
                                f"Async error during {city_name} analysis: {e}"
                            )
                            market_results = []

                        ai_totals_source: List[int] = []
                        ai_meta_runs: List[Dict[str, Any]] = []

                        ai_food: List[int] = []
                        ai_transport: List[int] = []
                        ai_misc: List[int] = []

                        for j, market_result in enumerate(
                            market_results, 1
                        ):
                            city_data[f"market_data_{j}"] = market_result
                            if (
                                market_result.get("status") == "ok"
                                and market_result.get("total") is not None
                            ):
                                ai_totals_source.append(
                                    market_result["total"]
                                )
                                ai_food.append(market_result.get("food", 0))
                                ai_transport.append(
                                    market_result.get("transport", 0)
                                )
                                ai_misc.append(market_result.get("misc", 0))

                            if "meta" in market_result:
                                ai_meta_runs.append(market_result["meta"])

                        city_data["ai_provenance"] = ai_meta_runs

                        # ìµœì¢… Per Diem ê³„ì‚°
                        final_allowance = None
                        un_per_diem_raw = (
                            city_data.get("un", {}).get(
                                "per_diem_excl_lodging"
                            )
                        )
                        un_per_diem = (
                            float(un_per_diem_raw)
                            if isinstance(un_per_diem_raw, (int, float))
                            else None
                        )

                        ai_stats = aggregate_ai_totals(ai_totals_source)
                        season_factor = (season_context or {}).get(
                            "factor", 1.0
                        )
                        ai_base_mean = ai_stats.get("mean_raw")
                        ai_season_adjusted = (
                            ai_base_mean * season_factor
                            if ai_base_mean is not None
                            else None
                        )

                        admin_weights = get_weight_config()
                        ai_vc_score = ai_stats.get("variation_coeff")

                        if un_per_diem is not None:
                            weights_cfg = get_dynamic_weights(
                                ai_vc_score, admin_weights
                            )
                        else:
                            # UN ë°ì´í„° ì—†ìœ¼ë©´ AI 100%
                            weights_cfg = {
                                "un_weight": 0.0,
                                "ai_weight": 1.0,
                                "source": "AI Only (UN-DSA Missing)",
                            }

                        city_data["ai_summary"] = {
                            "raw_totals": ai_totals_source,
                            "used_totals": ai_stats.get("used_values", []),
                            "removed_totals": ai_stats.get(
                                "removed_values", []
                            ),
                            "mean_base": ai_base_mean,
                            "mean_base_rounded": ai_stats.get("mean"),
                            "ai_consistency_vc": ai_vc_score,
                            "mean_food": mean(ai_food) if ai_food else 0,
                            "mean_transport": mean(ai_transport)
                            if ai_transport
                            else 0,
                            "mean_misc": mean(ai_misc) if ai_misc else 0,
                            "season_factor": season_factor,
                            "season_label": (season_context or {}).get(
                                "label"
                            ),
                            "season_adjusted_mean_raw": ai_season_adjusted,
                            "season_adjusted_mean_rounded": round(
                                ai_season_adjusted
                            )
                            if ai_season_adjusted is not None
                            else None,
                            "successful_runs": len(
                                ai_stats.get("used_values", [])
                            ),
                            "attempted_runs": NUM_AI_CALLS,
                            "reference_links": city_data.get(
                                "reference_links", []
                            ),
                            "weighted_average_components": {
                                "un_per_diem": un_per_diem,
                                "ai_season_adjusted": ai_season_adjusted,
                                "weights": weights_cfg,
                            },
                        }

                        # ë™ì  ê°€ì¤‘ì¹˜ ì ìš©
                        if (
                            un_per_diem is not None
                            and ai_season_adjusted is not None
                        ):
                            weighted_average = (
                                un_per_diem * weights_cfg["un_weight"]
                                + ai_season_adjusted * weights_cfg["ai_weight"]
                            )
                            final_allowance = round(weighted_average)
                        elif un_per_diem is not None:
                            final_allowance = round(un_per_diem)
                        elif ai_season_adjusted is not None:
                            final_allowance = round(ai_season_adjusted)

                        city_data["final_allowance"] = final_allowance

                        if (
                            final_allowance
                            and un_per_diem
                            and un_per_diem > 0
                        ):
                            city_data["delta_vs_un_pct"] = round(
                                (
                                    (final_allowance - un_per_diem)
                                    / un_per_diem
                                )
                                * 100
                            )
                        else:
                            city_data["delta_vs_un_pct"] = "N/A"

                        city_index += 1

                    save_report_data(processed_data)
                    st.session_state.latest_analysis_result = processed_data
                    st.success("AI analysis completed.")
                    progress_bar.empty()
                    st.rerun()

                # ì‹¤ì œ ì‹¤í–‰
                with st.spinner(
                    "Processing PDF and running AI analysis... (Takes approx. 10-30 seconds)"
                ):
                    progress_bar = st.progress(
                        0, text="Starting analysis..."
                    )
                    asyncio.run(run_analysis(progress_bar, openai_api_key))

        # --- [ì„¹ì…˜ 4] Latest Analysis Summary í…Œì´ë¸” ---
        if st.session_state.latest_analysis_result:
            st.markdown("---")
            st.subheader("Latest Analysis Summary")
            df_data = []
            for city in st.session_state.latest_analysis_result["cities"]:
                row = {
                    "City": city.get("city", "N/A"),
                    "Country": city.get("country_display", "N/A"),
                    "UN-DSA": city.get("un", {}).get(
                        "per_diem_excl_lodging"
                    ),
                }
                for j in range(1, NUM_AI_CALLS + 1):
                    row[f"AI {j}"] = city.get(
                        f"market_data_{j}", {}
                    ).get("total")

                delta_val = city.get("delta_vs_un_pct")
                if isinstance(delta_val, (int, float)):
                    delta_display = f"{delta_val:.0f}%"
                else:
                    delta_display = "N/A"

                row.update(
                    {
                        "Final Allowance": city.get("final_allowance"),
                        "Delta (%)": delta_display,
                        "Trip Lengths": DEFAULT_TRIP_LENGTH[0],
                        "Notes": city.get("notes", ""),
                    }
                )
                df_data.append(row)

            st.dataframe(
                pd.DataFrame(df_data), use_container_width=True
            )
            with st.expander("View generated markdown report"):
                st.markdown(
                    generate_markdown_report(
                        st.session_state.latest_analysis_result
                    )
                )



def auto_fill_all_city_coordinates() -> tuple[int, int]:
    """
    target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
    DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
    """
    try:
        geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
    except Exception as e:
        st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return 0, 0

    current_entries = get_target_city_entries()
    entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

    if not entries_to_update:
        return 0, 0

    success_count = 0
    fail_count = 0

    progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

    with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
        for i, entry in enumerate(entries_to_update):
            city = entry["city"]
            country = entry["country"]
            query = f"{city}, {country}"

            try:
                location = geolocator.geocode(query, timeout=5)
                time.sleep(1)  # Nominatim rate limit

                if location:
                    entry["lat"] = location.latitude
                    entry["lon"] = location.longitude
                    st.toast(f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
                    success_count += 1
                else:
                    st.toast(f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="â“")
                    fail_count += 1
            except (GeocoderTimedOut, GeocoderUnavailable):
                st.toast(f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", icon="ğŸ”¥")
                fail_count += 1
            except Exception as e:
                st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
                fail_count += 1

            progress_bar.progress((i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}")

    # DB + ì„¸ì…˜ì— ì €ì¥
    set_target_city_entries(current_entries)
    progress_bar.empty()
    return success_count, fail_count


# --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
if admin_config_tab is not None:
    with admin_config_tab:
        # ì´ë¯¸ Admin íƒ­ì´ ë³´ì¸ ì‹œì ì—ì„œ Access Code ê²€ì¦ ì™„ë£Œ ìƒíƒœ
        # ë°”ë¡œ ì„¤ì • UI ë Œë”ë§
        current_entries = get_target_city_entries()
        options = {
            f"{entry['region']} | {entry['country']} | {entry['city']}": idx
            for idx, entry in enumerate(current_entries)
        }
        sorted_labels = list(options.keys())
        # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
        if not st.session_state.get(ACCESS_CODE_KEY, False):
            st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤. 'ë³´ê³ ì„œ ë¶„ì„ (Admin)' íƒ­ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()
            
        # --- [v19.3] ë„ì‹œ í¸ì§‘/ìºì‹œ ê´€ë¦¬ê°€ ê³µìœ í•  ë„ì‹œ ëª©ë¡ì„ íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜ ---
        current_entries = get_target_city_entries()
        options = {
            f"{entry['region']} | {entry['country']} | {entry['city']}": idx
            for idx, entry in enumerate(current_entries)
        }
        sorted_labels = list(options.keys())
        
        # --- ì½œë°± í•¨ìˆ˜ 1: 'ë„ì‹œ í¸ì§‘' í¼ ë™ê¸°í™” ---
        def _sync_edit_form_from_selection():
            if "edit_city_selector" not in st.session_state or not st.session_state.edit_city_selector:
                # st.session_state.edit_city_selectorê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì¼ ë•Œ
                if sorted_labels:
                    st.session_state.edit_city_selector = sorted_labels[0]
                else:
                    return # ë„ì‹œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                
            selected_idx = options[st.session_state.edit_city_selector]
            selected_entry = current_entries[selected_idx]
            
            st.session_state.edit_region = selected_entry.get("region", "")
            st.session_state.edit_city = selected_entry.get("city", "")
            st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
            st.session_state.edit_country = selected_entry.get("country", "")
            st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
            
            existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
            st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
            
            sub_data = selected_entry.get("un_dsa_substitute") or {}
            st.session_state.edit_sub_city = sub_data.get("city", "")
            st.session_state.edit_sub_country = sub_data.get("country", "")

        # --- [v19.3] ì½œë°± í•¨ìˆ˜ 2: 'ìºì‹œ ì¶”ê°€' í¼ ë™ê¸°í™” ---
        def _sync_cache_form_from_selection():
            selected_label = st.session_state.get("cache_city_selector") # get()ìœ¼ë¡œ ì˜¤ë¥˜ ë°©ì§€
            
            if selected_label in options: # 'options' dictë¥¼ ê³µìœ 
                selected_idx = options[selected_label]
                selected_entry = current_entries[selected_idx]
                st.session_state.new_cache_country = selected_entry.get("country", "")
                st.session_state.new_cache_city = selected_entry.get("city", "")
                st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
            else: # (placeholder ì„ íƒ ì‹œ)
                st.session_state.new_cache_country = ""
                st.session_state.new_cache_city = ""
                st.session_state.new_cache_neighborhood = ""
            
            # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” í•­ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            st.session_state.new_cache_vendor = ""
            st.session_state.new_cache_category = "Food"
            st.session_state.new_cache_price = 0.0
            st.session_state.new_cache_currency = "USD"
            st.session_state.new_cache_url = ""

        # --- [v19.3 í•«í”½ìŠ¤] ì½œë°± í•¨ìˆ˜ 3: 'ìºì‹œ ì €ì¥' ë¡œì§ ---
        def handle_cache_submit():
            # 1. ìœ íš¨ì„± ê²€ì‚¬
            if (not st.session_state.new_cache_country or 
                not st.session_state.new_cache_city or 
                not st.session_state.new_cache_vendor):
                st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
                return # ì—¬ê¸°ì„œ ì¤‘ë‹¨ (í¼ ê°’ ìœ ì§€ë¨)

            # 2. ìƒˆ í•­ëª© ìƒì„±
            new_entry = {
                "country": st.session_state.new_cache_country.strip(),
                "city": st.session_state.new_cache_city.strip(),
                "neighborhood": st.session_state.new_cache_neighborhood.strip(),
                "vendor": st.session_state.new_cache_vendor.strip(),
                "category": st.session_state.new_cache_category,
                "price": st.session_state.new_cache_price,
                "currency": st.session_state.new_cache_currency.strip().upper(),
                "url": st.session_state.new_cache_url.strip(),
            }
            
            # 3. íŒŒì¼ì— ì €ì¥
            if add_menu_cache_entry(new_entry):
                st.success(f"'{new_entry['vendor']}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                
                # 4. (ì¤‘ìš”) í¼ ë¦¬ì…‹: session_state ê°’ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”
                # ì´ ë¡œì§ì€ on_click ì½œë°± ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
                st.session_state.new_cache_country = ""
                st.session_state.new_cache_city = ""
                st.session_state.new_cache_neighborhood = ""
                st.session_state.new_cache_vendor = ""
                st.session_state.new_cache_category = "Food"
                st.session_state.new_cache_price = 0.0
                st.session_state.new_cache_currency = "USD"
                st.session_state.new_cache_url = ""
                st.session_state.cache_city_selector = None # ë“œë¡­ë‹¤ìš´ë„ ë¦¬ì…‹
                
                # st.rerun()ì€ on_click ì½œë°±ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•  í•„ìš” ì—†ìŒ
            else:
                st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        # --- [v19.3 í•«í”½ìŠ¤] ë ---

        st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
        visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
        if visibility_toggle != stored_employee_tab_visible:
            updated_settings = dict(ui_settings)
            updated_settings["show_employee_tab"] = visibility_toggle
            updated_settings["employee_sections"] = employee_sections_visibility
            save_ui_settings(updated_settings)
            ui_settings = updated_settings
            st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
            st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")
            time.sleep(1) # ìœ ì €ê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤Œ
            st.rerun()

        st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
        section_toggle_values: Dict[str, bool] = {}
        for section_key, label in EMPLOYEE_SECTION_LABELS:
            current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
            section_toggle_values[section_key] = st.toggle(
                label,
                value=current_value,
                key=f"employee_section_toggle_{section_key}",
            )
        if section_toggle_values != employee_sections_visibility:
            updated_settings = dict(ui_settings)
            updated_settings["employee_sections"] = section_toggle_values
            save_ui_settings(updated_settings)
            ui_settings["employee_sections"] = section_toggle_values
            st.session_state.employee_sections_visibility = section_toggle_values
            employee_sections_visibility = section_toggle_values
            st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            time.sleep(1)
            st.rerun()

        st.divider()
        st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
        st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
        current_weights = get_weight_config()
        st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
        with st.form("weight_config_form"):
            un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
            ai_weight_preview = max(0.0, 1.0 - un_weight_input)
            st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
            st.caption("Weights are normalised to sum to 1.0 when saved.")
            weight_submit = st.form_submit_button("Save weights")
        if weight_submit:
            updated = update_weight_config(un_weight_input, ai_weight_preview)
            st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
            st.rerun()

        st.divider()
        st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬ (target_cities_config.json)")
        entries_df = pd.DataFrame(get_target_city_entries())
        if not entries_df.empty:
            entries_display = entries_df.copy()
            # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
            entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
            st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch') # [v19.3] ê²½ê³  ìˆ˜ì •
        else:
            st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

        # --- [ì‹ ê·œ 2] ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„± ê¸°ëŠ¥ (ìƒˆë¡œ ì¶”ê°€) ---
        st.divider()
        st.subheader("ë„ì‹œ ì¢Œí‘œ ê´€ë¦¬")
        
        if st.button(
            "ëª¨ë“  ë„ì‹œ ì¢Œí‘œ(Lat/Lon) ìë™ ì™„ì„±",
            help="target_cities_config.jsonì˜ ëª¨ë“  ë„ì‹œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¢Œí‘œê°€ ì—†ëŠ” ë„ì‹œì— ëŒ€í•´ geopyë¥¼ í˜¸ì¶œí•´ ì¢Œí‘œë¥¼ ìë™ ì €ì¥í•©ë‹ˆë‹¤.",
        ):
            success_count, fail_count = auto_fill_all_city_coordinates()

            if success_count == 0 and fail_count == 0:
                st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
            else:
                st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")
            st.rerun()

        # --- [ì‹ ê·œ 2] ë ---


        existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
        st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
        with st.form("add_target_city_form", clear_on_submit=True):
            col_a, col_b = st.columns(2)
            with col_a:
                region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
                region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
                new_region = ""
                if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
                    new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
            with col_b:
                trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

            col_c, col_d = st.columns(2)
            with col_c:
                city_name = st.text_input("ë„ì‹œ", key="add_city")
                neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
            with col_d:
                country_name = st.text_input("êµ­ê°€", key="add_country")
                hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

            with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
                substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
                substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

            add_submitted = st.form_submit_button("ì¶”ê°€")

        if add_submitted:
            region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
            if not region_value or not city_name.strip() or not country_name.strip():
                st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            else:
                current_entries = get_target_city_entries()
                canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
                duplicate_exists = any(
                    (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
                    for entry in current_entries
                )
                if duplicate_exists:
                    st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                else:
                    new_entry = {
                        "region": region_value,
                        "country": country_name.strip(),
                        "city": city_name.strip(),
                        "neighborhood": neighborhood.strip(),
                        "hotel_cluster": hotel_cluster.strip(),
                        "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
                    }
                    if substitute_city.strip() and substitute_country.strip():
                        new_entry["un_dsa_substitute"] = {
                            "city": substitute_city.strip(),
                            "country": substitute_country.strip(),
                        }
                    current_entries.append(new_entry)
                    set_target_city_entries(current_entries)
                    st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                    st.rerun()

        st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
        
        if current_entries:
            # ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
            selected_label = st.selectbox(
                "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
                sorted_labels, 
                key="edit_city_selector",
                on_change=_sync_edit_form_from_selection
            )

            # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
            if "edit_region" not in st.session_state:
                _sync_edit_form_from_selection()

            # í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
            with st.form("edit_target_city_form"):
                col_e, col_f = st.columns(2)
                with col_e:
                    region_edit = st.text_input("ì§€ì—­", key="edit_region")
                    city_edit = st.text_input("ë„ì‹œ", key="edit_city")
                    neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
                with col_f:
                    country_edit = st.text_input("êµ­ê°€", key="edit_country")
                    hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

                trip_lengths_edit = st.multiselect(
                    "ì¶œì¥ ê¸°ê°„",
                    TRIP_LENGTH_OPTIONS,
                    key="edit_trip_lengths", 
                )

                with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
                    sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
                    sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

                col_btn1, col_btn2 = st.columns(2)
                with col_btn1:
                    update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
                with col_btn2:
                    delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

            # ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
            if update_btn:
                if (not st.session_state.edit_region.strip() or 
                    not st.session_state.edit_city.strip() or 
                    not st.session_state.edit_country.strip()):
                    st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                else:
                    selected_idx = options[st.session_state.edit_city_selector]
                    current_entries[selected_idx] = {
                        "region": st.session_state.edit_region.strip(),
                        "country": st.session_state.edit_country.strip(),
                        "city": st.session_state.edit_city.strip(),
                        "neighborhood": st.session_state.edit_neighborhood.strip(),
                        "hotel_cluster": st.session_state.edit_hotel.strip(),
                        "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
                    }
                    if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
                        current_entries[selected_idx]["un_dsa_substitute"] = {
                            "city": st.session_state.edit_sub_city.strip(),
                            "country": st.session_state.edit_sub_country.strip(),
                        }
                    else:
                        current_entries[selected_idx].pop("un_dsa_substitute", None)

                    set_target_city_entries(current_entries)
                    st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
                    st.rerun()
            
            if delete_btn:
                selected_idx = options[st.session_state.edit_city_selector]
                del current_entries[selected_idx]
                set_target_city_entries(current_entries)
                st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                st.rerun()
        else:
            st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

        # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
        st.divider()
        st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

        if not MENU_CACHE_ENABLED:
            st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

            # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
            st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
            
            st.selectbox(
                "ë„ì‹œ ì„ íƒ (ìë™ ì±„ìš°ê¸°):", 
                sorted_labels,  # íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜
                key="cache_city_selector",
                on_change=_sync_cache_form_from_selection, # ìƒˆë¡œ ë§Œë“  ì½œë°±
                index=None,
                placeholder="ë„ì‹œë¥¼ ì„ íƒí•˜ë©´ êµ­ê°€, ë„ì‹œ, ì„¸ë¶€ ì§€ì—­ì´ ìë™ ì…ë ¥ë©ë‹ˆë‹¤."
            )

            # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ìºì‹œ í¼ ì´ˆê¸°í™”
            if "new_cache_country" not in st.session_state:
                _sync_cache_form_from_selection() # ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            
            # --- [v19.3 í•«í”½ìŠ¤] clear_on_submit=False, on_click ì½œë°± ì‚¬ìš© ---
            with st.form("add_menu_cache_form"): # clear_on_submit ì œê±°
                st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
                c1, c2 = st.columns(2)
                with c1:
                    new_cache_country = st.text_input("êµ­ê°€ (Country)", key="new_cache_country", help="ì˜ˆ: Philippines")
                    new_cache_city = st.text_input("ë„ì‹œ (City)", key="new_cache_city", help="ì˜ˆ: Manila")
                    new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", key="new_cache_neighborhood", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
                    new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", key="new_cache_vendor", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
                with c2:
                    new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"], key="new_cache_category")
                    new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01, key="new_cache_price")
                    new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", key="new_cache_currency", help="ì˜ˆ: PHP, USD")
                    new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)", key="new_cache_url")
                
                # [v19.3] on_click ì½œë°±ìœ¼ë¡œ ì €ì¥/ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
                add_cache_submitted = st.form_submit_button(
                    "ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥",
                    on_click=handle_cache_submit # <-- í•µì‹¬ ìˆ˜ì •
                )
            # --- [v19.3 í•«í”½ìŠ¤] ë ---

            # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
            st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
            all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
            
            if not all_cache_data:
                st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                df_cache = pd.DataFrame(all_cache_data)
                # [v19.3] ê²½ê³  ìˆ˜ì •
                st.dataframe(df_cache[[
                    "country", "city", "neighborhood", "vendor", 
                    "category", "price", "currency", "last_updated", "url"
                ]], width='stretch')

                # ì‚­ì œ ê¸°ëŠ¥
                st.markdown("---")
                st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
                
                delete_options_map = {
                    f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
                    for idx, entry in enumerate(reversed(all_cache_data))
                }
                delete_labels = list(delete_options_map.keys())
                
                label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
                
                if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
                    original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                    
                    entry_to_delete = all_cache_data.pop(original_list_index)
                    
                    if save_cached_menu_prices(all_cache_data):
                        st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
                    else:
                        st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # --- [ì‹ ê·œ 3] UI ë ---
    


# 2025-11-18 ìµœì¢… ì‹œì—° í”¼ë“œë°± ë°˜ì˜ ì „
# # 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
# import math  # NaN ì²´í¬ìš©
# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv

# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple
# from sqlalchemy import text


# import altair as alt  # [ì‹ ê·œ 2] ê³ ê¸‰ ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# import pydeck as pdk  # [ì‹ ê·œ 2] ê³ ê¸‰ 3D ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬

# # [ì‹ ê·œ 2(ì§€ë„)] ê´€ë ¨ ì„í¬íŠ¸
# from geopy.geocoders import Nominatim
# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable

# # --- [v20.0 DBì—°ë™] Streamlit Connection ì´ˆê¸°í™” ---
# # Supabase (Postgres) DBì— ì—°ê²°í•©ë‹ˆë‹¤.
# # ì´ ì—°ê²°ì€ Streamlitì˜ Secretsì—ì„œ "connections.supabase_db" ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
# conn = st.connection("supabase_db", type="sql", dialect="postgresql")


# from datetime import date, datetime

# def _json_default(obj):
#     """report_dataë¥¼ JSONìœ¼ë¡œ ì €ì¥í•  ë•Œ, ë‚ ì§œ/ì‹œê°„ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜."""
#     if isinstance(obj, (date, datetime)):
#         return obj.isoformat()
#     return obj  # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ ë™ì‘ (ì—ëŸ¬ ë‚˜ë©´ ê·¸ë•Œ í™•ì¸)

# # --- [v20.0 DBì—°ë™] data_sources.menu_cache.py ê¸°ëŠ¥ ëŒ€ì²´ ---
# # ì´ì œ 'menu_cache.py' íŒŒì¼ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# def load_cached_menu_prices(
#     city: str, 
#     country: str, 
#     neighborhood: Optional[str]
# ) -> List[Dict[str, Any]]:
#     """DBì—ì„œ íŠ¹ì • ìœ„ì¹˜ì˜ ë©”ë‰´ ê°€ê²© ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     city_lower = city.lower()
#     country_lower = country.lower()
    
#     query_base = "SELECT * FROM menu_cache WHERE LOWER(city) = :city AND LOWER(country) = :country"
#     params = {"city": city_lower, "country": country_lower}

#     if neighborhood:
#         # 1ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°
#         query_neighborhood = query_base + " AND LOWER(neighborhood) = :neighborhood"
#         params_neighborhood = params.copy()
#         params_neighborhood["neighborhood"] = neighborhood.lower().strip()
#         df_neighborhood = conn.query(query_neighborhood, params=params_neighborhood)
#         if not df_neighborhood.empty:
#             return df_neighborhood.to_dict('records')

#     # 2ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ë¹„ì–´ìˆëŠ” 'ë„ì‹œ ì „ì²´' ë°ì´í„°
#     query_city = query_base + " AND (neighborhood IS NULL OR neighborhood = '')"
#     df_city = conn.query(query_city, params=params)
#     return df_city.to_dict('records')

# def load_all_cache() -> List[Dict[str, Any]]:
#     """DBì—ì„œ *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     df = conn.query("SELECT * FROM menu_cache ORDER BY last_updated DESC, id DESC", ttl=5) # 5ì´ˆ ìºì‹œ
#     return df.to_dict('records')

# def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool:
#     """ìƒˆë¡œìš´ ìºì‹œ í•­ëª© 1ê°œë¥¼ DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
#     try:
#         new_entry["last_updated"] = datetime.now().date()
#         df_new = pd.DataFrame([new_entry])
#         # conn.insert(...) ëŒ€ì‹  ì§ì ‘ INSERT
#         _bulk_insert_dataframe("menu_cache", df_new)
#         return True
#     except Exception as e:
#         st.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
#         return False


# def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool:
#     """(ì‚­ì œ ì‹œ ì‚¬ìš©) *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
#     try:
#         # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
#         with conn.session as s:
#             s.execute(text("DELETE FROM menu_cache"))
#             s.commit()
            
#         # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
#         if all_samples:
#             df_new = pd.DataFrame(all_samples)
#             df_new = df_new.drop(columns=["id", "created_at"], errors='ignore')
#             _bulk_insert_dataframe("menu_cache", df_new)

#         return True
#     except Exception as e:
#         st.error(f"DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
#         return False


# MENU_CACHE_ENABLED = True # DBë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•­ìƒ í™œì„±í™”
# # --- [v20.0 DBì—°ë™] ë ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # --- [ê³µí†µ] Admin Access Code ì„¤ì • (ì „ì—­) ---
# ACCESS_CODE_KEY = "admin_access_code_valid"
# ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE")  # .envì—ì„œ ë¡œë“œ

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# # --- [v20.0 DBì—°ë™] Weight Config í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def _get_config_from_db(config_key: str, default_config: Dict[str, Any]) -> Dict[str, Any]:
#     """DBì˜ app_config í…Œì´ë¸”ì—ì„œ ì„¤ì •ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
#     try:
#         result = conn.query(
#             "SELECT config_value FROM app_config WHERE config_key = :key",
#             params={"key": config_key},
#             ttl=10 # 10ì´ˆ ìºì‹œ
#         )
#         if result.empty:
#             return default_config
        
#         db_value = result.iloc[0]["config_value"]
#         if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
#             return json.loads(db_value)
#         return db_value
#     except Exception:
#         return default_config

# def _save_config_to_db(config_key: str, config_value: Dict[str, Any]) -> None:
#     """DBì˜ app_config í…Œì´ë¸”ì— ì„¤ì •ì„ ì €ì¥(ì—…ë°ì´íŠ¸)í•©ë‹ˆë‹¤."""
#     try:
#         # PostgreSQLì˜ JSONB íƒ€ì…ì„ ìœ„í•´ json.dumps ì‚¬ìš©
#         config_json = json.dumps(config_value) 
        
#         # 'UPSERT' (Update or Insert) ì¿¼ë¦¬
#         with conn.session as s:
#             s.execute(text(
#                 f"""
#                 INSERT INTO app_config (config_key, config_value, last_updated)
#                 VALUES (:key, :value, :now)
#                 ON CONFLICT (config_key) 
#                 DO UPDATE SET config_value = :value, last_updated = :now
#                 """
#             ), params={"key": config_key, "value": config_json, "now": datetime.now()})
#             s.commit()
#     except Exception as e:
#         st.error(f"DB ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")

# def get_weight_config() -> Dict[str, float]:
#     """DBì—ì„œ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
#     config = _get_config_from_db("weight_config", DEFAULT_WEIGHT_CONFIG)
#     normalized = _normalize_weight_config(config)
    
#     # st.session_stateì—ë„ ì €ì¥ (ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜ì„± ìœ ì§€)
#     try:
#         st.session_state["weight_config"] = normalized
#     except Exception:
#         pass
#     return normalized

# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """DBì™€ ì„¸ì…˜ì˜ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = _normalize_weight_config(config)
#     _save_config_to_db("weight_config", normalized)
    
#     try:
#         st.session_state["weight_config"] = normalized
#     except Exception:
#         pass
#     return normalized
# # --- [v20.0 DBì—°ë™] ë ---
# def load_weight_config() -> Dict[str, float]:
#     """
#     (Backwards compatibility)
#     ì˜ˆì „ íŒŒì¼ ê¸°ë°˜ í•¨ìˆ˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì“°ë˜,
#     ì‹¤ì œ êµ¬í˜„ì€ DB ê¸°ë°˜ get_weight_config()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     """
#     return get_weight_config()



# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">Estimated Total Per Diem ({level_label})</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>Calculation</span><br/>
#             $ {daily:,} Ã— {days} days Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# # --- [v20.0 DBì—°ë™] UI Settings í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """DBì— UI ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤."""
#     normalized = _normalize_ui_settings(settings)
#     _save_config_to_db("ui_settings", normalized)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """DBì—ì„œ UI ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
    
#     config = _get_config_from_db("ui_settings", DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(config)
    
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)
# # --- [v20.0 DBì—°ë™] ë ---

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "Short-term", "Long-term": "Long-term"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
#     return entry


# # --- [v20.0 DBì—°ë™] Target Cities í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def load_target_city_entries() -> List[Dict[str, Any]]:
#     """DBì—ì„œ ëª¨ë“  ë„ì‹œ ëª©ë¡ì„ ë¡œë“œí•˜ë˜, ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±."""
#     df = conn.query("SELECT * FROM target_cities ORDER BY region, country, city", ttl=10)
#     if df.empty:
#         # DB ë¹„ì–´ ìˆìœ¼ë©´ ì½”ë“œì— ë°•í˜€ ìˆëŠ” DEFAULT_TARGET_CITY_ENTRIES ì‚¬ìš©
#         return [normalize_target_entry(e) for e in DEFAULT_TARGET_CITY_ENTRIES]
#     return df.to_dict('records')

# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     """(ìˆ˜ì •/ì‚­ì œ ì‹œ ì‚¬ìš©) ì „ì²´ ë„ì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
#     try:
#         # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
#         with conn.session as s:
#             s.execute(text("DELETE FROM target_cities"))
#             s.commit()
            
#         # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
#         if entries:
#             df_new = pd.DataFrame(entries)
#             df_new = df_new.drop(columns=["id", "created_at"], errors='ignore')

#             # ---------- âœ¨ ì—¬ê¸°ë¶€í„° ì¶”ê°€/ìˆ˜ì • ë¶€ë¶„ âœ¨ ----------

#             # NaN â†’ None ìœ¼ë¡œ í†µì¼ (íŠ¹íˆ un_dsa_substitute ì»¬ëŸ¼ì˜ NaN ë°©ì§€)
#             df_new = df_new.where(pd.notnull(df_new), None)

#             # un_dsa_substitute ì»¬ëŸ¼: dict/list â†’ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
#             if "un_dsa_substitute" in df_new.columns:
#                 def _normalize_sub(v):
#                     # dict/list ëŠ” json.dumps
#                     if isinstance(v, (dict, list)):
#                         return json.dumps(v)
#                     return v  # None ì´ë‚˜ ê¸°ì¡´ ë¬¸ìì—´ì€ ê·¸ëŒ€ë¡œ
#                 df_new["un_dsa_substitute"] = df_new["un_dsa_substitute"].apply(_normalize_sub)

#             # ---------- âœ¨ ì¶”ê°€/ìˆ˜ì • ë âœ¨ ----------

#             _bulk_insert_dataframe("target_cities", df_new)
#     except Exception as e:
#         st.error(f"DB ë„ì‹œ ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")


# def _bulk_insert_dataframe(table_name: str, df: pd.DataFrame) -> None:
#     """
#     Streamlit SQLConnection ì—ëŠ” insert() ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ,
#     session + raw SQL ë¡œ DataFrame ì„ ì¼ê´„ insert í•œë‹¤.
#     NaN ê°’ì€ ëª¨ë‘ None(NULL)ìœ¼ë¡œ ë³€í™˜í•´ì„œ íƒ€ì… ì˜¤ë¥˜ë¥¼ ë°©ì§€í•œë‹¤.
#     """
#     if df.empty:
#         return

#     cols = list(df.columns)
#     col_list = ", ".join(cols)
#     placeholders = ", ".join([f":{c}" for c in cols])
#     insert_sql = text(
#         f"INSERT INTO {table_name} ({col_list}) VALUES ({placeholders})"
#     )

#     with conn.session as s:
#         for _, row in df.iterrows():
#             raw_dict = row.to_dict()
#             clean_dict = {}

#             for k, v in raw_dict.items():
#                 # pandas ê°€ ë„£ì–´ì¤€ NaN(float) â†’ None ìœ¼ë¡œ ë³€í™˜
#                 if isinstance(v, float) and math.isnan(v):
#                     clean_dict[k] = None
#                 else:
#                     clean_dict[k] = v

#             s.execute(insert_sql, params=clean_dict)
#         s.commit()




# TARGET_CITIES_ENTRIES = load_target_city_entries() # ì•± ì‹œì‘ ì‹œ DBì—ì„œ ë¡œë“œ

# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     # DBì—ì„œ ë¡œë“œí•œ ìµœì‹ ë³¸ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
#     st.session_state["target_cities_entries"] = load_target_city_entries()
#     return st.session_state["target_cities_entries"]

# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     # 1. DBì— ì˜êµ¬ ì €ì¥
#     save_target_city_entries(entries) 
#     # 2. í˜„ì¬ ì„¸ì…˜ ìƒíƒœì—ë„ ì¦‰ì‹œ ë°˜ì˜
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
# # --- [v20.0 DBì—°ë™] ë ---

# def auto_fill_all_city_coordinates() -> tuple[int, int]:
#     """
#     target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
#     DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
#     í•œêµ­(Seoul/Busan/Incheon/Jeju Island) ë“±ì€ ë³„ì¹­ ë° ìˆ˜ë™ ì¢Œí‘œë¡œ ë³´ì™„.
#     """
#     try:
#         geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#     except Exception as e:
#         st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#         return 0, 0

#     current_entries = get_target_city_entries()
#     entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

#     if not entries_to_update:
#         return 0, 0

#     success_count = 0
#     fail_count = 0

#     progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

#     with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#         for i, entry in enumerate(entries_to_update):
#             city = entry["city"]
#             country = entry["country"]

#             # 1) country ë³„ì¹­ ì ìš© (ì˜ˆ: "Korea, Republic of" â†’ "South Korea")
#             country_for_query = GEOCODING_COUNTRY_ALIASES.get(country, country)

#             # 2) (city, country) ì¡°í•©ë³„ ì¿¼ë¦¬ override
#             key = (city.lower(), country_for_query.lower())
#             if key in GEOCODING_CITY_QUERY_OVERRIDES:
#                 query = GEOCODING_CITY_QUERY_OVERRIDES[key]
#             else:
#                 query = f"{city}, {country_for_query}"

#             try:
#                 location = geolocator.geocode(query, timeout=5)
#                 time.sleep(1)  # Nominatim rate limit

#                 if location:
#                     entry["lat"] = float(location.latitude)
#                     entry["lon"] = float(location.longitude)
#                     st.toast(
#                         f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})",
#                         icon="ğŸŒ",
#                     )
#                     success_count += 1
#                 else:
#                     # 3) Nominatim ì—ì„œ ëª» ì°¾ìœ¼ë©´ ìˆ˜ë™ ì¢Œí‘œ fallback ì‹œë„
#                     manual = MANUAL_COORDS.get((city, country))
#                     if manual:
#                         lat, lon = manual
#                         entry["lat"] = float(lat)
#                         entry["lon"] = float(lon)
#                         st.toast(
#                             f"âœ… ìˆ˜ë™ ì¢Œí‘œ ì‚¬ìš©: {city}, {country} ({lat:.4f}, {lon:.4f})",
#                             icon="ğŸ“Œ",
#                         )
#                         success_count += 1
#                     else:
#                         st.toast(
#                             f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
#                             icon="â“",
#                         )
#                         fail_count += 1

#             except (GeocoderTimedOut, GeocoderUnavailable):
#                 st.toast(
#                     f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.",
#                     icon="ğŸ”¥",
#                 )
#                 fail_count += 1
#             except Exception as e:
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                 fail_count += 1

#             progress_bar.progress(
#                 (i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}"
#             )

#     # DB + ì„¸ì…˜ì— ì €ì¥
#     set_target_city_entries(current_entries)
#     progress_bar.empty()
#     return success_count, fail_count




# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]


# # --- Geocoding ì „ìš© ë³„ì¹­ / ìˆ˜ë™ ì¢Œí‘œ ---

# GEOCODING_COUNTRY_ALIASES = {
#     # UN í‘œê¸° â†’ ì‹¤ì œ ì§€ì˜¤ì½”ë”©ìš© í‘œê¸°
#     "Korea, Republic of": "South Korea",
# }

# # íŠ¹ì • (city, country) ì¡°í•©ì— ëŒ€í•´, Nominatimì— ë˜ì§ˆ ì¿¼ë¦¬ë¥¼ ê°•ì œë¡œ ì§€ì •
# GEOCODING_CITY_QUERY_OVERRIDES = {
#     ("seoul", "south korea"): "Seoul, South Korea",
#     ("busan", "south korea"): "Busan, South Korea",
#     ("incheon", "south korea"): "Incheon, South Korea",
#     ("jeju island", "south korea"): "Jeju-do, South Korea",  # ì œì£¼
# }

# # ê·¸ë˜ë„ ëª» ì°¾ì•˜ì„ ë•Œ ì‚¬ìš©í•  ìˆ˜ë™ ì¢Œí‘œ (ë§ˆì§€ë§‰ ì•ˆì „ë§)
# MANUAL_COORDS = {
#     ("Seoul", "Korea, Republic of"): (37.5665, 126.9780),
#     ("Busan", "Korea, Republic of"): (35.1796, 129.0756),
#     ("Incheon", "Korea, Republic of"): (37.4563, 126.7052),
#     ("Jeju Island", "Korea, Republic of"): (33.4996, 126.5312),
# }


# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# # --- [v20.0 DBì—°ë™] Report History í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def get_history_files() -> List[str]:
#     """DBì—ì„œ ê³¼ê±° ë³´ê³ ì„œ 'ì´ë¦„' ëª©ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
#     df = conn.query("SELECT name FROM analysis_reports ORDER BY created_at DESC", ttl=10) # 10ì´ˆ ìºì‹œ
#     return df['name'].tolist()

# from sqlalchemy import text

# def save_report_data(data):
#     """ë¶„ì„ ê²°ê³¼ë¥¼ DB(JSONB)ì— ì €ì¥í•©ë‹ˆë‹¤."""
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"report_{timestamp}.json"

#     try:
#         report_json = json.dumps(data)

#         with conn.session as s:
#             s.execute(
#                 text("""
#                     INSERT INTO analysis_reports (name, report_data)
#                     VALUES (:name, :report_data)
#                 """),
#                 {"name": filename, "report_data": report_json},
#             )
#             s.commit()

#     except Exception as e:
#         st.error(f"DB ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")



# def load_report_data(filename):
#     """DBì—ì„œ íŠ¹ì • ë³´ê³ ì„œ(JSON)ë¥¼ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
#     try:
#         result = conn.query(
#             "SELECT report_data FROM analysis_reports WHERE name = :name",
#             params={"name": filename},
#             ttl=3600 # 1ì‹œê°„ ìºì‹œ
#         )
#         if result.empty:
#             return None
        
#         db_value = result.iloc[0]["report_data"]
        
#         if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
#             data = json.loads(db_value)
#         else: # ì´ë¯¸ dict/listë¡œ ë°˜í™˜ëœ ê²½ìš°
#             data = db_value
            
#         return _sanitize_report_data(data)
#     except Exception as e:
#         st.error(f"DB ë³´ê³ ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return None
# # --- [v20.0 DBì—°ë™] ë ---


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data




# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None

#         # --- [ìˆ˜ì •] un_dsa_substitute ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ ---
#         sub_raw = target.get("un_dsa_substitute")

#         sub_value = None
#         if isinstance(sub_raw, dict):
#             sub_value = sub_raw
#         elif isinstance(sub_raw, str) and sub_raw.strip():
#             # JSON ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ëŒ€ë¹„
#             try:
#                 sub_value = json.loads(sub_raw)
#             except Exception:
#                 sub_value = None

#         if (
#             isinstance(sub_value, dict)
#             and sub_value.get("city")
#             and sub_value.get("country")
#         ):
#             # ìœ íš¨í•œ ëŒ€ì²´ ë„ì‹œ ì •ë³´ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©
#             search_target = sub_value
#             is_substitute = True
#         else:
#             # ì—†ìœ¼ë©´ ì›ë˜ ë„ì‹œ ì‚¬ìš©
#             search_target = target
#             is_substitute = False
#         # --- [ìˆ˜ì • ë] ---

#         country_df = df[
#             df["Country"].str.contains(search_target["country"], case=False, na=False)
#         ]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- Streamlit UI Configuration ---
# st.set_page_config(
#     layout="wide",
#     initial_sidebar_state="collapsed"  # ğŸ‘ˆ ì‚¬ì´ë“œë°” ê¸°ë³¸ ì ‘í˜
# )
# st.title("NSUS GROUP Per Diem Calculation & Inquiry System")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility

# # --- [NEW] Global Admin Access + Tab Layout (v21.1) ---

# # .env ì— ADMIN_ACCESS_CODE ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì•± ì¤‘ë‹¨
# if not ACCESS_CODE_VALUE:
#     st.error(
#         "Security Error: 'ADMIN_ACCESS_CODE' is not set in the .env file. "
#         "Please stop the app and set the .env file."
#     )
#     st.stop()

# # í˜„ì¬ Admin ë¡œê·¸ì¸ ìƒíƒœ
# is_admin = bool(st.session_state.get(ACCESS_CODE_KEY, False))

# # 0) ì¢Œì¸¡ ì‚¬ì´ë“œë°”: ë¡œê·¸ì¸ / ë¡œê·¸ì•„ì›ƒ UI (ê¸°ë³¸ ë‹«í˜)
# with st.sidebar.expander("ğŸ” Admin Access", expanded=False):
#     if not is_admin:
#         code_input = st.text_input(
#             "Access Code",
#             type="password",
#             placeholder="Enter admin code",
#             key="sidebar_admin_code",
#         )
#         if st.button("Login", key="sidebar_admin_login_btn"):
#             if code_input == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted. Admin tabs unlocked.")
#                 st.rerun()
#             else:
#                 st.error("The Access Code is incorrect.")
#     else:
#         st.markdown("âœ… **Admin mode is active.**")
#         if st.button("Logout", key="sidebar_admin_logout_btn"):
#             # ê´€ë¦¬ì ì„¸ì…˜ í•´ì œ
#             st.session_state[ACCESS_CODE_KEY] = False
#             # ë¡œê·¸ì•„ì›ƒ ì‹œ ì§ì›ìš© íƒ­ì€ í•­ìƒ ë³´ì´ë„ë¡(Per Diem Inquiryë§Œ ë‚¨ë„ë¡)
#             st.session_state["employee_tab_visibility"] = True
#             st.success("You have been logged out.")
#             st.rerun()

# # 1) ìƒë‹¨ íƒ­ ë ˆì´ì•„ì›ƒ
# top_left, _ = st.columns([6, 2])

# # 2) ë©”ì¸ ì˜ì—­: íƒ­ ë ˆì´ì•„ì›ƒ
# #   - employee_tab_visible, is_admin ì€ ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
# tab_definitions = []

# # (1) ì§ì›ìš© íƒ­ â€“ ê¸°ë³¸ ì²« íƒ­
# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ Per Diem Inquiry (Employee)")

# # (2) Admin íƒ­ â€“ ë¡œê·¸ì¸ ìƒíƒœì—ì„œë§Œ í‘œì‹œ
# if is_admin:
#     tab_definitions.append("ğŸ“ˆ Report Analysis (Admin)")
#     tab_definitions.append("ğŸ› ï¸ System Settings (Admin)")
#     tab_definitions.append("ğŸ“Š Executive Dashboard (Admin)")

# # íƒ­ì´ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
# if not tab_definitions:
#     tab_definitions.append("ğŸ”’ Admin Login")

# tabs = st.tabs(tab_definitions)

# # íƒ­ ì¸ë±ìŠ¤ ë§¤í•‘
# employee_tab = admin_analysis_tab = admin_config_tab = dashboard_tab = None
# idx = 0

# if employee_tab_visible:
#     employee_tab = tabs[idx]
#     idx += 1

# if is_admin:
#     admin_analysis_tab = tabs[idx]; idx += 1
#     admin_config_tab   = tabs[idx]; idx += 1
#     dashboard_tab      = tabs[idx]; idx += 1

# # ì´ ì•„ë˜ë¶€í„°ëŠ” employee_tab / admin_analysis_tab / admin_config_tab / dashboard_tab ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©


# # ì´ ì•„ë˜ë¶€í„°ëŠ” employee_tab / admin_analysis_tab / admin_config_tab / dashboard_tab ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©


# # --- [End of modification] ---
# if dashboard_tab is not None:
#     with dashboard_tab:
#         st.header("Global Cost Dashboard")
#         st.info("Visualizes the global business trip cost status based on the latest report data.")

#         try:
#             alt.theme.enable("streamlit")
#         except Exception:
#             pass 

#         history_files = get_history_files()
#         if not history_files:
#             st.warning("No data to display. Please run AI analysis at least once in the 'Report Analysis' tab.")
#         else:
#             latest_report_file = history_files[0]
#             st.subheader(f"Reference Report: `{latest_report_file}`")
          
#             report_data = load_report_data(latest_report_file)
#             if not report_data or 'cities' not in report_data:
#                 st.error("Failed to load data.")
#             else:
#                 # --- ì¢Œí‘œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼ (System Settings ì™€ ë™ì¼ ê¸°ëŠ¥) ---
#                 if st.button("ì¢Œí‘œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°", key="reload_coords_from_dashboard"):
#                     success_count, fail_count = auto_fill_all_city_coordinates()
#                     if success_count == 0 and fail_count == 0:
#                         st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
#                     else:
#                         st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")

#                 # í•­ìƒ ìµœì‹  target_cities ë¥¼ ë‹¤ì‹œ ë¡œë“œ
#                 config_entries = get_target_city_entries()
#                 if not config_entries:
#                     st.error("Failed to load target cities.")
#                 else:
#                     # 1. Report + Target Cities ë¨¸ì§€
#                     df_report = pd.DataFrame(report_data['cities'])
#                     df_config = pd.DataFrame(config_entries)

#                     df_merged = pd.merge(
#                         df_report,
#                         df_config,
#                         left_on=["city", "country_display"],
#                         right_on=["city", "country"],
#                         suffixes=("_report", "_config")
#                     )

#                     # 2. ì§€ë„ìš© ë°ì´í„°: target_cities ì˜ lat/lon ì‚¬ìš©
#                     if not {'lat', 'lon', 'final_allowance'}.issubset(df_merged.columns):
#                         map_data = pd.DataFrame(columns=["city", "country", "lat", "lon", "final_allowance"])
#                     else:
#                         map_data = df_merged[["city", "country", "lat", "lon", "final_allowance"]].copy()
#                         map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
#                         map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
#                         map_data['final_allowance'] = pd.to_numeric(map_data['final_allowance'], errors='coerce')
#                         map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)

#                     if map_data.empty:
#                         st.warning(
#                             "ì´ ë¦¬í¬íŠ¸ì— ë§¤ì¹­ë˜ëŠ” ë„ì‹œ ì¢Œí‘œ(lat/lon)ê°€ ì—†ìŠµë‹ˆë‹¤. "
#                             "ìœ„ì˜ 'ì¢Œí‘œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ target_cities ì¢Œí‘œë¥¼ ìë™ ì™„ì„±í•œ ë’¤, "
#                             "ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•´ì£¼ì„¸ìš”."
#                         )
#                         st.caption("No data to display on the map. (Check if coordinates were generated.)")
#                     else:
#                         # 3. Pydeck ì§€ë„ ë Œë”ë§
#                         min_cost = map_data['final_allowance'].min()
#                         max_cost = map_data['final_allowance'].max()
#                         range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

#                         def get_color_and_size(cost):
#                             norm_cost = (cost - min_cost) / range_cost
#                             r = int(255 * norm_cost)
#                             g = int(255 * (1 - norm_cost))
#                             b = 0
#                             size = 50000 + (norm_cost * 450000)
#                             return [r, g, b, 160], size

#                         color_size = map_data['final_allowance'].apply(get_color_and_size)
#                         map_data['color'] = [item[0] for item in color_size]
#                         map_data['size'] = [item[1] for item in color_size]

#                         view_state = pdk.ViewState(
#                             latitude=map_data['lat'].mean(),
#                             longitude=map_data['lon'].mean(),
#                             zoom=0.5,
#                             pitch=0,
#                             bearing=0
#                         )

#                         layer = pdk.Layer(
#                             'ScatterplotLayer',
#                             data=map_data,
#                             get_position='[lon, lat]',
#                             get_color='color',
#                             get_radius='size',
#                             pickable=True,
#                             opacity=0.8,
#                             stroked=True,
#                             filled=True,
#                             radius_scale=0.5,
#                             get_line_color=[255, 255, 255, 100],
#                             get_line_width=10000,
#                         )

#                         tooltip = {
#                             "html": "<b>{city}, {country}</b><br/>"
#                                     "Final Allowance: <b>${final_allowance}</b>",
#                             "style": { "color": "white", "backgroundColor": "#1e3c72" }
#                         }
                        
#                         map_col, legend_col = st.columns([4, 1])

#                         with map_col:
#                             st.pydeck_chart(
#                                 pdk.Deck(
#                                     layers=[layer],
#                                     initial_view_state=view_state,
#                                     tooltip=tooltip
#                                 ),
#                                 use_container_width=True
#                             )

#                         with legend_col:
#                             st.write("##### Legend (Cost)")
#                             st.markdown(f"""
#                             <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                                 <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                                 <span style="margin-left: 10px;">High Cost (~${max_cost:,.0f})</span>
#                             </div>
#                             <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                                 <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                                 <span style="margin-left: 10px;">Medium Cost</span>
#                             </div>
#                             <div style="display: flex; align-items: center;">
#                                 <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                                 <span style="margin-left: 10px;">Low Cost (~${min_cost:,.0f})</span>
#                             </div>
#                             """, unsafe_allow_html=True)
#                             st.caption("The larger the circle and the redder the color, the higher the cost of the city.")

#                     # --- ì´í•˜ ê¸°ì¡´ Top 10 ì°¨íŠ¸ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ ---
#                     st.divider()
#                     col1, col2 = st.columns(2)
                    
#                     if 'final_allowance' in df_merged.columns:
#                         with col1:
#                             st.write("##### ğŸ’° Top 10 High Cost Cities (AI Final)")
#                             top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                            
#                             average_cost = df_merged['final_allowance'].mean()
#                             top_10_cost_df['average'] = average_cost
                            
#                             base_cost = alt.Chart(top_10_cost_df).encode(
#                                 x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                                 y=alt.Y('final_allowance', title="Final Allowance ($)", axis=alt.Axis(format='$,.0f')),
#                                 tooltip=[
#                                     alt.Tooltip('city', title="City"),
#                                     alt.Tooltip('final_allowance', title="Final Allowance ($)", format='$,.0f'),
#                                     alt.Tooltip('average', title="Overall Average", format='$,.0f')
#                                 ]
#                             )
                            
#                             bars_cost = base_cost.mark_bar().encode()
                            
#                             rule_cost = alt.Chart(pd.DataFrame({'average_cost': [average_cost]})).mark_rule(
#                                 strokeDash=[3, 3]
#                             ).encode(
#                                 y=alt.Y('average_cost', title=''),
#                                 tooltip=[alt.Tooltip('average_cost', title="Overall Average", format='$,.0f')] 
#                             )
                            
#                             chart_cost = (bars_cost + rule_cost).properties(
#                                 background='transparent',
#                                 title=f"Overall Average: ${average_cost:,.0f}" 
#                             ).interactive()
#                             st.altair_chart(chart_cost, use_container_width=True)
                        
#                         with col2:
#                             st.write("##### âš ï¸ Top 10 High Volatility Cities (AI Confidence)")
#                             df_report_vc = pd.DataFrame(report_data['cities'])
#                             df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
#                             df_report_vc.dropna(subset=['vc'], inplace=True)
                            
#                             if df_report_vc.empty:
#                                 st.info("Volatility (VC) data is missing. (AI analysis with the latest version is required)")
#                             else:
#                                 top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
#                                 average_vc = df_report_vc['vc'].mean()
#                                 top_10_vc_df['average'] = average_vc
                                
#                                 base_vc = alt.Chart(top_10_vc_df).encode(
#                                     x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                                     y=alt.Y('vc', title="Variation Coefficient (VC)", axis=alt.Axis(format='%')),
#                                     tooltip=[
#                                         alt.Tooltip('city', title="City"),
#                                         alt.Tooltip('vc', title="Variation Coefficient (VC)", format='.2%'),
#                                         alt.Tooltip('average', title="Overall Average", format='.2%')
#                                     ]
#                                 )
                                
#                                 bars_vc = base_vc.mark_bar().encode()
                                
#                                 rule_vc = alt.Chart(pd.DataFrame({'average_vc': [average_vc]})).mark_rule(
#                                     strokeDash=[3, 3]
#                                 ).encode(
#                                     y=alt.Y('average_vc', title=''),
#                                     tooltip=[alt.Tooltip('average_vc', title="Overall Average", format='.2%')]
#                                 )
                                
#                                 chart_vc = (bars_vc + rule_vc).properties(
#                                     background='transparent',
#                                     title=f"Overall Average: {average_vc:.2%}"
#                                 ).interactive()
#                                 st.altair_chart(chart_vc, use_container_width=True)
#                                 st.caption("The higher the volatility (VC), the less confident the AI is in its price estimation for the city.")
#                     else:
#                         st.warning("No 'final_allowance' data to display the chart.")


# if employee_tab is not None:
#     with employee_tab:
#         st.header("Per Diem Inquiry by City")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("Please analyze a PDF in the 'Report Analysis' tab first.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("Country:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("City:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("There are no registered cities for the selected country.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "Trip Start Date",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "Trip End Date",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("Job Level:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("The end date must be on or after the start date.")
#                     trip_days = 0 # Set to 0
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"Auto-classified trip type: {trip_term_label} Â· {trip_days}-day trip")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("No city data available for this period. Adjust the trip type to 'Short-term' or check city settings.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} Results")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [New 2] Employee tab total card
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} Daily Recommended Per Diem", "No Amount")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**Basis of Calculation (Daily Rate)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"Success {ai_runs}/{ai_attempts} runs"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"Outliers {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"Season {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "No AI Data"
                        
#                         # [New 1] Reason for applying dynamic weights
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | Reason: {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"Short-term base $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"Short-term $ {un_base:,} â†’ Long-term $ {un_display:,}"
#                                         render_stat_card("UN-DSA Basis", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"Short-term base $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"Short-term $ {ai_avg:,} â†’ Long-term $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI Market Estimate (Seasonal Adj.)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"Short-term $ {final_allowance:,} â†’ Long-term $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("Weighted Average Result", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [New 2] Detailed cost breakdown (merged with show_ai_market_detail logic)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # line break
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # Apply long-term/seasonal rates
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI Estimate Details (Daily Rate)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("Est. Food", f"$ {food_display:,}", f"Short-term base: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("Est. Transport", f"$ {trans_display:,}", f"Short-term base: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("Est. Misc", f"$ {misc_display:,}", f"Short-term base: $ {round(mean_misc)}", "muted")
                        
#                         # [Improvement 3] The show_weighted_result card is redundant, so the block below is removed
#                         # (Original second_row_keys logic removed)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("The administrator has hidden the detailed calculation basis.")

# # --- Admin: Report Analysis íƒ­ ì „ì²´ ì½”ë“œ ---
# if admin_analysis_tab is not None:
#     with admin_analysis_tab:
#         # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ì‚¬ì´ë“œë°”ì—ì„œ Admin ë¡œê·¸ì¸ ê²€ì¦ì´ ëë‚œ ìƒíƒœë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
#         st.subheader("Report Version Management")

#         history_files = get_history_files()

#         # --- [ì„¹ì…˜ 1] í™œì„± ë¦¬í¬íŠ¸ ë²„ì „ ì„ íƒ ---
#         st.subheader("Report Version Management")
#         if history_files:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             default_index = history_files.index(st.session_state["selected_report_file"])
#             selected_file = st.selectbox(
#                 "Select the active report version:",
#                 history_files,
#                 index=default_index,
#                 key="admin_report_file_select",
#             )
#             st.session_state["selected_report_file"] = selected_file
#         else:
#             st.info("No reports have been generated.")

#         # --- [ì„¹ì…˜ 2] ê³¼ê±° ë¦¬í¬íŠ¸ ë¹„êµ ---
#         st.divider()
#         st.subheader("Compare Past Reports")
#         if len(history_files) < 2:
#             st.info("At least 2 reports are required for comparison.")
#         else:
#             col_a, col_b = st.columns(2)
#             with col_a:
#                 file_a = st.selectbox(
#                     "Base Report (A)",
#                     history_files,
#                     index=1 if len(history_files) > 1 else 0,
#                     key="compare_a",
#                 )
#             with col_b:
#                 file_b = st.selectbox(
#                     "Comparison Report (B)",
#                     history_files,
#                     index=0,
#                     key="compare_b",
#                 )

#             if st.button("Compare Reports"):
#                 if file_a == file_b:
#                     st.warning("You must select two different reports.")
#                 else:
#                     with st.spinner("Comparing reports..."):
#                         data_a = load_report_data(file_a)
#                         data_b = load_report_data(file_b)

#                         if (
#                             data_a
#                             and data_b
#                             and "cities" in data_a
#                             and "cities" in data_b
#                         ):
#                             df_a = pd.DataFrame(data_a["cities"])[
#                                 ["city", "country_display", "final_allowance"]
#                             ]
#                             df_b = pd.DataFrame(data_b["cities"])[
#                                 ["city", "country_display", "final_allowance"]
#                             ]

#                             df_merged = pd.merge(
#                                 df_a,
#                                 df_b,
#                                 on=["city", "country_display"],
#                                 suffixes=("_A", "_B"),
#                             )

#                             report_a_label = (
#                                 file_a.split("report_")[-1].split(".")[0]
#                             )
#                             report_b_label = (
#                                 file_b.split("report_")[-1].split(".")[0]
#                             )

#                             df_merged[f"A ({report_a_label})"] = df_merged[
#                                 "final_allowance_A"
#                             ]
#                             df_merged[f"B ({report_b_label})"] = df_merged[
#                                 "final_allowance_B"
#                             ]

#                             df_merged["Change ($)"] = (
#                                 df_merged["final_allowance_B"]
#                                 - df_merged["final_allowance_A"]
#                             )

#                             # Prevent division by zero
#                             df_merged["Change (%)"] = (
#                                 df_merged["Change ($)"]
#                                 / df_merged["final_allowance_A"].replace(
#                                     0, pd.NA
#                                 )
#                                 * 100
#                             )

#                             st.dataframe(
#                                 df_merged[
#                                     [
#                                         "city",
#                                         "country_display",
#                                         f"A ({report_a_label})",
#                                         f"B ({report_b_label})",
#                                         "Change ($)",
#                                         "Change (%)",
#                                     ]
#                                 ]
#                                 .style.format(
#                                     {
#                                         "Change (%)": "{:,.1f}%",
#                                         "Change ($)": "{:,.0f}",
#                                     }
#                                 ),
#                                 width="stretch",
#                             )
#                         else:
#                             st.error("Failed to load report files.")

#         # --- [ì„¹ì…˜ 3] UN-DSA PDF ë¶„ì„ + AI ì‹¤í–‰ ---
#         st.divider()
#         st.subheader("UN-DSA (PDF) Analysis & AI Execution")
#         st.warning(
#             f"Note that the AI will be called {NUM_AI_CALLS} times, which will consume time and cost. "
#             "(Improvement 1: Async processing for faster speed)"
#         )
#         uploaded_file = st.file_uploader(
#             "Upload UN-DSA PDF file.", type="pdf"
#         )

#         # --- Async AI analysis ì‹¤í–‰ ë¡œì§ ---
#         if uploaded_file and st.button("Run AI Analysis", type="primary"):
#             openai_api_key = os.getenv("OPENAI_API_KEY")
#             if not openai_api_key:
#                 st.error("Please set OPENAI_API_KEY in the .env file.")
#             else:
#                 st.session_state.latest_analysis_result = None

#                 async def run_analysis(progress_bar, openai_api_key):
#                     # 1) PDF â†’ Text
#                     progress_bar.progress(0, text="Extracting PDF text...")
#                     full_text = parse_pdf_to_text(uploaded_file)

#                     # 2) Text â†’ TSV (OpenAI)
#                     CHUNK_SIZE = 15000
#                     text_chunks = [
#                         full_text[i : i + CHUNK_SIZE]
#                         for i in range(0, len(full_text), CHUNK_SIZE)
#                     ]
#                     all_tsv_lines = []
#                     analysis_failed = False

#                     for i, chunk in enumerate(text_chunks):
#                         progress_bar.progress(
#                             i / (len(text_chunks) + 1),
#                             text=f"AI PDF->TSV converting... ({i+1}/{len(text_chunks)})",
#                         )
#                         chunk_tsv = call_openai_for_tsv_conversion(
#                             chunk, openai_api_key
#                         )
#                         if chunk_tsv:
#                             lines = chunk_tsv.strip().split("\n")
#                             if not all_tsv_lines:
#                                 all_tsv_lines.extend(lines)
#                             else:
#                                 all_tsv_lines.extend(lines[1:])
#                         else:
#                             analysis_failed = True
#                             break

#                     if analysis_failed:
#                         st.error("Failed to convert PDF->TSV.")
#                         progress_bar.empty()
#                         return

#                     processed_data = process_tsv_data(
#                         "\n".join(all_tsv_lines)
#                     )
#                     if not processed_data:
#                         st.error("Failed to process TSV data.")
#                         progress_bar.empty()
#                         return

#                     # 3) ë¦¬í¬íŠ¸ ë‚´ ë„ì‹œ ì¢Œí‘œ(lat/lon) ìƒì„± í›„ ì €ì¥ (ì§€ë„ìš©)
#                     try:
#                         geolocator = Nominatim(
#                             user_agent=f"aicp_report_map_{random.randint(1000,9999)}"
#                         )
#                     except Exception as e:
#                         st.warning(f"ì§€ë„ ì¢Œí‘œìš© geopy ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#                     else:
#                         with st.spinner(
#                             "ì§€ë„ìš© ë„ì‹œ ì¢Œí‘œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
#                         ):
#                             for city in processed_data.get("cities", []):
#                                 if city.get("lat") and city.get("lon"):
#                                     continue

#                                 city_name = city.get("city")
#                                 country_name = city.get("country_display")
#                                 if not city_name or not country_name:
#                                     continue

#                                 query = f"{city_name}, {country_name}"
#                                 try:
#                                     location = geolocator.geocode(
#                                         query, timeout=5
#                                     )
#                                     time.sleep(1)  # Nominatim rate limit
#                                     if location:
#                                         city["lat"] = float(
#                                             location.latitude
#                                         )
#                                         city["lon"] = float(
#                                             location.longitude
#                                         )
#                                 except Exception:
#                                     # ì¢Œí‘œë¥¼ ëª» ì°¾ì•„ë„ ì „ì²´ ë¶„ì„ì€ ê³„ì† ì§„í–‰
#                                     continue

#                     # 4) Async OpenAI Client ìƒì„±
#                     client = openai.AsyncOpenAI(api_key=openai_api_key)

#                     total_cities = len(processed_data["cities"])
#                     all_tasks = []  # [ [City1-10runs], [City2-10runs], ... ]

#                     # 5) ë„ì‹œë³„ AI í˜¸ì¶œ Task ì¤€ë¹„
#                     for city_data in processed_data["cities"]:
#                         city_name, country_name = (
#                             city_data["city"],
#                             city_data["country_display"],
#                         )
#                         city_context = {
#                             "neighborhood": city_data.get("neighborhood"),
#                             "hotel_cluster": city_data.get("hotel_cluster"),
#                         }
#                         season_context = city_data.get(
#                             "season_context"
#                         ) or get_current_season_info(
#                             city_name, country_name
#                         )
#                         menu_samples = load_cached_menu_prices(
#                             city_name,
#                             country_name,
#                             city_context.get("neighborhood"),
#                         )

#                         city_data["menu_samples"] = menu_samples
#                         city_data["reference_links"] = (
#                             build_reference_link_lines(
#                                 menu_samples, max_items=8
#                             )
#                         )

#                         city_tasks = []
#                         for j in range(1, NUM_AI_CALLS + 1):
#                             task = get_market_data_from_ai_async(
#                                 client,
#                                 city_name,
#                                 country_name,
#                                 f"Run {j}",
#                                 context=city_context,
#                                 season_context=season_context,
#                                 menu_samples=menu_samples,
#                             )
#                             city_tasks.append(task)

#                         all_tasks.append(city_tasks)

#                     # 6) ë„ì‹œë³„ë¡œ 10ê°œ Task ë™ì‹œ ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬
#                     city_index = 0
#                     for city_tasks in all_tasks:
#                         city_data = processed_data["cities"][city_index]
#                         city_name = city_data["city"]
#                         progress_text = (
#                             f"Calculating AI estimates... "
#                             f"({city_index+1}/{total_cities}) {city_name}"
#                         )
#                         progress_bar.progress(
#                             (city_index + 1) / max(total_cities, 1),
#                             text=progress_text,
#                         )

#                         try:
#                             market_results = await asyncio.gather(
#                                 *city_tasks
#                             )
#                         except Exception as e:
#                             st.error(
#                                 f"Async error during {city_name} analysis: {e}"
#                             )
#                             market_results = []

#                         ai_totals_source: List[int] = []
#                         ai_meta_runs: List[Dict[str, Any]] = []

#                         ai_food: List[int] = []
#                         ai_transport: List[int] = []
#                         ai_misc: List[int] = []

#                         for j, market_result in enumerate(
#                             market_results, 1
#                         ):
#                             city_data[f"market_data_{j}"] = market_result
#                             if (
#                                 market_result.get("status") == "ok"
#                                 and market_result.get("total") is not None
#                             ):
#                                 ai_totals_source.append(
#                                     market_result["total"]
#                                 )
#                                 ai_food.append(market_result.get("food", 0))
#                                 ai_transport.append(
#                                     market_result.get("transport", 0)
#                                 )
#                                 ai_misc.append(market_result.get("misc", 0))

#                             if "meta" in market_result:
#                                 ai_meta_runs.append(market_result["meta"])

#                         city_data["ai_provenance"] = ai_meta_runs

#                         # ìµœì¢… Per Diem ê³„ì‚°
#                         final_allowance = None
#                         un_per_diem_raw = (
#                             city_data.get("un", {}).get(
#                                 "per_diem_excl_lodging"
#                             )
#                         )
#                         un_per_diem = (
#                             float(un_per_diem_raw)
#                             if isinstance(un_per_diem_raw, (int, float))
#                             else None
#                         )

#                         ai_stats = aggregate_ai_totals(ai_totals_source)
#                         season_factor = (season_context or {}).get(
#                             "factor", 1.0
#                         )
#                         ai_base_mean = ai_stats.get("mean_raw")
#                         ai_season_adjusted = (
#                             ai_base_mean * season_factor
#                             if ai_base_mean is not None
#                             else None
#                         )

#                         admin_weights = get_weight_config()
#                         ai_vc_score = ai_stats.get("variation_coeff")

#                         if un_per_diem is not None:
#                             weights_cfg = get_dynamic_weights(
#                                 ai_vc_score, admin_weights
#                             )
#                         else:
#                             # UN ë°ì´í„° ì—†ìœ¼ë©´ AI 100%
#                             weights_cfg = {
#                                 "un_weight": 0.0,
#                                 "ai_weight": 1.0,
#                                 "source": "AI Only (UN-DSA Missing)",
#                             }

#                         city_data["ai_summary"] = {
#                             "raw_totals": ai_totals_source,
#                             "used_totals": ai_stats.get("used_values", []),
#                             "removed_totals": ai_stats.get(
#                                 "removed_values", []
#                             ),
#                             "mean_base": ai_base_mean,
#                             "mean_base_rounded": ai_stats.get("mean"),
#                             "ai_consistency_vc": ai_vc_score,
#                             "mean_food": mean(ai_food) if ai_food else 0,
#                             "mean_transport": mean(ai_transport)
#                             if ai_transport
#                             else 0,
#                             "mean_misc": mean(ai_misc) if ai_misc else 0,
#                             "season_factor": season_factor,
#                             "season_label": (season_context or {}).get(
#                                 "label"
#                             ),
#                             "season_adjusted_mean_raw": ai_season_adjusted,
#                             "season_adjusted_mean_rounded": round(
#                                 ai_season_adjusted
#                             )
#                             if ai_season_adjusted is not None
#                             else None,
#                             "successful_runs": len(
#                                 ai_stats.get("used_values", [])
#                             ),
#                             "attempted_runs": NUM_AI_CALLS,
#                             "reference_links": city_data.get(
#                                 "reference_links", []
#                             ),
#                             "weighted_average_components": {
#                                 "un_per_diem": un_per_diem,
#                                 "ai_season_adjusted": ai_season_adjusted,
#                                 "weights": weights_cfg,
#                             },
#                         }

#                         # ë™ì  ê°€ì¤‘ì¹˜ ì ìš©
#                         if (
#                             un_per_diem is not None
#                             and ai_season_adjusted is not None
#                         ):
#                             weighted_average = (
#                                 un_per_diem * weights_cfg["un_weight"]
#                                 + ai_season_adjusted * weights_cfg["ai_weight"]
#                             )
#                             final_allowance = round(weighted_average)
#                         elif un_per_diem is not None:
#                             final_allowance = round(un_per_diem)
#                         elif ai_season_adjusted is not None:
#                             final_allowance = round(ai_season_adjusted)

#                         city_data["final_allowance"] = final_allowance

#                         if (
#                             final_allowance
#                             and un_per_diem
#                             and un_per_diem > 0
#                         ):
#                             city_data["delta_vs_un_pct"] = round(
#                                 (
#                                     (final_allowance - un_per_diem)
#                                     / un_per_diem
#                                 )
#                                 * 100
#                             )
#                         else:
#                             city_data["delta_vs_un_pct"] = "N/A"

#                         city_index += 1

#                     save_report_data(processed_data)
#                     st.session_state.latest_analysis_result = processed_data
#                     st.success("AI analysis completed.")
#                     progress_bar.empty()
#                     st.rerun()

#                 # ì‹¤ì œ ì‹¤í–‰
#                 with st.spinner(
#                     "Processing PDF and running AI analysis... (Takes approx. 10-30 seconds)"
#                 ):
#                     progress_bar = st.progress(
#                         0, text="Starting analysis..."
#                     )
#                     asyncio.run(run_analysis(progress_bar, openai_api_key))

#         # --- [ì„¹ì…˜ 4] Latest Analysis Summary í…Œì´ë¸” ---
#         if st.session_state.latest_analysis_result:
#             st.markdown("---")
#             st.subheader("Latest Analysis Summary")
#             df_data = []
#             for city in st.session_state.latest_analysis_result["cities"]:
#                 row = {
#                     "City": city.get("city", "N/A"),
#                     "Country": city.get("country_display", "N/A"),
#                     "UN-DSA": city.get("un", {}).get(
#                         "per_diem_excl_lodging"
#                     ),
#                 }
#                 for j in range(1, NUM_AI_CALLS + 1):
#                     row[f"AI {j}"] = city.get(
#                         f"market_data_{j}", {}
#                     ).get("total")

#                 delta_val = city.get("delta_vs_un_pct")
#                 if isinstance(delta_val, (int, float)):
#                     delta_display = f"{delta_val:.0f}%"
#                 else:
#                     delta_display = "N/A"

#                 row.update(
#                     {
#                         "Final Allowance": city.get("final_allowance"),
#                         "Delta (%)": delta_display,
#                         "Trip Lengths": DEFAULT_TRIP_LENGTH[0],
#                         "Notes": city.get("notes", ""),
#                     }
#                 )
#                 df_data.append(row)

#             st.dataframe(
#                 pd.DataFrame(df_data), use_container_width=True
#             )
#             with st.expander("View generated markdown report"):
#                 st.markdown(
#                     generate_markdown_report(
#                         st.session_state.latest_analysis_result
#                     )
#                 )



# def auto_fill_all_city_coordinates() -> tuple[int, int]:
#     """
#     target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
#     DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
#     """
#     try:
#         geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#     except Exception as e:
#         st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#         return 0, 0

#     current_entries = get_target_city_entries()
#     entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

#     if not entries_to_update:
#         return 0, 0

#     success_count = 0
#     fail_count = 0

#     progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

#     with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#         for i, entry in enumerate(entries_to_update):
#             city = entry["city"]
#             country = entry["country"]
#             query = f"{city}, {country}"

#             try:
#                 location = geolocator.geocode(query, timeout=5)
#                 time.sleep(1)  # Nominatim rate limit

#                 if location:
#                     entry["lat"] = location.latitude
#                     entry["lon"] = location.longitude
#                     st.toast(f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
#                     success_count += 1
#                 else:
#                     st.toast(f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="â“")
#                     fail_count += 1
#             except (GeocoderTimedOut, GeocoderUnavailable):
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", icon="ğŸ”¥")
#                 fail_count += 1
#             except Exception as e:
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                 fail_count += 1

#             progress_bar.progress((i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}")

#     # DB + ì„¸ì…˜ì— ì €ì¥
#     set_target_city_entries(current_entries)
#     progress_bar.empty()
#     return success_count, fail_count


# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# if admin_config_tab is not None:
#     with admin_config_tab:
#         # ì´ë¯¸ Admin íƒ­ì´ ë³´ì¸ ì‹œì ì—ì„œ Access Code ê²€ì¦ ì™„ë£Œ ìƒíƒœ
#         # ë°”ë¡œ ì„¤ì • UI ë Œë”ë§
#         current_entries = get_target_city_entries()
#         options = {
#             f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#             for idx, entry in enumerate(current_entries)
#         }
#         sorted_labels = list(options.keys())
#         # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#         if not st.session_state.get(ACCESS_CODE_KEY, False):
#             st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤. 'ë³´ê³ ì„œ ë¶„ì„ (Admin)' íƒ­ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
#             st.stop()
            
#         # --- [v19.3] ë„ì‹œ í¸ì§‘/ìºì‹œ ê´€ë¦¬ê°€ ê³µìœ í•  ë„ì‹œ ëª©ë¡ì„ íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜ ---
#         current_entries = get_target_city_entries()
#         options = {
#             f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#             for idx, entry in enumerate(current_entries)
#         }
#         sorted_labels = list(options.keys())
        
#         # --- ì½œë°± í•¨ìˆ˜ 1: 'ë„ì‹œ í¸ì§‘' í¼ ë™ê¸°í™” ---
#         def _sync_edit_form_from_selection():
#             if "edit_city_selector" not in st.session_state or not st.session_state.edit_city_selector:
#                 # st.session_state.edit_city_selectorê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì¼ ë•Œ
#                 if sorted_labels:
#                     st.session_state.edit_city_selector = sorted_labels[0]
#                 else:
#                     return # ë„ì‹œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                
#             selected_idx = options[st.session_state.edit_city_selector]
#             selected_entry = current_entries[selected_idx]
            
#             st.session_state.edit_region = selected_entry.get("region", "")
#             st.session_state.edit_city = selected_entry.get("city", "")
#             st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#             st.session_state.edit_country = selected_entry.get("country", "")
#             st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
            
#             existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#             st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
            
#             sub_data = selected_entry.get("un_dsa_substitute") or {}
#             st.session_state.edit_sub_city = sub_data.get("city", "")
#             st.session_state.edit_sub_country = sub_data.get("country", "")

#         # --- [v19.3] ì½œë°± í•¨ìˆ˜ 2: 'ìºì‹œ ì¶”ê°€' í¼ ë™ê¸°í™” ---
#         def _sync_cache_form_from_selection():
#             selected_label = st.session_state.get("cache_city_selector") # get()ìœ¼ë¡œ ì˜¤ë¥˜ ë°©ì§€
            
#             if selected_label in options: # 'options' dictë¥¼ ê³µìœ 
#                 selected_idx = options[selected_label]
#                 selected_entry = current_entries[selected_idx]
#                 st.session_state.new_cache_country = selected_entry.get("country", "")
#                 st.session_state.new_cache_city = selected_entry.get("city", "")
#                 st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
#             else: # (placeholder ì„ íƒ ì‹œ)
#                 st.session_state.new_cache_country = ""
#                 st.session_state.new_cache_city = ""
#                 st.session_state.new_cache_neighborhood = ""
            
#             # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” í•­ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
#             st.session_state.new_cache_vendor = ""
#             st.session_state.new_cache_category = "Food"
#             st.session_state.new_cache_price = 0.0
#             st.session_state.new_cache_currency = "USD"
#             st.session_state.new_cache_url = ""

#         # --- [v19.3 í•«í”½ìŠ¤] ì½œë°± í•¨ìˆ˜ 3: 'ìºì‹œ ì €ì¥' ë¡œì§ ---
#         def handle_cache_submit():
#             # 1. ìœ íš¨ì„± ê²€ì‚¬
#             if (not st.session_state.new_cache_country or 
#                 not st.session_state.new_cache_city or 
#                 not st.session_state.new_cache_vendor):
#                 st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#                 return # ì—¬ê¸°ì„œ ì¤‘ë‹¨ (í¼ ê°’ ìœ ì§€ë¨)

#             # 2. ìƒˆ í•­ëª© ìƒì„±
#             new_entry = {
#                 "country": st.session_state.new_cache_country.strip(),
#                 "city": st.session_state.new_cache_city.strip(),
#                 "neighborhood": st.session_state.new_cache_neighborhood.strip(),
#                 "vendor": st.session_state.new_cache_vendor.strip(),
#                 "category": st.session_state.new_cache_category,
#                 "price": st.session_state.new_cache_price,
#                 "currency": st.session_state.new_cache_currency.strip().upper(),
#                 "url": st.session_state.new_cache_url.strip(),
#             }
            
#             # 3. íŒŒì¼ì— ì €ì¥
#             if add_menu_cache_entry(new_entry):
#                 st.success(f"'{new_entry['vendor']}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                
#                 # 4. (ì¤‘ìš”) í¼ ë¦¬ì…‹: session_state ê°’ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”
#                 # ì´ ë¡œì§ì€ on_click ì½œë°± ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
#                 st.session_state.new_cache_country = ""
#                 st.session_state.new_cache_city = ""
#                 st.session_state.new_cache_neighborhood = ""
#                 st.session_state.new_cache_vendor = ""
#                 st.session_state.new_cache_category = "Food"
#                 st.session_state.new_cache_price = 0.0
#                 st.session_state.new_cache_currency = "USD"
#                 st.session_state.new_cache_url = ""
#                 st.session_state.cache_city_selector = None # ë“œë¡­ë‹¤ìš´ë„ ë¦¬ì…‹
                
#                 # st.rerun()ì€ on_click ì½œë°±ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•  í•„ìš” ì—†ìŒ
#             else:
#                 st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#         # --- [v19.3 í•«í”½ìŠ¤] ë ---

#         st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#         visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#         if visibility_toggle != stored_employee_tab_visible:
#             updated_settings = dict(ui_settings)
#             updated_settings["show_employee_tab"] = visibility_toggle
#             updated_settings["employee_sections"] = employee_sections_visibility
#             save_ui_settings(updated_settings)
#             ui_settings = updated_settings
#             st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#             st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")
#             time.sleep(1) # ìœ ì €ê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤Œ
#             st.rerun()

#         st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#         section_toggle_values: Dict[str, bool] = {}
#         for section_key, label in EMPLOYEE_SECTION_LABELS:
#             current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#             section_toggle_values[section_key] = st.toggle(
#                 label,
#                 value=current_value,
#                 key=f"employee_section_toggle_{section_key}",
#             )
#         if section_toggle_values != employee_sections_visibility:
#             updated_settings = dict(ui_settings)
#             updated_settings["employee_sections"] = section_toggle_values
#             save_ui_settings(updated_settings)
#             ui_settings["employee_sections"] = section_toggle_values
#             st.session_state.employee_sections_visibility = section_toggle_values
#             employee_sections_visibility = section_toggle_values
#             st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
#             time.sleep(1)
#             st.rerun()

#         st.divider()
#         st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#         st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#         current_weights = get_weight_config()
#         st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#         with st.form("weight_config_form"):
#             un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#             ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#             st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#             st.caption("Weights are normalised to sum to 1.0 when saved.")
#             weight_submit = st.form_submit_button("Save weights")
#         if weight_submit:
#             updated = update_weight_config(un_weight_input, ai_weight_preview)
#             st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#             st.rerun()

#         st.divider()
#         st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬ (target_cities_config.json)")
#         entries_df = pd.DataFrame(get_target_city_entries())
#         if not entries_df.empty:
#             entries_display = entries_df.copy()
#             # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#             entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#             st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch') # [v19.3] ê²½ê³  ìˆ˜ì •
#         else:
#             st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#         # --- [ì‹ ê·œ 2] ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„± ê¸°ëŠ¥ (ìƒˆë¡œ ì¶”ê°€) ---
#         st.divider()
#         st.subheader("ë„ì‹œ ì¢Œí‘œ ê´€ë¦¬")
        
#         if st.button(
#             "ëª¨ë“  ë„ì‹œ ì¢Œí‘œ(Lat/Lon) ìë™ ì™„ì„±",
#             help="target_cities_config.jsonì˜ ëª¨ë“  ë„ì‹œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¢Œí‘œê°€ ì—†ëŠ” ë„ì‹œì— ëŒ€í•´ geopyë¥¼ í˜¸ì¶œí•´ ì¢Œí‘œë¥¼ ìë™ ì €ì¥í•©ë‹ˆë‹¤.",
#         ):
#             success_count, fail_count = auto_fill_all_city_coordinates()

#             if success_count == 0 and fail_count == 0:
#                 st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
#             else:
#                 st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")
#             st.rerun()

#         # --- [ì‹ ê·œ 2] ë ---


#         existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#         st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#         with st.form("add_target_city_form", clear_on_submit=True):
#             col_a, col_b = st.columns(2)
#             with col_a:
#                 region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#                 region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#                 new_region = ""
#                 if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                     new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#             with col_b:
#                 trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#             col_c, col_d = st.columns(2)
#             with col_c:
#                 city_name = st.text_input("ë„ì‹œ", key="add_city")
#                 neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#             with col_d:
#                 country_name = st.text_input("êµ­ê°€", key="add_country")
#                 hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#                 substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#             add_submitted = st.form_submit_button("ì¶”ê°€")

#         if add_submitted:
#             region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#             if not region_value or not city_name.strip() or not country_name.strip():
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 current_entries = get_target_city_entries()
#                 canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#                 duplicate_exists = any(
#                     (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                     for entry in current_entries
#                 )
#                 if duplicate_exists:
#                     st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#                 else:
#                     new_entry = {
#                         "region": region_value,
#                         "country": country_name.strip(),
#                         "city": city_name.strip(),
#                         "neighborhood": neighborhood.strip(),
#                         "hotel_cluster": hotel_cluster.strip(),
#                         "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                     }
#                     if substitute_city.strip() and substitute_country.strip():
#                         new_entry["un_dsa_substitute"] = {
#                             "city": substitute_city.strip(),
#                             "country": substitute_country.strip(),
#                         }
#                     current_entries.append(new_entry)
#                     set_target_city_entries(current_entries)
#                     st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()

#         st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
        
#         if current_entries:
#             # ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#             selected_label = st.selectbox(
#                 "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#                 sorted_labels, 
#                 key="edit_city_selector",
#                 on_change=_sync_edit_form_from_selection
#             )

#             # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#             if "edit_region" not in st.session_state:
#                 _sync_edit_form_from_selection()

#             # í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#             with st.form("edit_target_city_form"):
#                 col_e, col_f = st.columns(2)
#                 with col_e:
#                     region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                     city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                     neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#                 with col_f:
#                     country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                     hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#                 trip_lengths_edit = st.multiselect(
#                     "ì¶œì¥ ê¸°ê°„",
#                     TRIP_LENGTH_OPTIONS,
#                     key="edit_trip_lengths", 
#                 )

#                 with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                     sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                     sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#                 col_btn1, col_btn2 = st.columns(2)
#                 with col_btn1:
#                     update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#                 with col_btn2:
#                     delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#             # ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#             if update_btn:
#                 if (not st.session_state.edit_region.strip() or 
#                     not st.session_state.edit_city.strip() or 
#                     not st.session_state.edit_country.strip()):
#                     st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#                 else:
#                     selected_idx = options[st.session_state.edit_city_selector]
#                     current_entries[selected_idx] = {
#                         "region": st.session_state.edit_region.strip(),
#                         "country": st.session_state.edit_country.strip(),
#                         "city": st.session_state.edit_city.strip(),
#                         "neighborhood": st.session_state.edit_neighborhood.strip(),
#                         "hotel_cluster": st.session_state.edit_hotel.strip(),
#                         "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                     }
#                     if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                         current_entries[selected_idx]["un_dsa_substitute"] = {
#                             "city": st.session_state.edit_sub_city.strip(),
#                             "country": st.session_state.edit_sub_country.strip(),
#                         }
#                     else:
#                         current_entries[selected_idx].pop("un_dsa_substitute", None)

#                     set_target_city_entries(current_entries)
#                     st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
            
#             if delete_btn:
#                 selected_idx = options[st.session_state.edit_city_selector]
#                 del current_entries[selected_idx]
#                 set_target_city_entries(current_entries)
#                 st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()
#         else:
#             st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#         # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#         st.divider()
#         st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#         if not MENU_CACHE_ENABLED:
#             st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#             # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#             st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
            
#             st.selectbox(
#                 "ë„ì‹œ ì„ íƒ (ìë™ ì±„ìš°ê¸°):", 
#                 sorted_labels,  # íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜
#                 key="cache_city_selector",
#                 on_change=_sync_cache_form_from_selection, # ìƒˆë¡œ ë§Œë“  ì½œë°±
#                 index=None,
#                 placeholder="ë„ì‹œë¥¼ ì„ íƒí•˜ë©´ êµ­ê°€, ë„ì‹œ, ì„¸ë¶€ ì§€ì—­ì´ ìë™ ì…ë ¥ë©ë‹ˆë‹¤."
#             )

#             # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ìºì‹œ í¼ ì´ˆê¸°í™”
#             if "new_cache_country" not in st.session_state:
#                 _sync_cache_form_from_selection() # ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            
#             # --- [v19.3 í•«í”½ìŠ¤] clear_on_submit=False, on_click ì½œë°± ì‚¬ìš© ---
#             with st.form("add_menu_cache_form"): # clear_on_submit ì œê±°
#                 st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#                 c1, c2 = st.columns(2)
#                 with c1:
#                     new_cache_country = st.text_input("êµ­ê°€ (Country)", key="new_cache_country", help="ì˜ˆ: Philippines")
#                     new_cache_city = st.text_input("ë„ì‹œ (City)", key="new_cache_city", help="ì˜ˆ: Manila")
#                     new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", key="new_cache_neighborhood", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                     new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", key="new_cache_vendor", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#                 with c2:
#                     new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"], key="new_cache_category")
#                     new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01, key="new_cache_price")
#                     new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", key="new_cache_currency", help="ì˜ˆ: PHP, USD")
#                     new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)", key="new_cache_url")
                
#                 # [v19.3] on_click ì½œë°±ìœ¼ë¡œ ì €ì¥/ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
#                 add_cache_submitted = st.form_submit_button(
#                     "ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥",
#                     on_click=handle_cache_submit # <-- í•µì‹¬ ìˆ˜ì •
#                 )
#             # --- [v19.3 í•«í”½ìŠ¤] ë ---

#             # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#             st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#             all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
            
#             if not all_cache_data:
#                 st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#             else:
#                 df_cache = pd.DataFrame(all_cache_data)
#                 # [v19.3] ê²½ê³  ìˆ˜ì •
#                 st.dataframe(df_cache[[
#                     "country", "city", "neighborhood", "vendor", 
#                     "category", "price", "currency", "last_updated", "url"
#                 ]], width='stretch')

#                 # ì‚­ì œ ê¸°ëŠ¥
#                 st.markdown("---")
#                 st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
                
#                 delete_options_map = {
#                     f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                     for idx, entry in enumerate(reversed(all_cache_data))
#                 }
#                 delete_labels = list(delete_options_map.keys())
                
#                 label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
                
#                 if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                     original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                    
#                     entry_to_delete = all_cache_data.pop(original_list_index)
                    
#                     if save_cached_menu_prices(all_cache_data):
#                         st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                         st.rerun()
#                     else:
#                         st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ì‹ ê·œ 3] UI ë ---
    


# 2025-11-17
# # 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple
# from sqlalchemy import text


# import altair as alt  # [ì‹ ê·œ 2] ê³ ê¸‰ ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# import pydeck as pdk  # [ì‹ ê·œ 2] ê³ ê¸‰ 3D ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬

# # [ì‹ ê·œ 2(ì§€ë„)] ê´€ë ¨ ì„í¬íŠ¸
# from geopy.geocoders import Nominatim
# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable

# # --- [v20.0 DBì—°ë™] Streamlit Connection ì´ˆê¸°í™” ---
# # Supabase (Postgres) DBì— ì—°ê²°í•©ë‹ˆë‹¤.
# # ì´ ì—°ê²°ì€ Streamlitì˜ Secretsì—ì„œ "connections.supabase_db" ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
# conn = st.connection("supabase_db", type="sql", dialect="postgresql")


# from datetime import date, datetime

# def _json_default(obj):
#     """report_dataë¥¼ JSONìœ¼ë¡œ ì €ì¥í•  ë•Œ, ë‚ ì§œ/ì‹œê°„ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜."""
#     if isinstance(obj, (date, datetime)):
#         return obj.isoformat()
#     return obj  # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ ë™ì‘ (ì—ëŸ¬ ë‚˜ë©´ ê·¸ë•Œ í™•ì¸)

# # --- [v20.0 DBì—°ë™] data_sources.menu_cache.py ê¸°ëŠ¥ ëŒ€ì²´ ---
# # ì´ì œ 'menu_cache.py' íŒŒì¼ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# def load_cached_menu_prices(
#     city: str, 
#     country: str, 
#     neighborhood: Optional[str]
# ) -> List[Dict[str, Any]]:
#     """DBì—ì„œ íŠ¹ì • ìœ„ì¹˜ì˜ ë©”ë‰´ ê°€ê²© ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     city_lower = city.lower()
#     country_lower = country.lower()
    
#     query_base = "SELECT * FROM menu_cache WHERE LOWER(city) = :city AND LOWER(country) = :country"
#     params = {"city": city_lower, "country": country_lower}

#     if neighborhood:
#         # 1ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°
#         query_neighborhood = query_base + " AND LOWER(neighborhood) = :neighborhood"
#         params_neighborhood = params.copy()
#         params_neighborhood["neighborhood"] = neighborhood.lower().strip()
#         df_neighborhood = conn.query(query_neighborhood, params=params_neighborhood)
#         if not df_neighborhood.empty:
#             return df_neighborhood.to_dict('records')

#     # 2ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ë¹„ì–´ìˆëŠ” 'ë„ì‹œ ì „ì²´' ë°ì´í„°
#     query_city = query_base + " AND (neighborhood IS NULL OR neighborhood = '')"
#     df_city = conn.query(query_city, params=params)
#     return df_city.to_dict('records')

# def load_all_cache() -> List[Dict[str, Any]]:
#     """DBì—ì„œ *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     df = conn.query("SELECT * FROM menu_cache ORDER BY last_updated DESC, id DESC", ttl=5) # 5ì´ˆ ìºì‹œ
#     return df.to_dict('records')

# def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool:
#     """ìƒˆë¡œìš´ ìºì‹œ í•­ëª© 1ê°œë¥¼ DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
#     try:
#         new_entry["last_updated"] = datetime.now().date()
#         # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ insert (to_dict('records')ì™€ í˜•ì‹ì„ ë§ì¶¤)
#         df_new = pd.DataFrame([new_entry]) 
#         conn.insert("menu_cache", df_new)
#         return True
#     except Exception as e:
#         st.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
#         return False

# def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool:
#     """(ì‚­ì œ ì‹œ ì‚¬ìš©) *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
#     try:
#         # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
#         with conn.session as s:
#             s.execute(text("DELETE FROM menu_cache"))
#             s.commit()
            
#         # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
#         if all_samples:
#             df_new = pd.DataFrame(all_samples)
#             # id, created_at ë“± ìë™ ìƒì„± ì»¬ëŸ¼ì€ DataFrameì— ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì™¸
#             df_new = df_new.drop(columns=["id", "created_at"], errors='ignore') 
#             conn.insert("menu_cache", df_new)
#         return True
#     except Exception as e:
#         st.error(f"DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
#         return False

# MENU_CACHE_ENABLED = True # DBë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•­ìƒ í™œì„±í™”
# # --- [v20.0 DBì—°ë™] ë ---

# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# # --- [v20.0 DBì—°ë™] Weight Config í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def _get_config_from_db(config_key: str, default_config: Dict[str, Any]) -> Dict[str, Any]:
#     """DBì˜ app_config í…Œì´ë¸”ì—ì„œ ì„¤ì •ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
#     try:
#         result = conn.query(
#             "SELECT config_value FROM app_config WHERE config_key = :key",
#             params={"key": config_key},
#             ttl=10 # 10ì´ˆ ìºì‹œ
#         )
#         if result.empty:
#             return default_config
        
#         db_value = result.iloc[0]["config_value"]
#         if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
#             return json.loads(db_value)
#         return db_value
#     except Exception:
#         return default_config

# def _save_config_to_db(config_key: str, config_value: Dict[str, Any]) -> None:
#     """DBì˜ app_config í…Œì´ë¸”ì— ì„¤ì •ì„ ì €ì¥(ì—…ë°ì´íŠ¸)í•©ë‹ˆë‹¤."""
#     try:
#         # PostgreSQLì˜ JSONB íƒ€ì…ì„ ìœ„í•´ json.dumps ì‚¬ìš©
#         config_json = json.dumps(config_value) 
        
#         # 'UPSERT' (Update or Insert) ì¿¼ë¦¬
#         with conn.session as s:
#             s.execute(text(
#                 f"""
#                 INSERT INTO app_config (config_key, config_value, last_updated)
#                 VALUES (:key, :value, :now)
#                 ON CONFLICT (config_key) 
#                 DO UPDATE SET config_value = :value, last_updated = :now
#                 """
#             ), params={"key": config_key, "value": config_json, "now": datetime.now()})
#             s.commit()
#     except Exception as e:
#         st.error(f"DB ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")

# def get_weight_config() -> Dict[str, float]:
#     """DBì—ì„œ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
#     config = _get_config_from_db("weight_config", DEFAULT_WEIGHT_CONFIG)
#     normalized = _normalize_weight_config(config)
    
#     # st.session_stateì—ë„ ì €ì¥ (ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜ì„± ìœ ì§€)
#     try:
#         st.session_state["weight_config"] = normalized
#     except Exception:
#         pass
#     return normalized

# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """DBì™€ ì„¸ì…˜ì˜ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = _normalize_weight_config(config)
#     _save_config_to_db("weight_config", normalized)
    
#     try:
#         st.session_state["weight_config"] = normalized
#     except Exception:
#         pass
#     return normalized
# # --- [v20.0 DBì—°ë™] ë ---
# def load_weight_config() -> Dict[str, float]:
#     """
#     (Backwards compatibility)
#     ì˜ˆì „ íŒŒì¼ ê¸°ë°˜ í•¨ìˆ˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì“°ë˜,
#     ì‹¤ì œ êµ¬í˜„ì€ DB ê¸°ë°˜ get_weight_config()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     """
#     return get_weight_config()



# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">Estimated Total Per Diem ({level_label})</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>Calculation</span><br/>
#             $ {daily:,} Ã— {days} days Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# # --- [v20.0 DBì—°ë™] UI Settings í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """DBì— UI ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤."""
#     normalized = _normalize_ui_settings(settings)
#     _save_config_to_db("ui_settings", normalized)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """DBì—ì„œ UI ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
    
#     config = _get_config_from_db("ui_settings", DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(config)
    
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)
# # --- [v20.0 DBì—°ë™] ë ---

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "Short-term", "Long-term": "Long-term"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
#     return entry


# # --- [v20.0 DBì—°ë™] Target Cities í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def load_target_city_entries() -> List[Dict[str, Any]]:
#     """DBì—ì„œ ëª¨ë“  ë„ì‹œ ëª©ë¡ì„ ë¡œë“œí•˜ë˜, ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±."""
#     df = conn.query("SELECT * FROM target_cities ORDER BY region, country, city", ttl=10)
#     if df.empty:
#         # DB ë¹„ì–´ ìˆìœ¼ë©´ ì½”ë“œì— ë°•í˜€ ìˆëŠ” DEFAULT_TARGET_CITY_ENTRIES ì‚¬ìš©
#         return [normalize_target_entry(e) for e in DEFAULT_TARGET_CITY_ENTRIES]
#     return df.to_dict('records')

# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     """(ìˆ˜ì •/ì‚­ì œ ì‹œ ì‚¬ìš©) ì „ì²´ ë„ì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
#     try:
#         # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
#         with conn.session as s:
#             s.execute(text("DELETE FROM target_cities"))
#             s.commit()
            
#         # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
#         if entries:
#             df_new = pd.DataFrame(entries)
#             # DBê°€ ìë™ ìƒì„±í•˜ëŠ” 'id', 'created_at'ì€ ì‚½ì… ì‹œ ì œì™¸
#             df_new = df_new.drop(columns=["id", "created_at"], errors='ignore') 
#             conn.insert("target_cities", df_new)
#     except Exception as e:
#         st.error(f"DB ë„ì‹œ ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")

# TARGET_CITIES_ENTRIES = load_target_city_entries() # ì•± ì‹œì‘ ì‹œ DBì—ì„œ ë¡œë“œ

# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     # DBì—ì„œ ë¡œë“œí•œ ìµœì‹ ë³¸ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
#     st.session_state["target_cities_entries"] = load_target_city_entries()
#     return st.session_state["target_cities_entries"]

# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     # 1. DBì— ì˜êµ¬ ì €ì¥
#     save_target_city_entries(entries) 
#     # 2. í˜„ì¬ ì„¸ì…˜ ìƒíƒœì—ë„ ì¦‰ì‹œ ë°˜ì˜
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
# # --- [v20.0 DBì—°ë™] ë ---

# def auto_fill_all_city_coordinates() -> tuple[int, int]:
#     """
#     target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
#     DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
#     """
#     try:
#         geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#     except Exception as e:
#         st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#         return 0, 0

#     current_entries = get_target_city_entries()
#     entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

#     if not entries_to_update:
#         return 0, 0

#     success_count = 0
#     fail_count = 0

#     progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

#     with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#         for i, entry in enumerate(entries_to_update):
#             city = entry["city"]
#             country = entry["country"]
#             query = f"{city}, {country}"

#             try:
#                 location = geolocator.geocode(query, timeout=5)
#                 time.sleep(1)  # Nominatim rate limit

#                 if location:
#                     entry["lat"] = location.latitude
#                     entry["lon"] = location.longitude
#                     st.toast(
#                         f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})",
#                         icon="ğŸŒ",
#                     )
#                     success_count += 1
#                 else:
#                     st.toast(
#                         f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
#                         icon="â“",
#                     )
#                     fail_count += 1
#             except (GeocoderTimedOut, GeocoderUnavailable):
#                 st.toast(
#                     f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.",
#                     icon="ğŸ”¥",
#                 )
#                 fail_count += 1
#             except Exception as e:
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                 fail_count += 1

#             progress_bar.progress(
#                 (i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}"
#             )

#     # DB + ì„¸ì…˜ì— ì €ì¥
#     set_target_city_entries(current_entries)
#     progress_bar.empty()
#     return success_count, fail_count



# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# # --- [v20.0 DBì—°ë™] Report History í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def get_history_files() -> List[str]:
#     """DBì—ì„œ ê³¼ê±° ë³´ê³ ì„œ 'ì´ë¦„' ëª©ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
#     df = conn.query("SELECT name FROM analysis_reports ORDER BY created_at DESC", ttl=10) # 10ì´ˆ ìºì‹œ
#     return df['name'].tolist()

# from sqlalchemy import text

# def save_report_data(data):
#     """ë¶„ì„ ê²°ê³¼ë¥¼ DB(JSONB)ì— ì €ì¥í•©ë‹ˆë‹¤."""
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"report_{timestamp}.json"

#     try:
#         report_json = json.dumps(data)

#         with conn.session as s:
#             s.execute(
#                 text("""
#                     INSERT INTO analysis_reports (name, report_data)
#                     VALUES (:name, :report_data)
#                 """),
#                 {"name": filename, "report_data": report_json},
#             )
#             s.commit()

#     except Exception as e:
#         st.error(f"DB ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")



# def load_report_data(filename):
#     """DBì—ì„œ íŠ¹ì • ë³´ê³ ì„œ(JSON)ë¥¼ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
#     try:
#         result = conn.query(
#             "SELECT report_data FROM analysis_reports WHERE name = :name",
#             params={"name": filename},
#             ttl=3600 # 1ì‹œê°„ ìºì‹œ
#         )
#         if result.empty:
#             return None
        
#         db_value = result.iloc[0]["report_data"]
        
#         if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
#             data = json.loads(db_value)
#         else: # ì´ë¯¸ dict/listë¡œ ë°˜í™˜ëœ ê²½ìš°
#             data = db_value
            
#         return _sanitize_report_data(data)
#     except Exception as e:
#         st.error(f"DB ë³´ê³ ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return None
# # --- [v20.0 DBì—°ë™] ë ---


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data




# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None

#         # --- [ìˆ˜ì •] un_dsa_substitute ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ ---
#         sub_raw = target.get("un_dsa_substitute")

#         sub_value = None
#         if isinstance(sub_raw, dict):
#             sub_value = sub_raw
#         elif isinstance(sub_raw, str) and sub_raw.strip():
#             # JSON ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ëŒ€ë¹„
#             try:
#                 sub_value = json.loads(sub_raw)
#             except Exception:
#                 sub_value = None

#         if (
#             isinstance(sub_value, dict)
#             and sub_value.get("city")
#             and sub_value.get("country")
#         ):
#             # ìœ íš¨í•œ ëŒ€ì²´ ë„ì‹œ ì •ë³´ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©
#             search_target = sub_value
#             is_substitute = True
#         else:
#             # ì—†ìœ¼ë©´ ì›ë˜ ë„ì‹œ ì‚¬ìš©
#             search_target = target
#             is_substitute = False
#         # --- [ìˆ˜ì • ë] ---

#         country_df = df[
#             df["Country"].str.contains(search_target["country"], case=False, na=False)
#         ]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- Streamlit UI Configuration ---
# st.set_page_config(layout="wide")
# st.title("AICP: NSUS GROUP Per Diem Calculation & Inquiry System")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [Improvement 3 & New 2] Tab structure change (v18.0) ---
# tab_definitions = [
#     "ğŸ“Š Executive Dashboard", # [New 2] Dashboard tab added
# ]

# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ Per Diem Inquiry (Employee)")

# # Split admin tab into two
# tab_definitions.append("ğŸ“ˆ Report Analysis (Admin)")
# tab_definitions.append("ğŸ› ï¸ System Settings (Admin)")

# tabs = st.tabs(tab_definitions)

# # Assign tab variables
# dashboard_tab = tabs[0]
# tab_index_offset = 1

# if employee_tab_visible:
#     employee_tab = tabs[tab_index_offset]
#     admin_analysis_tab = tabs[tab_index_offset + 1]
#     admin_config_tab = tabs[tab_index_offset + 2]
#     tab_index_offset += 1
# else:
#     employee_tab = None
#     admin_analysis_tab = tabs[tab_index_offset]
#     admin_config_tab = tabs[tab_index_offset + 1]

# # --- [End of modification] ---

# with dashboard_tab:
#     st.header("Global Cost Dashboard")
#     st.info("Visualizes the global business trip cost status based on the latest report data.")

#     try:
#         alt.theme.enable("streamlit")
#     except Exception:
#         pass 

#     history_files = get_history_files()
#     if not history_files:
#         st.warning("No data to display. Please run AI analysis at least once in the 'Report Analysis' tab.")
#     else:
#         latest_report_file = history_files[0]
#         st.subheader(f"Reference Report: `{latest_report_file}`")
        
#         report_data = load_report_data(latest_report_file)
#         config_entries = get_target_city_entries()
        
#         if not report_data or 'cities' not in report_data or not config_entries:
#             st.error("Failed to load data.")
#         else:
#             # 1. Prepare DataFrame (Report + Coordinates)
#             df_report = pd.DataFrame(report_data['cities'])
#             df_config = pd.DataFrame(config_entries)
            
#             df_merged = pd.merge(
#                 df_report,
#                 df_config,
#                 left_on=["city", "country_display"],
#                 right_on=["city", "country"],
#                 suffixes=("_report", "_config")
#             )
            
#             required_map_cols = ['city', 'country', 'lat', 'lon', 'final_allowance']

#             # ë¨¸ì§€ í›„ ì–´ëŠ ì»¬ëŸ¼ì— lat/lon ì´ ë“¤ì–´ìˆëŠ”ì§€ ì°¾ì•„ì„œ ì‚¬ìš©
#             lat_candidates = [c for c in ["lat", "lat_config", "lat_report"] if c in df_merged.columns]
#             lon_candidates = [c for c in ["lon", "lon_config", "lon_report"] if c in df_merged.columns]

#             if not lat_candidates or not lon_candidates:
#                 st.warning(
#                     "Coordinate (lat/lon) data for the map is missing. ğŸ—ºï¸\n\n"
#                     "ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ëª¨ë“  ë„ì‹œ ì¢Œí‘œë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•œ ë’¤ ì§€ë„ë¥¼ ë‹¤ì‹œ ê·¸ë¦½ë‹ˆë‹¤."
#                 )

#                 if st.button("ëª¨ë“  ë„ì‹œ ì¢Œí‘œ ìë™ ìƒì„± (Admin)", key="auto_fill_coords_from_dashboard"):
#                     success_count, fail_count = auto_fill_all_city_coordinates()
#                     if success_count == 0 and fail_count == 0:
#                         st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#                     else:
#                         st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")
#                     st.rerun()

#                 map_data = pd.DataFrame(columns=required_map_cols)
#             else:
#                 # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ì´ë¦„ì„ lat/lon ìœ¼ë¡œ í†µì¼í•´ì„œ ì‚¬ìš©
#                 lat_col = lat_candidates[0]
#                 lon_col = lon_candidates[0]

#                 map_data = df_merged.copy()
#                 map_data = map_data.rename(columns={lat_col: "lat", lon_col: "lon"})
#                 map_data = map_data[required_map_cols]
#                 map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)
#                 map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
#                 map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
#                 map_data.dropna(subset=['lat', 'lon'], inplace=True)


#             if map_data.empty:
#                 st.caption("No data to display on the map. (Check if coordinates were generated.)")
#             else:
#                 # 2. Calculate color (R,G,B) and size based on cost
#                 min_cost = map_data['final_allowance'].min()
#                 max_cost = map_data['final_allowance'].max()
#                 range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

#                 def get_color_and_size(cost):
#                     norm_cost = (cost - min_cost) / range_cost
#                     r = int(255 * norm_cost)
#                     g = int(255 * (1 - norm_cost))
#                     b = 0
#                     size = 50000 + (norm_cost * 450000)
#                     return [r, g, b, 160], size

#                 color_size = map_data['final_allowance'].apply(get_color_and_size)
#                 map_data['color'] = [item[0] for item in color_size]
#                 map_data['size'] = [item[1] for item in color_size]

#                 # 3. Create Pydeck chart
#                 view_state = pdk.ViewState(
#                     latitude=map_data['lat'].mean(),
#                     longitude=map_data['lon'].mean(),
#                     zoom=0.5,
#                     pitch=0,
#                     bearing=0
#                 )

#                 layer = pdk.Layer(
#                     'ScatterplotLayer',
#                     data=map_data,
#                     get_position='[lon, lat]',
#                     get_color='color',
#                     get_radius='size',
#                     pickable=True,
#                     opacity=0.8,
#                     stroked=True,
#                     filled=True,
#                     radius_scale=0.5,
#                     get_line_color=[255, 255, 255, 100],
#                     get_line_width=10000,
#                 )

#                 tooltip = {
#                     "html": "<b>{city}, {country}</b><br/>"
#                             "Final Allowance: <b>${final_allowance}</b>",
#                     "style": { "color": "white", "backgroundColor": "#1e3c72" }
#                 }
                
#                 r = pdk.Deck(
#                     layers=[layer],
#                     initial_view_state=view_state,
#                     tooltip=tooltip
#                 )

#                 map_col, legend_col = st.columns([4, 1])

#                 with map_col:
#                     st.pydeck_chart(r, use_container_width=True)

#                 with legend_col:
#                     st.write("##### Legend (Cost)")
#                     st.markdown(f"""
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">High Cost (~${max_cost:,.0f})</span>
#                     </div>
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">Medium Cost</span>
#                     </div>
#                     <div style="display: flex; align-items: center;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">Low Cost (~${min_cost:,.0f})</span>
#                     </div>
#                     """, unsafe_allow_html=True)
#                     st.caption("The larger the circle and the redder the color, the higher the cost of the city.")

#             # 4. (Apply Idea 1) Top 10 Charts
#             st.divider()
#             col1, col2 = st.columns(2)
            
#             if 'final_allowance' in df_merged.columns:
#                 with col1:
#                     st.write("##### ğŸ’° Top 10 High Cost Cities (AI Final)")
#                     top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                    
#                     average_cost = df_merged['final_allowance'].mean()
                    
#                     # --- [v19.1 Hotfix] Add 'average' column for tooltip ---
#                     top_10_cost_df['average'] = average_cost
                    
#                     base_cost = alt.Chart(top_10_cost_df).encode(
#                         x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                         y=alt.Y('final_allowance', title="Final Allowance ($)", axis=alt.Axis(format='$,.0f')),
#                         tooltip=[
#                             alt.Tooltip('city', title="City"),
#                             alt.Tooltip('final_allowance', title="Final Allowance ($)", format='$,.0f'),
#                             alt.Tooltip('average', title="Overall Average", format='$,.0f') # <-- Modified
#                         ]
#                     )
                    
#                     bars_cost = base_cost.mark_bar(color="#0D6EFD").encode()
                    
#                     rule_cost = alt.Chart(pd.DataFrame({'average_cost': [average_cost]})).mark_rule(
#                         color='gray', strokeDash=[3, 3] # [v19.1] Change line color
#                     ).encode(
#                         y=alt.Y('average_cost', title=''),
#                         tooltip=[alt.Tooltip('average_cost', title="Overall Average", format='$,.0f')] 
#                     )
                    
#                     chart_cost = (bars_cost + rule_cost).properties(
#                         background='transparent',
#                         title=f"Overall Average: ${average_cost:,.0f}" 
#                     ).interactive()
#                     st.altair_chart(chart_cost, use_container_width=True)
                
#                 with col2:
#                     st.write("##### âš ï¸ Top 10 High Volatility Cities (AI Confidence)")
#                     df_report_vc = pd.DataFrame(report_data['cities'])
#                     df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
#                     df_report_vc.dropna(subset=['vc'], inplace=True)
                    
#                     if df_report_vc.empty:
#                         st.info("Volatility (VC) data is missing. (AI analysis with the latest version is required)")
#                     else:
#                         top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                        
#                         average_vc = df_report_vc['vc'].mean()

#                         # --- [v19.1 Hotfix] Add 'average' column for tooltip ---
#                         top_10_vc_df['average'] = average_vc
                        
#                         base_vc = alt.Chart(top_10_vc_df).encode(
#                             x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                             y=alt.Y('vc', title="Variation Coefficient (VC)", axis=alt.Axis(format='%')),
#                             tooltip=[
#                                 alt.Tooltip('city', title="City"),
#                                 alt.Tooltip('vc', title="Variation Coefficient (VC)", format='.2%'),
#                                 alt.Tooltip('average', title="Overall Average", format='.2%') # <-- Modified
#                             ]
#                         )
                        
#                         bars_vc = base_vc.mark_bar(color="#DC3545").encode()
                        
#                         rule_vc = alt.Chart(pd.DataFrame({'average_vc': [average_vc]})).mark_rule(
#                             color='gray', strokeDash=[3, 3] # [v19.1] Change line color
#                         ).encode(
#                             y=alt.Y('average_vc', title=''),
#                             tooltip=[alt.Tooltip('average_vc', title="Overall Average", format='.2%')]
#                         )
                        
#                         chart_vc = (bars_vc + rule_vc).properties(
#                             background='transparent',
#                             title=f"Overall Average: {average_vc:.2%}"
#                         ).interactive()
#                         st.altair_chart(chart_vc, use_container_width=True)
#                         st.caption("The higher the volatility (VC), the less confident the AI is in its price estimation for the city.")
#             else:
#                 st.warning("No 'final_allowance' data to display the chart.")

# if employee_tab is not None:
#     with employee_tab:
#         st.header("Per Diem Inquiry by City")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("Please analyze a PDF in the 'Report Analysis' tab first.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("Country:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("City:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("There are no registered cities for the selected country.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "Trip Start Date",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "Trip End Date",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("Job Level:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("The end date must be on or after the start date.")
#                     trip_days = 0 # Set to 0
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"Auto-classified trip type: {trip_term_label} Â· {trip_days}-day trip")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("No city data available for this period. Adjust the trip type to 'Short-term' or check city settings.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} Results")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [New 2] Employee tab total card
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} Daily Recommended Per Diem", "No Amount")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**Basis of Calculation (Daily Rate)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"Success {ai_runs}/{ai_attempts} runs"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"Outliers {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"Season {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "No AI Data"
                        
#                         # [New 1] Reason for applying dynamic weights
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | Reason: {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"Short-term base $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"Short-term $ {un_base:,} â†’ Long-term $ {un_display:,}"
#                                         render_stat_card("UN-DSA Basis", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"Short-term base $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"Short-term $ {ai_avg:,} â†’ Long-term $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI Market Estimate (Seasonal Adj.)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"Short-term $ {final_allowance:,} â†’ Long-term $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("Weighted Average Result", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [New 2] Detailed cost breakdown (merged with show_ai_market_detail logic)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # line break
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # Apply long-term/seasonal rates
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI Estimate Details (Daily Rate)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("Est. Food", f"$ {food_display:,}", f"Short-term base: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("Est. Transport", f"$ {trans_display:,}", f"Short-term base: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("Est. Misc", f"$ {misc_display:,}", f"Short-term base: $ {round(mean_misc)}", "muted")
                        
#                         # [Improvement 3] The show_weighted_result card is redundant, so the block below is removed
#                         # (Original second_row_keys logic removed)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("The administrator has hidden the detailed calculation basis.")

# # --- [Improvement 2] Changed admin_tab -> admin_analysis_tab ---
# with admin_analysis_tab:
    
#     # [Improvement 2] Load ADMIN_ACCESS_CODE and check .env
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # Load from .env

#     if not ACCESS_CODE_VALUE:
#         st.error("Security Error: 'ADMIN_ACCESS_CODE' is not set in the .env file. Please stop the app and set the .env file.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [Improvement 3] Rerun on success
#             else:
#                 st.error("The Access Code is incorrect.")
#                 st.stop() # [Improvement 3] Stop on failure
#         else:
#             st.stop() # [Improvement 3] Stop before form submission

#     # --- [Improvement 3] "Report Version Management" feature (analysis_sub_tab) ---
#     st.subheader("Report Version Management")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("Select the active report version:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("No reports have been generated.")

#     # --- [New 4] Past Report Comparison feature (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("Compare Past Reports")
#     if len(history_files) < 2:
#         st.info("At least 2 reports are required for comparison.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("Base Report (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("Comparison Report (B)", history_files, index=0, key="compare_b")
        
#         if st.button("Compare Reports"):
#             if file_a == file_b:
#                 st.warning("You must select two different reports.")
#             else:
#                 with st.spinner("Comparing reports..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["Change ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # Prevent division by zero
#                         df_merged["Change (%)"] = (df_merged["Change ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "Change ($)", "Change (%)"
#                         ]].style.format({"Change (%)": "{:,.1f}%", "Change ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("Failed to load report files.")
    
#     # --- [Improvement 3] "UN-DSA (PDF) Analysis" feature (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) Analysis & AI Execution")
#     st.warning(f"Note that the AI will be called {NUM_AI_CALLS} times, which will consume time and cost. (Improvement 1: Async processing for faster speed)")
#     uploaded_file = st.file_uploader("Upload UN-DSA PDF file.", type="pdf")

#     # --- [Improvement 1] Async AI analysis execution logic ---
#     if uploaded_file and st.button("Run AI Analysis", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error("Please set OPENAI_API_KEY in the .env file.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- Define async execution function ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="Extracting PDF text...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV converting... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("Failed to convert PDF->TSV.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("Failed to process TSV data.")
#                     progress_bar.empty()
#                     return

#                 # Create async OpenAI client
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # List to hold all AI call tasks

#                 # 1. Pre-create all AI call tasks for all cities
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [City1-10runs], [City2-10runs], ... ]

#                 # 2. Execute all tasks asynchronously and collect results
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"Calculating AI estimates... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # Run 10 tasks for this city concurrently
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"Async error during {city_name} analysis: {e}")
#                         market_results = [] # Handle failure

#                     # 3. Process results
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [New 2] Lists for detailed cost breakdown
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [New 2] Add detailed costs
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. Calculate final allowance
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [New 1] Calculate dynamic weights
#                     admin_weights = get_weight_config() # Load admin settings
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # If no UN data, use AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [New 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [New 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [New 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [New 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [New 1] Save dynamic weights
#                         },
#                     }

#                     # [New 1] Calculate final value with dynamic weights
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # Next city

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- Async execution ---
#             with st.spinner("Processing PDF and running AI analysis... (Takes approx. 10-30 seconds)"):
#                 progress_bar = st.progress(0, text="Starting analysis...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [Improvement 3] "Latest Analysis Summary" feature (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] Prevent ArrowInvalid Error ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # Change number to string format like "12%"
#             else:
#                 delta_display = "N/A" # Already "N/A" string
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- Use modified string value
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- Added use_container_width (change to width='stretch' if needed)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))


# def auto_fill_all_city_coordinates() -> tuple[int, int]:
#     """
#     target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
#     DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
#     """
#     try:
#         geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#     except Exception as e:
#         st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#         return 0, 0

#     current_entries = get_target_city_entries()
#     entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

#     if not entries_to_update:
#         return 0, 0

#     success_count = 0
#     fail_count = 0

#     progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

#     with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#         for i, entry in enumerate(entries_to_update):
#             city = entry["city"]
#             country = entry["country"]
#             query = f"{city}, {country}"

#             try:
#                 location = geolocator.geocode(query, timeout=5)
#                 time.sleep(1)  # Nominatim rate limit

#                 if location:
#                     entry["lat"] = location.latitude
#                     entry["lon"] = location.longitude
#                     st.toast(f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
#                     success_count += 1
#                 else:
#                     st.toast(f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="â“")
#                     fail_count += 1
#             except (GeocoderTimedOut, GeocoderUnavailable):
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", icon="ğŸ”¥")
#                 fail_count += 1
#             except Exception as e:
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                 fail_count += 1

#             progress_bar.progress((i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}")

#     # DB + ì„¸ì…˜ì— ì €ì¥
#     set_target_city_entries(current_entries)
#     progress_bar.empty()
#     return success_count, fail_count


# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# with admin_config_tab:
#     # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤. 'ë³´ê³ ì„œ ë¶„ì„ (Admin)' íƒ­ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
#         st.stop()
        
#     # --- [v19.3] ë„ì‹œ í¸ì§‘/ìºì‹œ ê´€ë¦¬ê°€ ê³µìœ í•  ë„ì‹œ ëª©ë¡ì„ íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜ ---
#     current_entries = get_target_city_entries()
#     options = {
#         f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#         for idx, entry in enumerate(current_entries)
#     }
#     sorted_labels = list(options.keys())
    
#     # --- ì½œë°± í•¨ìˆ˜ 1: 'ë„ì‹œ í¸ì§‘' í¼ ë™ê¸°í™” ---
#     def _sync_edit_form_from_selection():
#         if "edit_city_selector" not in st.session_state or not st.session_state.edit_city_selector:
#              # st.session_state.edit_city_selectorê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì¼ ë•Œ
#              if sorted_labels:
#                  st.session_state.edit_city_selector = sorted_labels[0]
#              else:
#                  return # ë„ì‹œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¤‘ë‹¨
             
#         selected_idx = options[st.session_state.edit_city_selector]
#         selected_entry = current_entries[selected_idx]
        
#         st.session_state.edit_region = selected_entry.get("region", "")
#         st.session_state.edit_city = selected_entry.get("city", "")
#         st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#         st.session_state.edit_country = selected_entry.get("country", "")
#         st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
        
#         existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#         st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
        
#         sub_data = selected_entry.get("un_dsa_substitute") or {}
#         st.session_state.edit_sub_city = sub_data.get("city", "")
#         st.session_state.edit_sub_country = sub_data.get("country", "")

#     # --- [v19.3] ì½œë°± í•¨ìˆ˜ 2: 'ìºì‹œ ì¶”ê°€' í¼ ë™ê¸°í™” ---
#     def _sync_cache_form_from_selection():
#         selected_label = st.session_state.get("cache_city_selector") # get()ìœ¼ë¡œ ì˜¤ë¥˜ ë°©ì§€
        
#         if selected_label in options: # 'options' dictë¥¼ ê³µìœ 
#             selected_idx = options[selected_label]
#             selected_entry = current_entries[selected_idx]
#             st.session_state.new_cache_country = selected_entry.get("country", "")
#             st.session_state.new_cache_city = selected_entry.get("city", "")
#             st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
#         else: # (placeholder ì„ íƒ ì‹œ)
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
        
#         # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” í•­ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
#         st.session_state.new_cache_vendor = ""
#         st.session_state.new_cache_category = "Food"
#         st.session_state.new_cache_price = 0.0
#         st.session_state.new_cache_currency = "USD"
#         st.session_state.new_cache_url = ""

#     # --- [v19.3 í•«í”½ìŠ¤] ì½œë°± í•¨ìˆ˜ 3: 'ìºì‹œ ì €ì¥' ë¡œì§ ---
#     def handle_cache_submit():
#         # 1. ìœ íš¨ì„± ê²€ì‚¬
#         if (not st.session_state.new_cache_country or 
#             not st.session_state.new_cache_city or 
#             not st.session_state.new_cache_vendor):
#             st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#             return # ì—¬ê¸°ì„œ ì¤‘ë‹¨ (í¼ ê°’ ìœ ì§€ë¨)

#         # 2. ìƒˆ í•­ëª© ìƒì„±
#         new_entry = {
#             "country": st.session_state.new_cache_country.strip(),
#             "city": st.session_state.new_cache_city.strip(),
#             "neighborhood": st.session_state.new_cache_neighborhood.strip(),
#             "vendor": st.session_state.new_cache_vendor.strip(),
#             "category": st.session_state.new_cache_category,
#             "price": st.session_state.new_cache_price,
#             "currency": st.session_state.new_cache_currency.strip().upper(),
#             "url": st.session_state.new_cache_url.strip(),
#         }
        
#         # 3. íŒŒì¼ì— ì €ì¥
#         if add_menu_cache_entry(new_entry):
#             st.success(f"'{new_entry['vendor']}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            
#             # 4. (ì¤‘ìš”) í¼ ë¦¬ì…‹: session_state ê°’ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”
#             # ì´ ë¡œì§ì€ on_click ì½œë°± ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
#             st.session_state.new_cache_vendor = ""
#             st.session_state.new_cache_category = "Food"
#             st.session_state.new_cache_price = 0.0
#             st.session_state.new_cache_currency = "USD"
#             st.session_state.new_cache_url = ""
#             st.session_state.cache_city_selector = None # ë“œë¡­ë‹¤ìš´ë„ ë¦¬ì…‹
            
#             # st.rerun()ì€ on_click ì½œë°±ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•  í•„ìš” ì—†ìŒ
#         else:
#             st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#     # --- [v19.3 í•«í”½ìŠ¤] ë ---

#     st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")
#         time.sleep(1) # ìœ ì €ê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤Œ
#         st.rerun()

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
#         time.sleep(1)
#         st.rerun()

#     st.divider()
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬ (target_cities_config.json)")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch') # [v19.3] ê²½ê³  ìˆ˜ì •
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     # --- [ì‹ ê·œ 2] ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„± ê¸°ëŠ¥ (ìƒˆë¡œ ì¶”ê°€) ---
#     st.divider()
#     st.subheader("ë„ì‹œ ì¢Œí‘œ ê´€ë¦¬")
    
#     if st.button(
#         "ëª¨ë“  ë„ì‹œ ì¢Œí‘œ(Lat/Lon) ìë™ ì™„ì„±",
#         help="target_cities_config.jsonì˜ ëª¨ë“  ë„ì‹œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¢Œí‘œê°€ ì—†ëŠ” ë„ì‹œì— ëŒ€í•´ geopyë¥¼ í˜¸ì¶œí•´ ì¢Œí‘œë¥¼ ìë™ ì €ì¥í•©ë‹ˆë‹¤.",
#     ):
#         success_count, fail_count = auto_fill_all_city_coordinates()

#         if success_count == 0 and fail_count == 0:
#             st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
#         else:
#             st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")
#         st.rerun()

#     # --- [ì‹ ê·œ 2] ë ---


#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()

#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
    
#     if current_entries:
#         # ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state:
#             _sync_edit_form_from_selection()

#         # í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", 
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 selected_idx = options[st.session_state.edit_city_selector]
#                 current_entries[selected_idx] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[selected_idx]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[selected_idx].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()
        
#         if delete_btn:
#             selected_idx = options[st.session_state.edit_city_selector]
#             del current_entries[selected_idx]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun()
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#     st.divider()
#     st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#         # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#         st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
        
#         st.selectbox(
#             "ë„ì‹œ ì„ íƒ (ìë™ ì±„ìš°ê¸°):", 
#             sorted_labels,  # íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜
#             key="cache_city_selector",
#             on_change=_sync_cache_form_from_selection, # ìƒˆë¡œ ë§Œë“  ì½œë°±
#             index=None,
#             placeholder="ë„ì‹œë¥¼ ì„ íƒí•˜ë©´ êµ­ê°€, ë„ì‹œ, ì„¸ë¶€ ì§€ì—­ì´ ìë™ ì…ë ¥ë©ë‹ˆë‹¤."
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ìºì‹œ í¼ ì´ˆê¸°í™”
#         if "new_cache_country" not in st.session_state:
#             _sync_cache_form_from_selection() # ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        
#         # --- [v19.3 í•«í”½ìŠ¤] clear_on_submit=False, on_click ì½œë°± ì‚¬ìš© ---
#         with st.form("add_menu_cache_form"): # clear_on_submit ì œê±°
#             st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 new_cache_country = st.text_input("êµ­ê°€ (Country)", key="new_cache_country", help="ì˜ˆ: Philippines")
#                 new_cache_city = st.text_input("ë„ì‹œ (City)", key="new_cache_city", help="ì˜ˆ: Manila")
#                 new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", key="new_cache_neighborhood", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                 new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", key="new_cache_vendor", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"], key="new_cache_category")
#                 new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01, key="new_cache_price")
#                 new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", key="new_cache_currency", help="ì˜ˆ: PHP, USD")
#                 new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)", key="new_cache_url")
            
#             # [v19.3] on_click ì½œë°±ìœ¼ë¡œ ì €ì¥/ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
#             add_cache_submitted = st.form_submit_button(
#                 "ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥",
#                 on_click=handle_cache_submit # <-- í•µì‹¬ ìˆ˜ì •
#             )
#         # --- [v19.3 í•«í”½ìŠ¤] ë ---

#         # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#         st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#         all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
        
#         if not all_cache_data:
#             st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             # [v19.3] ê²½ê³  ìˆ˜ì •
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], width='stretch')

#             # ì‚­ì œ ê¸°ëŠ¥
#             st.markdown("---")
#             st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
            
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data))
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
            
#             if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
#                 else:
#                     st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ì‹ ê·œ 3] UI ë ---
    


# 2025-11-14 ì§€ë„ ì„œë²„ ì—°ë™
# # 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple
# from sqlalchemy import text


# import altair as alt  # [ì‹ ê·œ 2] ê³ ê¸‰ ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# import pydeck as pdk  # [ì‹ ê·œ 2] ê³ ê¸‰ 3D ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬

# # [ì‹ ê·œ 2(ì§€ë„)] ê´€ë ¨ ì„í¬íŠ¸
# from geopy.geocoders import Nominatim
# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable

# # --- [v20.0 DBì—°ë™] Streamlit Connection ì´ˆê¸°í™” ---
# # Supabase (Postgres) DBì— ì—°ê²°í•©ë‹ˆë‹¤.
# # ì´ ì—°ê²°ì€ Streamlitì˜ Secretsì—ì„œ "connections.supabase_db" ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
# conn = st.connection("supabase_db", type="sql", dialect="postgresql")


# from datetime import date, datetime

# def _json_default(obj):
#     """report_dataë¥¼ JSONìœ¼ë¡œ ì €ì¥í•  ë•Œ, ë‚ ì§œ/ì‹œê°„ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜."""
#     if isinstance(obj, (date, datetime)):
#         return obj.isoformat()
#     return obj  # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ ë™ì‘ (ì—ëŸ¬ ë‚˜ë©´ ê·¸ë•Œ í™•ì¸)

# # --- [v20.0 DBì—°ë™] data_sources.menu_cache.py ê¸°ëŠ¥ ëŒ€ì²´ ---
# # ì´ì œ 'menu_cache.py' íŒŒì¼ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# def load_cached_menu_prices(
#     city: str, 
#     country: str, 
#     neighborhood: Optional[str]
# ) -> List[Dict[str, Any]]:
#     """DBì—ì„œ íŠ¹ì • ìœ„ì¹˜ì˜ ë©”ë‰´ ê°€ê²© ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     city_lower = city.lower()
#     country_lower = country.lower()
    
#     query_base = "SELECT * FROM menu_cache WHERE LOWER(city) = :city AND LOWER(country) = :country"
#     params = {"city": city_lower, "country": country_lower}

#     if neighborhood:
#         # 1ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°
#         query_neighborhood = query_base + " AND LOWER(neighborhood) = :neighborhood"
#         params_neighborhood = params.copy()
#         params_neighborhood["neighborhood"] = neighborhood.lower().strip()
#         df_neighborhood = conn.query(query_neighborhood, params=params_neighborhood)
#         if not df_neighborhood.empty:
#             return df_neighborhood.to_dict('records')

#     # 2ìˆœìœ„: ì„¸ë¶€ ì§€ì—­ì´ ë¹„ì–´ìˆëŠ” 'ë„ì‹œ ì „ì²´' ë°ì´í„°
#     query_city = query_base + " AND (neighborhood IS NULL OR neighborhood = '')"
#     df_city = conn.query(query_city, params=params)
#     return df_city.to_dict('records')

# def load_all_cache() -> List[Dict[str, Any]]:
#     """DBì—ì„œ *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     df = conn.query("SELECT * FROM menu_cache ORDER BY last_updated DESC, id DESC", ttl=5) # 5ì´ˆ ìºì‹œ
#     return df.to_dict('records')

# def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool:
#     """ìƒˆë¡œìš´ ìºì‹œ í•­ëª© 1ê°œë¥¼ DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
#     try:
#         new_entry["last_updated"] = datetime.now().date()
#         # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ insert (to_dict('records')ì™€ í˜•ì‹ì„ ë§ì¶¤)
#         df_new = pd.DataFrame([new_entry]) 
#         conn.insert("menu_cache", df_new)
#         return True
#     except Exception as e:
#         st.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
#         return False

# def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool:
#     """(ì‚­ì œ ì‹œ ì‚¬ìš©) *ì „ì²´* ë©”ë‰´ ìºì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
#     try:
#         # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
#         with conn.session as s:
#             s.execute(text("DELETE FROM menu_cache"))
#             s.commit()
            
#         # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
#         if all_samples:
#             df_new = pd.DataFrame(all_samples)
#             # id, created_at ë“± ìë™ ìƒì„± ì»¬ëŸ¼ì€ DataFrameì— ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì™¸
#             df_new = df_new.drop(columns=["id", "created_at"], errors='ignore') 
#             conn.insert("menu_cache", df_new)
#         return True
#     except Exception as e:
#         st.error(f"DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
#         return False

# MENU_CACHE_ENABLED = True # DBë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•­ìƒ í™œì„±í™”
# # --- [v20.0 DBì—°ë™] ë ---

# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# # --- [v20.0 DBì—°ë™] Weight Config í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def _get_config_from_db(config_key: str, default_config: Dict[str, Any]) -> Dict[str, Any]:
#     """DBì˜ app_config í…Œì´ë¸”ì—ì„œ ì„¤ì •ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
#     try:
#         result = conn.query(
#             "SELECT config_value FROM app_config WHERE config_key = :key",
#             params={"key": config_key},
#             ttl=10 # 10ì´ˆ ìºì‹œ
#         )
#         if result.empty:
#             return default_config
        
#         db_value = result.iloc[0]["config_value"]
#         if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
#             return json.loads(db_value)
#         return db_value
#     except Exception:
#         return default_config

# def _save_config_to_db(config_key: str, config_value: Dict[str, Any]) -> None:
#     """DBì˜ app_config í…Œì´ë¸”ì— ì„¤ì •ì„ ì €ì¥(ì—…ë°ì´íŠ¸)í•©ë‹ˆë‹¤."""
#     try:
#         # PostgreSQLì˜ JSONB íƒ€ì…ì„ ìœ„í•´ json.dumps ì‚¬ìš©
#         config_json = json.dumps(config_value) 
        
#         # 'UPSERT' (Update or Insert) ì¿¼ë¦¬
#         with conn.session as s:
#             s.execute(text(
#                 f"""
#                 INSERT INTO app_config (config_key, config_value, last_updated)
#                 VALUES (:key, :value, :now)
#                 ON CONFLICT (config_key) 
#                 DO UPDATE SET config_value = :value, last_updated = :now
#                 """
#             ), params={"key": config_key, "value": config_json, "now": datetime.now()})
#             s.commit()
#     except Exception as e:
#         st.error(f"DB ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")

# def get_weight_config() -> Dict[str, float]:
#     """DBì—ì„œ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
#     config = _get_config_from_db("weight_config", DEFAULT_WEIGHT_CONFIG)
#     normalized = _normalize_weight_config(config)
    
#     # st.session_stateì—ë„ ì €ì¥ (ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜ì„± ìœ ì§€)
#     try:
#         st.session_state["weight_config"] = normalized
#     except Exception:
#         pass
#     return normalized

# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """DBì™€ ì„¸ì…˜ì˜ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = _normalize_weight_config(config)
#     _save_config_to_db("weight_config", normalized)
    
#     try:
#         st.session_state["weight_config"] = normalized
#     except Exception:
#         pass
#     return normalized
# # --- [v20.0 DBì—°ë™] ë ---
# def load_weight_config() -> Dict[str, float]:
#     """
#     (Backwards compatibility)
#     ì˜ˆì „ íŒŒì¼ ê¸°ë°˜ í•¨ìˆ˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì“°ë˜,
#     ì‹¤ì œ êµ¬í˜„ì€ DB ê¸°ë°˜ get_weight_config()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     """
#     return get_weight_config()



# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">Estimated Total Per Diem ({level_label})</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>Calculation</span><br/>
#             $ {daily:,} Ã— {days} days Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# # --- [v20.0 DBì—°ë™] UI Settings í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """DBì— UI ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤."""
#     normalized = _normalize_ui_settings(settings)
#     _save_config_to_db("ui_settings", normalized)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """DBì—ì„œ UI ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
    
#     config = _get_config_from_db("ui_settings", DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(config)
    
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)
# # --- [v20.0 DBì—°ë™] ë ---

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "Short-term", "Long-term": "Long-term"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
#     return entry


# # --- [v20.0 DBì—°ë™] Target Cities í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def load_target_city_entries() -> List[Dict[str, Any]]:
#     """DBì—ì„œ ëª¨ë“  ë„ì‹œ ëª©ë¡ì„ ë¡œë“œí•˜ë˜, ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±."""
#     df = conn.query("SELECT * FROM target_cities ORDER BY region, country, city", ttl=10)
#     if df.empty:
#         # DB ë¹„ì–´ ìˆìœ¼ë©´ ì½”ë“œì— ë°•í˜€ ìˆëŠ” DEFAULT_TARGET_CITY_ENTRIES ì‚¬ìš©
#         return [normalize_target_entry(e) for e in DEFAULT_TARGET_CITY_ENTRIES]
#     return df.to_dict('records')

# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     """(ìˆ˜ì •/ì‚­ì œ ì‹œ ì‚¬ìš©) ì „ì²´ ë„ì‹œ ëª©ë¡ì„ DBì— ë®ì–´ì”ë‹ˆë‹¤."""
#     try:
#         # 1. ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
#         with conn.session as s:
#             s.execute(text("DELETE FROM target_cities"))
#             s.commit()
            
#         # 2. ìƒˆ ëª©ë¡ìœ¼ë¡œ ì‚½ì… (ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´)
#         if entries:
#             df_new = pd.DataFrame(entries)
#             # DBê°€ ìë™ ìƒì„±í•˜ëŠ” 'id', 'created_at'ì€ ì‚½ì… ì‹œ ì œì™¸
#             df_new = df_new.drop(columns=["id", "created_at"], errors='ignore') 
#             conn.insert("target_cities", df_new)
#     except Exception as e:
#         st.error(f"DB ë„ì‹œ ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")

# TARGET_CITIES_ENTRIES = load_target_city_entries() # ì•± ì‹œì‘ ì‹œ DBì—ì„œ ë¡œë“œ

# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     # DBì—ì„œ ë¡œë“œí•œ ìµœì‹ ë³¸ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
#     st.session_state["target_cities_entries"] = load_target_city_entries()
#     return st.session_state["target_cities_entries"]

# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     # 1. DBì— ì˜êµ¬ ì €ì¥
#     save_target_city_entries(entries) 
#     # 2. í˜„ì¬ ì„¸ì…˜ ìƒíƒœì—ë„ ì¦‰ì‹œ ë°˜ì˜
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
# # --- [v20.0 DBì—°ë™] ë ---


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# # --- [v20.0 DBì—°ë™] Report History í•¨ìˆ˜ (DB ê¸°ë°˜) ---
# def get_history_files() -> List[str]:
#     """DBì—ì„œ ê³¼ê±° ë³´ê³ ì„œ 'ì´ë¦„' ëª©ë¡ì„ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
#     df = conn.query("SELECT name FROM analysis_reports ORDER BY created_at DESC", ttl=10) # 10ì´ˆ ìºì‹œ
#     return df['name'].tolist()

# from sqlalchemy import text

# def save_report_data(data):
#     """ë¶„ì„ ê²°ê³¼ë¥¼ DB(JSONB)ì— ì €ì¥í•©ë‹ˆë‹¤."""
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"report_{timestamp}.json"

#     try:
#         report_json = json.dumps(data)

#         with conn.session as s:
#             s.execute(
#                 text("""
#                     INSERT INTO analysis_reports (name, report_data)
#                     VALUES (:name, :report_data)
#                 """),
#                 {"name": filename, "report_data": report_json},
#             )
#             s.commit()

#     except Exception as e:
#         st.error(f"DB ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")



# def load_report_data(filename):
#     """DBì—ì„œ íŠ¹ì • ë³´ê³ ì„œ(JSON)ë¥¼ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
#     try:
#         result = conn.query(
#             "SELECT report_data FROM analysis_reports WHERE name = :name",
#             params={"name": filename},
#             ttl=3600 # 1ì‹œê°„ ìºì‹œ
#         )
#         if result.empty:
#             return None
        
#         db_value = result.iloc[0]["report_data"]
        
#         if isinstance(db_value, str): # DBê°€ JSONì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•  ê²½ìš°
#             data = json.loads(db_value)
#         else: # ì´ë¯¸ dict/listë¡œ ë°˜í™˜ëœ ê²½ìš°
#             data = db_value
            
#         return _sanitize_report_data(data)
#     except Exception as e:
#         st.error(f"DB ë³´ê³ ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return None
# # --- [v20.0 DBì—°ë™] ë ---


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data




# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None

#         # --- [ìˆ˜ì •] un_dsa_substitute ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ ---
#         sub_raw = target.get("un_dsa_substitute")

#         sub_value = None
#         if isinstance(sub_raw, dict):
#             sub_value = sub_raw
#         elif isinstance(sub_raw, str) and sub_raw.strip():
#             # JSON ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ëŒ€ë¹„
#             try:
#                 sub_value = json.loads(sub_raw)
#             except Exception:
#                 sub_value = None

#         if (
#             isinstance(sub_value, dict)
#             and sub_value.get("city")
#             and sub_value.get("country")
#         ):
#             # ìœ íš¨í•œ ëŒ€ì²´ ë„ì‹œ ì •ë³´ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©
#             search_target = sub_value
#             is_substitute = True
#         else:
#             # ì—†ìœ¼ë©´ ì›ë˜ ë„ì‹œ ì‚¬ìš©
#             search_target = target
#             is_substitute = False
#         # --- [ìˆ˜ì • ë] ---

#         country_df = df[
#             df["Country"].str.contains(search_target["country"], case=False, na=False)
#         ]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- Streamlit UI Configuration ---
# st.set_page_config(layout="wide")
# st.title("AICP: NSUS GROUP Per Diem Calculation & Inquiry System")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [Improvement 3 & New 2] Tab structure change (v18.0) ---
# tab_definitions = [
#     "ğŸ“Š Executive Dashboard", # [New 2] Dashboard tab added
# ]

# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ Per Diem Inquiry (Employee)")

# # Split admin tab into two
# tab_definitions.append("ğŸ“ˆ Report Analysis (Admin)")
# tab_definitions.append("ğŸ› ï¸ System Settings (Admin)")

# tabs = st.tabs(tab_definitions)

# # Assign tab variables
# dashboard_tab = tabs[0]
# tab_index_offset = 1

# if employee_tab_visible:
#     employee_tab = tabs[tab_index_offset]
#     admin_analysis_tab = tabs[tab_index_offset + 1]
#     admin_config_tab = tabs[tab_index_offset + 2]
#     tab_index_offset += 1
# else:
#     employee_tab = None
#     admin_analysis_tab = tabs[tab_index_offset]
#     admin_config_tab = tabs[tab_index_offset + 1]

# # --- [End of modification] ---

# with dashboard_tab:
#     st.header("Global Cost Dashboard")
#     st.info("Visualizes the global business trip cost status based on the latest report data.")

#     try:
#         alt.theme.enable("streamlit")
#     except Exception:
#         pass 

#     history_files = get_history_files()
#     if not history_files:
#         st.warning("No data to display. Please run AI analysis at least once in the 'Report Analysis' tab.")
#     else:
#         latest_report_file = history_files[0]
#         st.subheader(f"Reference Report: `{latest_report_file}`")
        
#         report_data = load_report_data(latest_report_file)
#         config_entries = get_target_city_entries()
        
#         if not report_data or 'cities' not in report_data or not config_entries:
#             st.error("Failed to load data.")
#         else:
#             # 1. Prepare DataFrame (Report + Coordinates)
#             df_report = pd.DataFrame(report_data['cities'])
#             df_config = pd.DataFrame(config_entries)
            
#             df_merged = pd.merge(
#                 df_report,
#                 df_config,
#                 left_on=["city", "country_display"],
#                 right_on=["city", "country"],
#                 suffixes=("_report", "_config")
#             )
            
#             required_map_cols = ['city', 'country', 'lat', 'lon', 'final_allowance']
            
#             if not all(col in df_merged.columns for col in ['lat', 'lon']):
#                 st.warning(
#                     "Coordinate (lat/lon) data for the map is missing. ğŸ—ºï¸\n\n"
#                     "**Solution:** Go to the 'ğŸ› ï¸ System Settings (Admin)' tab and press the [Auto-complete all city coordinates] button."
#                 )
#                 map_data = pd.DataFrame(columns=required_map_cols)
#             else:
#                 map_data = df_merged.copy()
#                 map_data = map_data[required_map_cols]
#                 map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)
#                 map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
#                 map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
#                 map_data.dropna(subset=['lat', 'lon'], inplace=True)

#             if map_data.empty:
#                 st.caption("No data to display on the map. (Check if coordinates were generated.)")
#             else:
#                 # 2. Calculate color (R,G,B) and size based on cost
#                 min_cost = map_data['final_allowance'].min()
#                 max_cost = map_data['final_allowance'].max()
#                 range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

#                 def get_color_and_size(cost):
#                     norm_cost = (cost - min_cost) / range_cost
#                     r = int(255 * norm_cost)
#                     g = int(255 * (1 - norm_cost))
#                     b = 0
#                     size = 50000 + (norm_cost * 450000)
#                     return [r, g, b, 160], size

#                 color_size = map_data['final_allowance'].apply(get_color_and_size)
#                 map_data['color'] = [item[0] for item in color_size]
#                 map_data['size'] = [item[1] for item in color_size]

#                 # 3. Create Pydeck chart
#                 view_state = pdk.ViewState(
#                     latitude=map_data['lat'].mean(),
#                     longitude=map_data['lon'].mean(),
#                     zoom=0.5,
#                     pitch=0,
#                     bearing=0
#                 )

#                 layer = pdk.Layer(
#                     'ScatterplotLayer',
#                     data=map_data,
#                     get_position='[lon, lat]',
#                     get_color='color',
#                     get_radius='size',
#                     pickable=True,
#                     opacity=0.8,
#                     stroked=True,
#                     filled=True,
#                     radius_scale=0.5,
#                     get_line_color=[255, 255, 255, 100],
#                     get_line_width=10000,
#                 )

#                 tooltip = {
#                     "html": "<b>{city}, {country}</b><br/>"
#                             "Final Allowance: <b>${final_allowance}</b>",
#                     "style": { "color": "white", "backgroundColor": "#1e3c72" }
#                 }
                
#                 r = pdk.Deck(
#                     layers=[layer],
#                     initial_view_state=view_state,
#                     tooltip=tooltip
#                 )

#                 map_col, legend_col = st.columns([4, 1])

#                 with map_col:
#                     st.pydeck_chart(r, use_container_width=True)

#                 with legend_col:
#                     st.write("##### Legend (Cost)")
#                     st.markdown(f"""
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">High Cost (~${max_cost:,.0f})</span>
#                     </div>
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">Medium Cost</span>
#                     </div>
#                     <div style="display: flex; align-items: center;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">Low Cost (~${min_cost:,.0f})</span>
#                     </div>
#                     """, unsafe_allow_html=True)
#                     st.caption("The larger the circle and the redder the color, the higher the cost of the city.")

#             # 4. (Apply Idea 1) Top 10 Charts
#             st.divider()
#             col1, col2 = st.columns(2)
            
#             if 'final_allowance' in df_merged.columns:
#                 with col1:
#                     st.write("##### ğŸ’° Top 10 High Cost Cities (AI Final)")
#                     top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                    
#                     average_cost = df_merged['final_allowance'].mean()
                    
#                     # --- [v19.1 Hotfix] Add 'average' column for tooltip ---
#                     top_10_cost_df['average'] = average_cost
                    
#                     base_cost = alt.Chart(top_10_cost_df).encode(
#                         x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                         y=alt.Y('final_allowance', title="Final Allowance ($)", axis=alt.Axis(format='$,.0f')),
#                         tooltip=[
#                             alt.Tooltip('city', title="City"),
#                             alt.Tooltip('final_allowance', title="Final Allowance ($)", format='$,.0f'),
#                             alt.Tooltip('average', title="Overall Average", format='$,.0f') # <-- Modified
#                         ]
#                     )
                    
#                     bars_cost = base_cost.mark_bar(color="#0D6EFD").encode()
                    
#                     rule_cost = alt.Chart(pd.DataFrame({'average_cost': [average_cost]})).mark_rule(
#                         color='gray', strokeDash=[3, 3] # [v19.1] Change line color
#                     ).encode(
#                         y=alt.Y('average_cost', title=''),
#                         tooltip=[alt.Tooltip('average_cost', title="Overall Average", format='$,.0f')] 
#                     )
                    
#                     chart_cost = (bars_cost + rule_cost).properties(
#                         background='transparent',
#                         title=f"Overall Average: ${average_cost:,.0f}" 
#                     ).interactive()
#                     st.altair_chart(chart_cost, use_container_width=True)
                
#                 with col2:
#                     st.write("##### âš ï¸ Top 10 High Volatility Cities (AI Confidence)")
#                     df_report_vc = pd.DataFrame(report_data['cities'])
#                     df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
#                     df_report_vc.dropna(subset=['vc'], inplace=True)
                    
#                     if df_report_vc.empty:
#                         st.info("Volatility (VC) data is missing. (AI analysis with the latest version is required)")
#                     else:
#                         top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                        
#                         average_vc = df_report_vc['vc'].mean()

#                         # --- [v19.1 Hotfix] Add 'average' column for tooltip ---
#                         top_10_vc_df['average'] = average_vc
                        
#                         base_vc = alt.Chart(top_10_vc_df).encode(
#                             x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                             y=alt.Y('vc', title="Variation Coefficient (VC)", axis=alt.Axis(format='%')),
#                             tooltip=[
#                                 alt.Tooltip('city', title="City"),
#                                 alt.Tooltip('vc', title="Variation Coefficient (VC)", format='.2%'),
#                                 alt.Tooltip('average', title="Overall Average", format='.2%') # <-- Modified
#                             ]
#                         )
                        
#                         bars_vc = base_vc.mark_bar(color="#DC3545").encode()
                        
#                         rule_vc = alt.Chart(pd.DataFrame({'average_vc': [average_vc]})).mark_rule(
#                             color='gray', strokeDash=[3, 3] # [v19.1] Change line color
#                         ).encode(
#                             y=alt.Y('average_vc', title=''),
#                             tooltip=[alt.Tooltip('average_vc', title="Overall Average", format='.2%')]
#                         )
                        
#                         chart_vc = (bars_vc + rule_vc).properties(
#                             background='transparent',
#                             title=f"Overall Average: {average_vc:.2%}"
#                         ).interactive()
#                         st.altair_chart(chart_vc, use_container_width=True)
#                         st.caption("The higher the volatility (VC), the less confident the AI is in its price estimation for the city.")
#             else:
#                 st.warning("No 'final_allowance' data to display the chart.")

# if employee_tab is not None:
#     with employee_tab:
#         st.header("Per Diem Inquiry by City")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("Please analyze a PDF in the 'Report Analysis' tab first.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("Country:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("City:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("There are no registered cities for the selected country.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "Trip Start Date",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "Trip End Date",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("Job Level:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("The end date must be on or after the start date.")
#                     trip_days = 0 # Set to 0
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"Auto-classified trip type: {trip_term_label} Â· {trip_days}-day trip")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("No city data available for this period. Adjust the trip type to 'Short-term' or check city settings.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} Results")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [New 2] Employee tab total card
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} Daily Recommended Per Diem", "No Amount")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**Basis of Calculation (Daily Rate)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"Success {ai_runs}/{ai_attempts} runs"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"Outliers {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"Season {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "No AI Data"
                        
#                         # [New 1] Reason for applying dynamic weights
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | Reason: {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"Short-term base $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"Short-term $ {un_base:,} â†’ Long-term $ {un_display:,}"
#                                         render_stat_card("UN-DSA Basis", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"Short-term base $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"Short-term $ {ai_avg:,} â†’ Long-term $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI Market Estimate (Seasonal Adj.)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"Short-term $ {final_allowance:,} â†’ Long-term $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("Weighted Average Result", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [New 2] Detailed cost breakdown (merged with show_ai_market_detail logic)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # line break
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # Apply long-term/seasonal rates
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI Estimate Details (Daily Rate)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("Est. Food", f"$ {food_display:,}", f"Short-term base: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("Est. Transport", f"$ {trans_display:,}", f"Short-term base: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("Est. Misc", f"$ {misc_display:,}", f"Short-term base: $ {round(mean_misc)}", "muted")
                        
#                         # [Improvement 3] The show_weighted_result card is redundant, so the block below is removed
#                         # (Original second_row_keys logic removed)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("The administrator has hidden the detailed calculation basis.")

# # --- [Improvement 2] Changed admin_tab -> admin_analysis_tab ---
# with admin_analysis_tab:
    
#     # [Improvement 2] Load ADMIN_ACCESS_CODE and check .env
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # Load from .env

#     if not ACCESS_CODE_VALUE:
#         st.error("Security Error: 'ADMIN_ACCESS_CODE' is not set in the .env file. Please stop the app and set the .env file.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [Improvement 3] Rerun on success
#             else:
#                 st.error("The Access Code is incorrect.")
#                 st.stop() # [Improvement 3] Stop on failure
#         else:
#             st.stop() # [Improvement 3] Stop before form submission

#     # --- [Improvement 3] "Report Version Management" feature (analysis_sub_tab) ---
#     st.subheader("Report Version Management")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("Select the active report version:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("No reports have been generated.")

#     # --- [New 4] Past Report Comparison feature (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("Compare Past Reports")
#     if len(history_files) < 2:
#         st.info("At least 2 reports are required for comparison.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("Base Report (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("Comparison Report (B)", history_files, index=0, key="compare_b")
        
#         if st.button("Compare Reports"):
#             if file_a == file_b:
#                 st.warning("You must select two different reports.")
#             else:
#                 with st.spinner("Comparing reports..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["Change ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # Prevent division by zero
#                         df_merged["Change (%)"] = (df_merged["Change ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "Change ($)", "Change (%)"
#                         ]].style.format({"Change (%)": "{:,.1f}%", "Change ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("Failed to load report files.")
    
#     # --- [Improvement 3] "UN-DSA (PDF) Analysis" feature (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) Analysis & AI Execution")
#     st.warning(f"Note that the AI will be called {NUM_AI_CALLS} times, which will consume time and cost. (Improvement 1: Async processing for faster speed)")
#     uploaded_file = st.file_uploader("Upload UN-DSA PDF file.", type="pdf")

#     # --- [Improvement 1] Async AI analysis execution logic ---
#     if uploaded_file and st.button("Run AI Analysis", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error("Please set OPENAI_API_KEY in the .env file.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- Define async execution function ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="Extracting PDF text...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV converting... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("Failed to convert PDF->TSV.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("Failed to process TSV data.")
#                     progress_bar.empty()
#                     return

#                 # Create async OpenAI client
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # List to hold all AI call tasks

#                 # 1. Pre-create all AI call tasks for all cities
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [City1-10runs], [City2-10runs], ... ]

#                 # 2. Execute all tasks asynchronously and collect results
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"Calculating AI estimates... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # Run 10 tasks for this city concurrently
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"Async error during {city_name} analysis: {e}")
#                         market_results = [] # Handle failure

#                     # 3. Process results
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [New 2] Lists for detailed cost breakdown
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [New 2] Add detailed costs
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. Calculate final allowance
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [New 1] Calculate dynamic weights
#                     admin_weights = get_weight_config() # Load admin settings
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # If no UN data, use AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [New 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [New 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [New 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [New 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [New 1] Save dynamic weights
#                         },
#                     }

#                     # [New 1] Calculate final value with dynamic weights
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # Next city

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- Async execution ---
#             with st.spinner("Processing PDF and running AI analysis... (Takes approx. 10-30 seconds)"):
#                 progress_bar = st.progress(0, text="Starting analysis...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [Improvement 3] "Latest Analysis Summary" feature (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] Prevent ArrowInvalid Error ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # Change number to string format like "12%"
#             else:
#                 delta_display = "N/A" # Already "N/A" string
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- Use modified string value
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- Added use_container_width (change to width='stretch' if needed)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))


# def auto_fill_all_city_coordinates() -> tuple[int, int]:
#     """
#     target_cities ì¤‘ lat/lon ì—†ëŠ” ë„ì‹œë“¤ ì¢Œí‘œë¥¼ geopy ë¡œ ìë™ ì±„ìš°ê³ 
#     DB + session_state ì— ì €ì¥í•œ ë’¤ (ì„±ê³µ ê°œìˆ˜, ì‹¤íŒ¨ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
#     """
#     try:
#         geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#     except Exception as e:
#         st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#         return 0, 0

#     current_entries = get_target_city_entries()
#     entries_to_update = [e for e in current_entries if not e.get("lat") or not e.get("lon")]

#     if not entries_to_update:
#         return 0, 0

#     success_count = 0
#     fail_count = 0

#     progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")

#     with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#         for i, entry in enumerate(entries_to_update):
#             city = entry["city"]
#             country = entry["country"]
#             query = f"{city}, {country}"

#             try:
#                 location = geolocator.geocode(query, timeout=5)
#                 time.sleep(1)  # Nominatim rate limit

#                 if location:
#                     entry["lat"] = location.latitude
#                     entry["lon"] = location.longitude
#                     st.toast(f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
#                     success_count += 1
#                 else:
#                     st.toast(f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="â“")
#                     fail_count += 1
#             except (GeocoderTimedOut, GeocoderUnavailable):
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", icon="ğŸ”¥")
#                 fail_count += 1
#             except Exception as e:
#                 st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                 fail_count += 1

#             progress_bar.progress((i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}")

#     # DB + ì„¸ì…˜ì— ì €ì¥
#     set_target_city_entries(current_entries)
#     progress_bar.empty()
#     return success_count, fail_count


# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# with admin_config_tab:
#     # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤. 'ë³´ê³ ì„œ ë¶„ì„ (Admin)' íƒ­ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
#         st.stop()
        
#     # --- [v19.3] ë„ì‹œ í¸ì§‘/ìºì‹œ ê´€ë¦¬ê°€ ê³µìœ í•  ë„ì‹œ ëª©ë¡ì„ íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜ ---
#     current_entries = get_target_city_entries()
#     options = {
#         f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#         for idx, entry in enumerate(current_entries)
#     }
#     sorted_labels = list(options.keys())
    
#     # --- ì½œë°± í•¨ìˆ˜ 1: 'ë„ì‹œ í¸ì§‘' í¼ ë™ê¸°í™” ---
#     def _sync_edit_form_from_selection():
#         if "edit_city_selector" not in st.session_state or not st.session_state.edit_city_selector:
#              # st.session_state.edit_city_selectorê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì¼ ë•Œ
#              if sorted_labels:
#                  st.session_state.edit_city_selector = sorted_labels[0]
#              else:
#                  return # ë„ì‹œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¤‘ë‹¨
             
#         selected_idx = options[st.session_state.edit_city_selector]
#         selected_entry = current_entries[selected_idx]
        
#         st.session_state.edit_region = selected_entry.get("region", "")
#         st.session_state.edit_city = selected_entry.get("city", "")
#         st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#         st.session_state.edit_country = selected_entry.get("country", "")
#         st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
        
#         existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#         st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
        
#         sub_data = selected_entry.get("un_dsa_substitute") or {}
#         st.session_state.edit_sub_city = sub_data.get("city", "")
#         st.session_state.edit_sub_country = sub_data.get("country", "")

#     # --- [v19.3] ì½œë°± í•¨ìˆ˜ 2: 'ìºì‹œ ì¶”ê°€' í¼ ë™ê¸°í™” ---
#     def _sync_cache_form_from_selection():
#         selected_label = st.session_state.get("cache_city_selector") # get()ìœ¼ë¡œ ì˜¤ë¥˜ ë°©ì§€
        
#         if selected_label in options: # 'options' dictë¥¼ ê³µìœ 
#             selected_idx = options[selected_label]
#             selected_entry = current_entries[selected_idx]
#             st.session_state.new_cache_country = selected_entry.get("country", "")
#             st.session_state.new_cache_city = selected_entry.get("city", "")
#             st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
#         else: # (placeholder ì„ íƒ ì‹œ)
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
        
#         # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” í•­ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
#         st.session_state.new_cache_vendor = ""
#         st.session_state.new_cache_category = "Food"
#         st.session_state.new_cache_price = 0.0
#         st.session_state.new_cache_currency = "USD"
#         st.session_state.new_cache_url = ""

#     # --- [v19.3 í•«í”½ìŠ¤] ì½œë°± í•¨ìˆ˜ 3: 'ìºì‹œ ì €ì¥' ë¡œì§ ---
#     def handle_cache_submit():
#         # 1. ìœ íš¨ì„± ê²€ì‚¬
#         if (not st.session_state.new_cache_country or 
#             not st.session_state.new_cache_city or 
#             not st.session_state.new_cache_vendor):
#             st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#             return # ì—¬ê¸°ì„œ ì¤‘ë‹¨ (í¼ ê°’ ìœ ì§€ë¨)

#         # 2. ìƒˆ í•­ëª© ìƒì„±
#         new_entry = {
#             "country": st.session_state.new_cache_country.strip(),
#             "city": st.session_state.new_cache_city.strip(),
#             "neighborhood": st.session_state.new_cache_neighborhood.strip(),
#             "vendor": st.session_state.new_cache_vendor.strip(),
#             "category": st.session_state.new_cache_category,
#             "price": st.session_state.new_cache_price,
#             "currency": st.session_state.new_cache_currency.strip().upper(),
#             "url": st.session_state.new_cache_url.strip(),
#         }
        
#         # 3. íŒŒì¼ì— ì €ì¥
#         if add_menu_cache_entry(new_entry):
#             st.success(f"'{new_entry['vendor']}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            
#             # 4. (ì¤‘ìš”) í¼ ë¦¬ì…‹: session_state ê°’ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”
#             # ì´ ë¡œì§ì€ on_click ì½œë°± ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
#             st.session_state.new_cache_vendor = ""
#             st.session_state.new_cache_category = "Food"
#             st.session_state.new_cache_price = 0.0
#             st.session_state.new_cache_currency = "USD"
#             st.session_state.new_cache_url = ""
#             st.session_state.cache_city_selector = None # ë“œë¡­ë‹¤ìš´ë„ ë¦¬ì…‹
            
#             # st.rerun()ì€ on_click ì½œë°±ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•  í•„ìš” ì—†ìŒ
#         else:
#             st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#     # --- [v19.3 í•«í”½ìŠ¤] ë ---

#     st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")
#         time.sleep(1) # ìœ ì €ê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤Œ
#         st.rerun()

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
#         time.sleep(1)
#         st.rerun()

#     st.divider()
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬ (target_cities_config.json)")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch') # [v19.3] ê²½ê³  ìˆ˜ì •
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     # --- [ì‹ ê·œ 2] ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„± ê¸°ëŠ¥ (ìƒˆë¡œ ì¶”ê°€) ---
#     st.divider()
#     st.subheader("ë„ì‹œ ì¢Œí‘œ ê´€ë¦¬")
    
#     if st.button("ëª¨ë“  ë„ì‹œ ì¢Œí‘œ(Lat/Lon) ìë™ ì™„ì„±", help="target_cities_config.jsonì˜ ëª¨ë“  ë„ì‹œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¢Œí‘œê°€ ì—†ëŠ” ë„ì‹œì— ëŒ€í•´ geopyë¥¼ í˜¸ì¶œí•´ ì¢Œí‘œë¥¼ ìë™ ì €ì¥í•©ë‹ˆë‹¤."):
        
#         # 1. ì§€ì˜¤ì½”ë” ì´ˆê¸°í™”
#         try:
#             geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#         except Exception as e:
#             st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#             st.stop()

#         # 2. ë„ì‹œ ëª©ë¡ ë¡œë“œ
#         current_entries = get_target_city_entries()
#         entries_to_update = [e for e in current_entries if not e.get('lat') or not e.get('lon')]
        
#         if not entries_to_update:
#             st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
#             st.stop()
            
#         st.info(f"ì´ {len(current_entries)}ê°œ ë„ì‹œ ì¤‘, ì¢Œí‘œê°€ ì—†ëŠ” {len(entries_to_update)}ê°œ ë„ì‹œì— ëŒ€í•œ ì¢Œí‘œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
        
#         progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")
#         success_count = 0
#         fail_count = 0
        
#         with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#             for i, entry in enumerate(entries_to_update):
#                 city = entry['city']
#                 country = entry['country']
#                 query = f"{city}, {country}"
                
#                 try:
#                     # 3. API í˜¸ì¶œ
#                     location = geolocator.geocode(query, timeout=5)
#                     time.sleep(1) # (ì¤‘ìš”) Nominatimì˜ API ì œí•œ(ì´ˆë‹¹ 1íšŒ) ì¤€ìˆ˜
                    
#                     if location:
#                         # 4. ì›ë³¸ entryì— lat/lon ì¶”ê°€
#                         entry['lat'] = location.latitude
#                         entry['lon'] = location.longitude
#                         st.toast(f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
#                         success_count += 1
#                     else:
#                         st.toast(f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="â“")
#                         fail_count += 1
                        
#                 except (GeocoderTimedOut, GeocoderUnavailable):
#                     st.toast(f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", icon="ğŸ”¥")
#                     fail_count += 1
#                 except Exception as e:
#                     st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                     fail_count += 1
                
#                 progress_bar.progress((i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}")

#         # 5. ì „ì²´ íŒŒì¼ ì €ì¥
#         set_target_city_entries(current_entries) # (save_target_city_entries í˜¸ì¶œ í¬í•¨)
        
#         st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")
#         st.rerun()
#     # --- [ì‹ ê·œ 2] ë ---


#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()

#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
    
#     if current_entries:
#         # ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state:
#             _sync_edit_form_from_selection()

#         # í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", 
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 selected_idx = options[st.session_state.edit_city_selector]
#                 current_entries[selected_idx] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[selected_idx]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[selected_idx].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()
        
#         if delete_btn:
#             selected_idx = options[st.session_state.edit_city_selector]
#             del current_entries[selected_idx]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun()
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#     st.divider()
#     st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#         # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#         st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
        
#         st.selectbox(
#             "ë„ì‹œ ì„ íƒ (ìë™ ì±„ìš°ê¸°):", 
#             sorted_labels,  # íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜
#             key="cache_city_selector",
#             on_change=_sync_cache_form_from_selection, # ìƒˆë¡œ ë§Œë“  ì½œë°±
#             index=None,
#             placeholder="ë„ì‹œë¥¼ ì„ íƒí•˜ë©´ êµ­ê°€, ë„ì‹œ, ì„¸ë¶€ ì§€ì—­ì´ ìë™ ì…ë ¥ë©ë‹ˆë‹¤."
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ìºì‹œ í¼ ì´ˆê¸°í™”
#         if "new_cache_country" not in st.session_state:
#             _sync_cache_form_from_selection() # ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        
#         # --- [v19.3 í•«í”½ìŠ¤] clear_on_submit=False, on_click ì½œë°± ì‚¬ìš© ---
#         with st.form("add_menu_cache_form"): # clear_on_submit ì œê±°
#             st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 new_cache_country = st.text_input("êµ­ê°€ (Country)", key="new_cache_country", help="ì˜ˆ: Philippines")
#                 new_cache_city = st.text_input("ë„ì‹œ (City)", key="new_cache_city", help="ì˜ˆ: Manila")
#                 new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", key="new_cache_neighborhood", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                 new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", key="new_cache_vendor", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"], key="new_cache_category")
#                 new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01, key="new_cache_price")
#                 new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", key="new_cache_currency", help="ì˜ˆ: PHP, USD")
#                 new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)", key="new_cache_url")
            
#             # [v19.3] on_click ì½œë°±ìœ¼ë¡œ ì €ì¥/ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
#             add_cache_submitted = st.form_submit_button(
#                 "ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥",
#                 on_click=handle_cache_submit # <-- í•µì‹¬ ìˆ˜ì •
#             )
#         # --- [v19.3 í•«í”½ìŠ¤] ë ---

#         # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#         st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#         all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
        
#         if not all_cache_data:
#             st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             # [v19.3] ê²½ê³  ìˆ˜ì •
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], width='stretch')

#             # ì‚­ì œ ê¸°ëŠ¥
#             st.markdown("---")
#             st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
            
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data))
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
            
#             if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
#                 else:
#                     st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ì‹ ê·œ 3] UI ë ---
    


# # 2025-11-13 ìŠ¤íŠ¸ë¦¼ë¦¿ ì •ì‹ ë°°í¬ ì „
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple

# import altair as alt  # [ì‹ ê·œ 2] ê³ ê¸‰ ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# import pydeck as pdk  # [ì‹ ê·œ 2] ê³ ê¸‰ 3D ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬

# # [ì‹ ê·œ 3] menu_cache ì„í¬íŠ¸ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ ê¸°ëŠ¥ì€ ì‘ë™í•˜ì§€ ì•ŠìŒ)
# from geopy.geocoders import Nominatim
# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable
# try:
#     from data_sources.menu_cache import (
#         load_cached_menu_prices, 
#         load_all_cache, 
#         add_menu_cache_entry, 
#         save_cached_menu_prices
#     )
#     MENU_CACHE_ENABLED = True
# except ImportError:
#     st.warning("`data_sources/menu_cache.py` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
#     # (ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì„ì‹œë¡œ ì •ì˜)
#     def load_cached_menu_prices(city: str, country: str, neighborhood: Optional[str]) -> List[Dict[str, Any]]: return []
#     def load_all_cache() -> List[Dict[str, Any]]: return []
#     def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool: return False
#     def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool: return False
#     MENU_CACHE_ENABLED = False


# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# def save_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Persist weight configuration to disk and update the in-memory cache."""
#     normalized = _normalize_weight_config(config)
#     with open(weight_config_path(), "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)

#     global _WEIGHT_CONFIG_CACHE
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return normalized


# def load_weight_config(force: bool = False) -> Dict[str, float]:
#     """Load weight configuration from disk (or defaults when missing)."""
#     global _WEIGHT_CONFIG_CACHE
#     if _WEIGHT_CONFIG_CACHE and not force:
#         return dict(_WEIGHT_CONFIG_CACHE)

#     if not os.path.exists(weight_config_path()):
#         normalized = save_weight_config(DEFAULT_WEIGHT_CONFIG)
#         return dict(normalized)

#     try:
#         with open(weight_config_path(), "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("Weight config must be a JSON object")
#     except Exception:
#         data = DEFAULT_WEIGHT_CONFIG

#     normalized = _normalize_weight_config(data)
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return dict(normalized)


# def get_weight_config() -> Dict[str, float]:
#     """Return the active weight configuration, favouring session state if available."""
#     try:
#         session_config = st.session_state.get("weight_config")  # type: ignore[attr-defined]
#     except RuntimeError:
#         session_config = None

#     if session_config:
#         normalized = _normalize_weight_config(session_config)
#         try:
#             st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#         except RuntimeError:
#             pass
#         return normalized

#     config = load_weight_config()
#     try:
#         st.session_state["weight_config"] = config  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return config


# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """Update weights both in session and on disk."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = save_weight_config(config)
#     try:
#         st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return normalized


# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">Estimated Total Per Diem ({level_label})</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>Calculation</span><br/>
#             $ {daily:,} Ã— {days} days Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Persist UI settings to disk and update cache."""
#     normalized = _normalize_ui_settings(settings)
#     with open(UI_SETTINGS_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """Load UI settings, defaulting gracefully when missing or malformed."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
#     if not os.path.exists(UI_SETTINGS_FILE):
#         normalized = save_ui_settings(DEFAULT_UI_SETTINGS)
#         return dict(normalized)
#     try:
#         with open(UI_SETTINGS_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("UI settings must be a JSON object")
#     except Exception:
#         data = dict(DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(data)
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "Short-term", "Long-term": "Long-term"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
#     return entry


# def load_target_city_entries() -> List[Dict[str, Any]]:
#     if not os.path.exists(TARGET_CONFIG_FILE):
#         save_target_city_entries(DEFAULT_TARGET_CITY_ENTRIES)
#     try:
#         with open(TARGET_CONFIG_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, list):
#             raise ValueError("Invalid target city config format")
#     except Exception:
#         data = DEFAULT_TARGET_CITY_ENTRIES
#     return [normalize_target_entry(item) for item in data]


# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     normalized = [normalize_target_entry(item) for item in entries]
#     with open(TARGET_CONFIG_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)


# TARGET_CITIES_ENTRIES = load_target_city_entries()


# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     return TARGET_CITIES_ENTRIES


# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
#     save_target_city_entries(st.session_state["target_cities_entries"])


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# def get_history_files():
#     if not os.path.exists(DATA_DIR):
#         return []
#     files = [f for f in os.listdir(DATA_DIR) if f.startswith("report_") and f.endswith(".json")]
#     return sorted(files, reverse=True)

# def save_report_data(data):
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = os.path.join(DATA_DIR, f"report_{timestamp}.json")
#     with open(filename, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data


# def load_report_data(filename):
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             try:
#                 data = json.load(f)
#                 return _sanitize_report_data(data)
#             except json.JSONDecodeError: return None
#     return None

# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None
#         search_target = target
#         is_substitute = "un_dsa_substitute" in target
#         if is_substitute: search_target = target["un_dsa_substitute"]
        
#         country_df = df[df['Country'].str.contains(search_target['country'], case=False, na=False)]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- Streamlit UI Configuration ---
# st.set_page_config(layout="wide")
# st.title("AICP: NSUS GROUP Per Diem Calculation & Inquiry System")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [Improvement 3 & New 2] Tab structure change (v18.0) ---
# tab_definitions = [
#     "ğŸ“Š Executive Dashboard", # [New 2] Dashboard tab added
# ]

# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ Per Diem Inquiry (Employee)")

# # Split admin tab into two
# tab_definitions.append("ğŸ“ˆ Report Analysis (Admin)")
# tab_definitions.append("ğŸ› ï¸ System Settings (Admin)")

# tabs = st.tabs(tab_definitions)

# # Assign tab variables
# dashboard_tab = tabs[0]
# tab_index_offset = 1

# if employee_tab_visible:
#     employee_tab = tabs[tab_index_offset]
#     admin_analysis_tab = tabs[tab_index_offset + 1]
#     admin_config_tab = tabs[tab_index_offset + 2]
#     tab_index_offset += 1
# else:
#     employee_tab = None
#     admin_analysis_tab = tabs[tab_index_offset]
#     admin_config_tab = tabs[tab_index_offset + 1]
# # --- [End of modification] ---


# # --- [New 2] "Executive Dashboard" Tab (Newly Added) ---
# # with dashboard_tab:
# #     st.header("Global Cost Dashboard")
# #     st.info("Visualizes the global business trip cost status based on the latest report data.")

# #     # --- [v18.5 Fix] Set Altair theme to auto-detect Streamlit theme ---
# #     try:
# #         alt.theme.enable("streamlit")
# #     except Exception:
# #         # (If library conflict, apply manual theme as before - omitted here)
# #         pass 

# #     history_files = get_history_files()
# #     if not history_files:
# #         st.warning("No data to display. Please run AI analysis at least once in the 'Report Analysis' tab.")
# #     else:
# #         latest_report_file = history_files[0]
# #         st.subheader(f"Reference Report: `{latest_report_file}`")
        
# #         report_data = load_report_data(latest_report_file)
# #         config_entries = get_target_city_entries()
        
# #         if not report_data or 'cities' not in report_data or not config_entries:
# #             st.error("Failed to load data.")
# #         else:
# #             # 1. Prepare DataFrame (Report + Coordinates)
# #             df_report = pd.DataFrame(report_data['cities'])
# #             df_config = pd.DataFrame(config_entries)
            
# #             df_merged = pd.merge(
# #                 df_report,
# #                 df_config,
# #                 left_on=["city", "country_display"],
# #                 right_on=["city", "country"],
# #                 suffixes=("_report", "_config")
# #             )
            
# #             required_map_cols = ['city', 'country', 'lat', 'lon', 'final_allowance']
            
# #             if not all(col in df_merged.columns for col in ['lat', 'lon']):
# #                 st.warning(
# #                     "Coordinate (lat/lon) data for the map is missing. ğŸ—ºï¸\n\n"
# #                     "**Solution:** Go to the 'ğŸ› ï¸ System Settings (Admin)' tab and press the [Auto-complete all city coordinates] button."
# #                 )
# #                 map_data = pd.DataFrame(columns=required_map_cols)
# #             else:
# #                 map_data = df_merged.copy()
# #                 map_data = map_data[required_map_cols]
# #                 map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)
# #                 map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
# #                 map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
# #                 map_data.dropna(subset=['lat', 'lon'], inplace=True)

# #             if map_data.empty:
# #                 st.caption("No data to display on the map. (Check if coordinates were generated.)")
# #             else:
# #                 # 2. Calculate color (R,G,B) and size based on cost
# #                 min_cost = map_data['final_allowance'].min()
# #                 max_cost = map_data['final_allowance'].max()
# #                 range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

# #                 def get_color_and_size(cost):
# #                     norm_cost = (cost - min_cost) / range_cost
# #                     r = int(255 * norm_cost)
# #                     g = int(255 * (1 - norm_cost))
# #                     b = 0
# #                     size = 50000 + (norm_cost * 450000)
# #                     return [r, g, b, 160], size

# #                 color_size = map_data['final_allowance'].apply(get_color_and_size)
# #                 map_data['color'] = [item[0] for item in color_size]
# #                 map_data['size'] = [item[1] for item in color_size]

# #                 # 3. Create Pydeck chart
# #                 view_state = pdk.ViewState(
# #                     latitude=map_data['lat'].mean(),
# #                     longitude=map_data['lon'].mean(),
# #                     zoom=0.5,
# #                     pitch=0,
# #                     bearing=0
# #                 )

# #                 layer = pdk.Layer(
# #                     'ScatterplotLayer',
# #                     data=map_data,
# #                     get_position='[lon, lat]',
# #                     get_color='color',
# #                     get_radius='size',
# #                     pickable=True,
# #                     opacity=0.8,
# #                     stroked=True,
# #                     filled=True,
# #                     radius_scale=0.5,
# #                     get_line_color=[255, 255, 255, 100],
# #                     get_line_width=10000,
# #                 )

# #                 tooltip = {
# #                     "html": "<b>{city}, {country}</b><br/>"
# #                             "Final Allowance: <b>${final_allowance}</b>",
# #                     "style": { "color": "white", "backgroundColor": "#1e3c72" }
# #                 }
                
# #                 r = pdk.Deck(
# #                     layers=[layer],
# #                     initial_view_state=view_state,
# #                     # --- [v18.5 Fix] Removed map_style to use Streamlit default (auto-detect theme) ---
# #                     tooltip=tooltip
# #                 )

# #                 map_col, legend_col = st.columns([4, 1])

# #                 with map_col:
# #                     st.pydeck_chart(r, use_container_width=True)

# #                 with legend_col:
# #                     st.write("##### Legend (Cost)")
# #                     st.markdown(f"""
# #                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
# #                         <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
# #                         <span style="margin-left: 10px;">High Cost (~${max_cost:,.0f})</span>
# #                     </div>
# #                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
# #                         <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
# #                         <span style="margin-left: 10px;">Medium Cost</span>
# #                     </div>
# #                     <div style="display: flex; align-items: center;">
# #                         <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
# #                         <span style="margin-left: 10px;">Low Cost (~${min_cost:,.0f})</span>
# #                     </div>
# #                     """, unsafe_allow_html=True)
# #                     st.caption("The larger the circle and the redder the color, the higher the cost of the city.")

# #                 # 4. (Apply Idea 1) Top 10 Charts
# #                 st.divider()
# #                 col1, col2 = st.columns(2)
                
# #                 if 'final_allowance' in df_merged.columns:
# #                     with col1:
# #                         st.write("##### ğŸ’° Top 10 High Cost Cities (AI Final)")
# #                         top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                        
# #                         # --- [v18.5 Fix] Altair Chart (Theme auto-manages text color) ---
# #                         chart_cost = alt.Chart(top_10_cost_df).mark_bar(
# #                             color="#0D6EFD"
# #                         ).encode(
# #                             x=alt.X('city', sort=None, title="City"),
# #                             y=alt.Y('final_allowance', title="Final Allowance ($)"),
# #                             tooltip=[
# #                                 alt.Tooltip('city', title="City"),
# #                                 alt.Tooltip('final_allowance', title="Final Allowance ($)", format='$,.0f')
# #                             ]
# #                         ).properties(
# #                             background='transparent' # Transparent background
# #                         ).interactive()
# #                         st.altair_chart(chart_cost, use_container_width=True)
                
# #                     with col2:
# #                         st.write("##### âš ï¸ Top 10 High Volatility Cities (AI Confidence)")
# #                         df_report_vc = pd.DataFrame(report_data['cities'])
# #                         df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
# #                         df_report_vc.dropna(subset=['vc'], inplace=True)
                        
# #                         if df_report_vc.empty:
# #                             st.info("Volatility (VC) data is missing. (AI analysis with the latest version is required)")
# #                         else:
# #                             top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                            
# #                             # --- [v18.5 Fix] Altair Chart (Theme auto-manages text color) ---
# #                             chart_vc = alt.Chart(top_10_vc_df).mark_bar(
# #                                 color="#DC3545"
# #                             ).encode(
# #                                 x=alt.X('city', sort=None, title="City"),
# #                                 y=alt.Y('vc', title="Variation Coefficient (VC)", axis=alt.Axis(format='%')),
# #                                 tooltip=[
# #                                     alt.Tooltip('city', title="City"),
# #                                     alt.Tooltip('vc', title="Variation Coefficient (VC)", format='.2%')
# #                                 ]
# #                             ).properties(
# #                                 background='transparent' # Transparent background
# #                             ).interactive()
# #                             st.altair_chart(chart_vc, use_container_width=True)
# #                             st.caption("The higher the volatility (VC), the less confident the AI is in its price estimation for the city.")
# #                 else:
# #                     st.warning("No 'final_allowance' data to display the chart.")
# with dashboard_tab:
#     st.header("Global Cost Dashboard")
#     st.info("Visualizes the global business trip cost status based on the latest report data.")

#     try:
#         alt.theme.enable("streamlit")
#     except Exception:
#         pass 

#     history_files = get_history_files()
#     if not history_files:
#         st.warning("No data to display. Please run AI analysis at least once in the 'Report Analysis' tab.")
#     else:
#         latest_report_file = history_files[0]
#         st.subheader(f"Reference Report: `{latest_report_file}`")
        
#         report_data = load_report_data(latest_report_file)
#         config_entries = get_target_city_entries()
        
#         if not report_data or 'cities' not in report_data or not config_entries:
#             st.error("Failed to load data.")
#         else:
#             # 1. Prepare DataFrame (Report + Coordinates)
#             df_report = pd.DataFrame(report_data['cities'])
#             df_config = pd.DataFrame(config_entries)
            
#             df_merged = pd.merge(
#                 df_report,
#                 df_config,
#                 left_on=["city", "country_display"],
#                 right_on=["city", "country"],
#                 suffixes=("_report", "_config")
#             )
            
#             required_map_cols = ['city', 'country', 'lat', 'lon', 'final_allowance']
            
#             if not all(col in df_merged.columns for col in ['lat', 'lon']):
#                 st.warning(
#                     "Coordinate (lat/lon) data for the map is missing. ğŸ—ºï¸\n\n"
#                     "**Solution:** Go to the 'ğŸ› ï¸ System Settings (Admin)' tab and press the [Auto-complete all city coordinates] button."
#                 )
#                 map_data = pd.DataFrame(columns=required_map_cols)
#             else:
#                 map_data = df_merged.copy()
#                 map_data = map_data[required_map_cols]
#                 map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)
#                 map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
#                 map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
#                 map_data.dropna(subset=['lat', 'lon'], inplace=True)

#             if map_data.empty:
#                 st.caption("No data to display on the map. (Check if coordinates were generated.)")
#             else:
#                 # 2. Calculate color (R,G,B) and size based on cost
#                 min_cost = map_data['final_allowance'].min()
#                 max_cost = map_data['final_allowance'].max()
#                 range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

#                 def get_color_and_size(cost):
#                     norm_cost = (cost - min_cost) / range_cost
#                     r = int(255 * norm_cost)
#                     g = int(255 * (1 - norm_cost))
#                     b = 0
#                     size = 50000 + (norm_cost * 450000)
#                     return [r, g, b, 160], size

#                 color_size = map_data['final_allowance'].apply(get_color_and_size)
#                 map_data['color'] = [item[0] for item in color_size]
#                 map_data['size'] = [item[1] for item in color_size]

#                 # 3. Create Pydeck chart
#                 view_state = pdk.ViewState(
#                     latitude=map_data['lat'].mean(),
#                     longitude=map_data['lon'].mean(),
#                     zoom=0.5,
#                     pitch=0,
#                     bearing=0
#                 )

#                 layer = pdk.Layer(
#                     'ScatterplotLayer',
#                     data=map_data,
#                     get_position='[lon, lat]',
#                     get_color='color',
#                     get_radius='size',
#                     pickable=True,
#                     opacity=0.8,
#                     stroked=True,
#                     filled=True,
#                     radius_scale=0.5,
#                     get_line_color=[255, 255, 255, 100],
#                     get_line_width=10000,
#                 )

#                 tooltip = {
#                     "html": "<b>{city}, {country}</b><br/>"
#                             "Final Allowance: <b>${final_allowance}</b>",
#                     "style": { "color": "white", "backgroundColor": "#1e3c72" }
#                 }
                
#                 r = pdk.Deck(
#                     layers=[layer],
#                     initial_view_state=view_state,
#                     tooltip=tooltip
#                 )

#                 map_col, legend_col = st.columns([4, 1])

#                 with map_col:
#                     st.pydeck_chart(r, use_container_width=True)

#                 with legend_col:
#                     st.write("##### Legend (Cost)")
#                     st.markdown(f"""
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">High Cost (~${max_cost:,.0f})</span>
#                     </div>
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">Medium Cost</span>
#                     </div>
#                     <div style="display: flex; align-items: center;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">Low Cost (~${min_cost:,.0f})</span>
#                     </div>
#                     """, unsafe_allow_html=True)
#                     st.caption("The larger the circle and the redder the color, the higher the cost of the city.")

#             # 4. (Apply Idea 1) Top 10 Charts
#             st.divider()
#             col1, col2 = st.columns(2)
            
#             if 'final_allowance' in df_merged.columns:
#                 with col1:
#                     st.write("##### ğŸ’° Top 10 High Cost Cities (AI Final)")
#                     top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                    
#                     average_cost = df_merged['final_allowance'].mean()
                    
#                     # --- [v19.1 Hotfix] Add 'average' column for tooltip ---
#                     top_10_cost_df['average'] = average_cost
                    
#                     base_cost = alt.Chart(top_10_cost_df).encode(
#                         x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                         y=alt.Y('final_allowance', title="Final Allowance ($)", axis=alt.Axis(format='$,.0f')),
#                         tooltip=[
#                             alt.Tooltip('city', title="City"),
#                             alt.Tooltip('final_allowance', title="Final Allowance ($)", format='$,.0f'),
#                             alt.Tooltip('average', title="Overall Average", format='$,.0f') # <-- Modified
#                         ]
#                     )
                    
#                     bars_cost = base_cost.mark_bar(color="#0D6EFD").encode()
                    
#                     rule_cost = alt.Chart(pd.DataFrame({'average_cost': [average_cost]})).mark_rule(
#                         color='gray', strokeDash=[3, 3] # [v19.1] Change line color
#                     ).encode(
#                         y=alt.Y('average_cost', title=''),
#                         tooltip=[alt.Tooltip('average_cost', title="Overall Average", format='$,.0f')] 
#                     )
                    
#                     chart_cost = (bars_cost + rule_cost).properties(
#                         background='transparent',
#                         title=f"Overall Average: ${average_cost:,.0f}" 
#                     ).interactive()
#                     st.altair_chart(chart_cost, use_container_width=True)
                
#                 with col2:
#                     st.write("##### âš ï¸ Top 10 High Volatility Cities (AI Confidence)")
#                     df_report_vc = pd.DataFrame(report_data['cities'])
#                     df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
#                     df_report_vc.dropna(subset=['vc'], inplace=True)
                    
#                     if df_report_vc.empty:
#                         st.info("Volatility (VC) data is missing. (AI analysis with the latest version is required)")
#                     else:
#                         top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                        
#                         average_vc = df_report_vc['vc'].mean()

#                         # --- [v19.1 Hotfix] Add 'average' column for tooltip ---
#                         top_10_vc_df['average'] = average_vc
                        
#                         base_vc = alt.Chart(top_10_vc_df).encode(
#                             x=alt.X('city', sort=None, title="City", axis=alt.Axis(labelAngle=-45)), 
#                             y=alt.Y('vc', title="Variation Coefficient (VC)", axis=alt.Axis(format='%')),
#                             tooltip=[
#                                 alt.Tooltip('city', title="City"),
#                                 alt.Tooltip('vc', title="Variation Coefficient (VC)", format='.2%'),
#                                 alt.Tooltip('average', title="Overall Average", format='.2%') # <-- Modified
#                             ]
#                         )
                        
#                         bars_vc = base_vc.mark_bar(color="#DC3545").encode()
                        
#                         rule_vc = alt.Chart(pd.DataFrame({'average_vc': [average_vc]})).mark_rule(
#                             color='gray', strokeDash=[3, 3] # [v19.1] Change line color
#                         ).encode(
#                             y=alt.Y('average_vc', title=''),
#                             tooltip=[alt.Tooltip('average_vc', title="Overall Average", format='.2%')]
#                         )
                        
#                         chart_vc = (bars_vc + rule_vc).properties(
#                             background='transparent',
#                             title=f"Overall Average: {average_vc:.2%}"
#                         ).interactive()
#                         st.altair_chart(chart_vc, use_container_width=True)
#                         st.caption("The higher the volatility (VC), the less confident the AI is in its price estimation for the city.")
#             else:
#                 st.warning("No 'final_allowance' data to display the chart.")

# if employee_tab is not None:
#     with employee_tab:
#         st.header("Per Diem Inquiry by City")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("Please analyze a PDF in the 'Report Analysis' tab first.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("Country:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("City:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("There are no registered cities for the selected country.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "Trip Start Date",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "Trip End Date",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("Job Level:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("The end date must be on or after the start date.")
#                     trip_days = 0 # Set to 0
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"Auto-classified trip type: {trip_term_label} Â· {trip_days}-day trip")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("No city data available for this period. Adjust the trip type to 'Short-term' or check city settings.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} Results")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [New 2] Employee tab total card
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} Daily Recommended Per Diem", "No Amount")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**Basis of Calculation (Daily Rate)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"Success {ai_runs}/{ai_attempts} runs"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"Outliers {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"Season {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "No AI Data"
                        
#                         # [New 1] Reason for applying dynamic weights
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | Reason: {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"Short-term base $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"Short-term $ {un_base:,} â†’ Long-term $ {un_display:,}"
#                                         render_stat_card("UN-DSA Basis", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"Short-term base $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"Short-term $ {ai_avg:,} â†’ Long-term $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI Market Estimate (Seasonal Adj.)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"Short-term $ {final_allowance:,} â†’ Long-term $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("Weighted Average Result", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [New 2] Detailed cost breakdown (merged with show_ai_market_detail logic)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # line break
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # Apply long-term/seasonal rates
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI Estimate Details (Daily Rate)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("Est. Food", f"$ {food_display:,}", f"Short-term base: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("Est. Transport", f"$ {trans_display:,}", f"Short-term base: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("Est. Misc", f"$ {misc_display:,}", f"Short-term base: $ {round(mean_misc)}", "muted")
                        
#                         # [Improvement 3] The show_weighted_result card is redundant, so the block below is removed
#                         # (Original second_row_keys logic removed)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("The administrator has hidden the detailed calculation basis.")

# # --- [Improvement 2] Changed admin_tab -> admin_analysis_tab ---
# with admin_analysis_tab:
    
#     # [Improvement 2] Load ADMIN_ACCESS_CODE and check .env
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # Load from .env

#     if not ACCESS_CODE_VALUE:
#         st.error("Security Error: 'ADMIN_ACCESS_CODE' is not set in the .env file. Please stop the app and set the .env file.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [Improvement 3] Rerun on success
#             else:
#                 st.error("The Access Code is incorrect.")
#                 st.stop() # [Improvement 3] Stop on failure
#         else:
#             st.stop() # [Improvement 3] Stop before form submission

#     # --- [Improvement 3] "Report Version Management" feature (analysis_sub_tab) ---
#     st.subheader("Report Version Management")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("Select the active report version:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("No reports have been generated.")

#     # --- [New 4] Past Report Comparison feature (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("Compare Past Reports")
#     if len(history_files) < 2:
#         st.info("At least 2 reports are required for comparison.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("Base Report (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("Comparison Report (B)", history_files, index=0, key="compare_b")
        
#         if st.button("Compare Reports"):
#             if file_a == file_b:
#                 st.warning("You must select two different reports.")
#             else:
#                 with st.spinner("Comparing reports..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["Change ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # Prevent division by zero
#                         df_merged["Change (%)"] = (df_merged["Change ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "Change ($)", "Change (%)"
#                         ]].style.format({"Change (%)": "{:,.1f}%", "Change ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("Failed to load report files.")
    
#     # --- [Improvement 3] "UN-DSA (PDF) Analysis" feature (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) Analysis & AI Execution")
#     st.warning(f"Note that the AI will be called {NUM_AI_CALLS} times, which will consume time and cost. (Improvement 1: Async processing for faster speed)")
#     uploaded_file = st.file_uploader("Upload UN-DSA PDF file.", type="pdf")

#     # --- [Improvement 1] Async AI analysis execution logic ---
#     if uploaded_file and st.button("Run AI Analysis", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error("Please set OPENAI_API_KEY in the .env file.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- Define async execution function ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="Extracting PDF text...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV converting... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("Failed to convert PDF->TSV.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("Failed to process TSV data.")
#                     progress_bar.empty()
#                     return

#                 # Create async OpenAI client
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # List to hold all AI call tasks

#                 # 1. Pre-create all AI call tasks for all cities
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [City1-10runs], [City2-10runs], ... ]

#                 # 2. Execute all tasks asynchronously and collect results
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"Calculating AI estimates... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # Run 10 tasks for this city concurrently
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"Async error during {city_name} analysis: {e}")
#                         market_results = [] # Handle failure

#                     # 3. Process results
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [New 2] Lists for detailed cost breakdown
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [New 2] Add detailed costs
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. Calculate final allowance
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [New 1] Calculate dynamic weights
#                     admin_weights = get_weight_config() # Load admin settings
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # If no UN data, use AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [New 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [New 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [New 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [New 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [New 1] Save dynamic weights
#                         },
#                     }

#                     # [New 1] Calculate final value with dynamic weights
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # Next city

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- Async execution ---
#             with st.spinner("Processing PDF and running AI analysis... (Takes approx. 10-30 seconds)"):
#                 progress_bar = st.progress(0, text="Starting analysis...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [Improvement 3] "Latest Analysis Summary" feature (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] Prevent ArrowInvalid Error ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # Change number to string format like "12%"
#             else:
#                 delta_display = "N/A" # Already "N/A" string
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- Use modified string value
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- Added use_container_width (change to width='stretch' if needed)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))

# # --- [Improvement 3] "System Settings" Tab (admin_config_tab) ---
# with admin_config_tab:
#     # Check password (required)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Code required. Please log in on the 'Report Analysis (Admin)' tab first.")
#         st.stop()
        
#     # --- [v19.3] Define shared city list at the top of the tab for city editing/cache management ---
#     current_entries = get_target_city_entries()
#     options = {
#         f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#         for idx, entry in enumerate(current_entries)
#     }
#     sorted_labels = list(options.keys())
    
#     # --- Callback Function 1: Sync 'Edit City' form ---
#     def _sync_edit_form_from_selection():
#         if "edit_city_selector" not in st.session_state or not st.session_state.edit_city_selector:
#             st.session_state.edit_city_selector = sorted_labels[0]
            
#         selected_idx = options[st.session_state.edit_city_selector]
#         selected_entry = current_entries[selected_idx]
        
#         st.session_state.edit_region = selected_entry.get("region", "")
#         st.session_state.edit_city = selected_entry.get("city", "")
#         st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#         st.session_state.edit_country = selected_entry.get("country", "")
#         st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
        
#         existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#         st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
        
#         sub_data = selected_entry.get("un_dsa_substitute") or {}
#         st.session_state.edit_sub_city = sub_data.get("city", "")
#         st.session_state.edit_sub_country = sub_data.get("country", "")

#     # --- [v19.3] Callback Function 2: Sync 'Add Cache' form ---
#     def _sync_cache_form_from_selection():
#         selected_label = st.session_state.get("cache_city_selector") # Use get() to prevent errors
        
#         if selected_label in options: # Share the 'options' dict
#             selected_idx = options[selected_label]
#             selected_entry = current_entries[selected_idx]
#             st.session_state.new_cache_country = selected_entry.get("country", "")
#             st.session_state.new_cache_city = selected_entry.get("city", "")
#             st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
#         else: # (If placeholder is selected)
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
        
#         # Always initialize remaining fields to default
#         st.session_state.new_cache_vendor = ""
#         st.session_state.new_cache_category = "Food"
#         st.session_state.new_cache_price = 0.0
#         st.session_state.new_cache_currency = "USD"
#         st.session_state.new_cache_url = ""

#     # --- [v19.3 Hotfix] Callback Function 3: 'Save Cache' logic ---
#     def handle_cache_submit():
#         # 1. Validation
#         if (not st.session_state.new_cache_country or 
#             not st.session_state.new_cache_city or 
#             not st.session_state.new_cache_vendor):
#             st.error("Country, City, and Vendor/Item Name are required.")
#             return # Stop here (form values are kept)

#         # 2. Create new entry
#         new_entry = {
#             "country": st.session_state.new_cache_country.strip(),
#             "city": st.session_state.new_cache_city.strip(),
#             "neighborhood": st.session_state.new_cache_neighborhood.strip(),
#             "vendor": st.session_state.new_cache_vendor.strip(),
#             "category": st.session_state.new_cache_category,
#             "price": st.session_state.new_cache_price,
#             "currency": st.session_state.new_cache_currency.strip().upper(),
#             "url": st.session_state.new_cache_url.strip(),
#         }
        
#         # 3. Save to file
#         if add_menu_cache_entry(new_entry):
#             st.success(f"Added '{new_entry['vendor']}' entry to cache.")
            
#             # 4. (Important) Reset form: Manually initialize session_state values
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
#             st.session_state.new_cache_vendor = ""
#             st.session_state.new_cache_category = "Food"
#             st.session_state.new_cache_price = 0.0
#             st.session_state.new_cache_currency = "USD"
#             st.session_state.new_cache_url = ""
#             st.session_state.cache_city_selector = None # Reset dropdown as well
            
#             # st.rerun() is called automatically after on_click callback finishes, no need to call explicitly
#         else:
#             st.error("Failed to add cache entry.")
#     # --- [v19.3 Hotfix] End ---

#     st.subheader("Employee Tab Visibility")
#     visibility_toggle = st.toggle("Show Employee Tab", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key name changed
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # Reflect in session state as well
#         st.success("Employee tab visibility updated. (Applies on refresh)")
#         time.sleep(1) # Give user time to read the message
#         st.rerun()

#     st.subheader("Employee View Section Settings")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("Employee view section settings updated.")
#         time.sleep(1)
#         st.rerun()

#     st.divider()
#     st.subheader("Weight Settings (Default)")
#     st.info("This setting is now used as the default for the 'Dynamic Weight' logic. If AI responses are unstable, the AI weight will be automatically lowered.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("Target City Management (target_cities_config.json)")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # Convert trip_lengths to a readable string
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch') # [v19.3] Fix warning
#     else:
#         st.info("No target cities registered. Please add a new entry below.")

#     # --- [New 2] Auto-complete City Coordinates feature (Newly Added) ---
#     st.divider()
#     st.subheader("City Coordinate Management")
    
#     if st.button("Auto-complete all city coordinates (Lat/Lon)", help="Calls geopy to auto-save coordinates for all cities in target_cities_config.json that are missing them."):
        
#         # 1. Initialize geocoder
#         try:
#             geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#         except Exception as e:
#             st.error(f"Geopy(Nominatim) initialization failed: {e}")
#             st.stop()

#         # 2. Load city list
#         current_entries = get_target_city_entries()
#         entries_to_update = [e for e in current_entries if not e.get('lat') or not e.get('lon')]
        
#         if not entries_to_update:
#             st.success("All cities already have coordinates set. (No update needed)")
#             st.stop()
            
#         st.info(f"Out of {len(current_entries)} total cities, fetching coordinates for {len(entries_to_update)} cities that are missing them...")
        
#         progress_bar = st.progress(0, text="Starting coordinate auto-completion...")
#         success_count = 0
#         fail_count = 0
        
#         with st.spinner("Fetching city coordinates one by one... (This may take a while)"):
#             for i, entry in enumerate(entries_to_update):
#                 city = entry['city']
#                 country = entry['country']
#                 query = f"{city}, {country}"
                
#                 try:
#                     # 3. API Call
#                     location = geolocator.geocode(query, timeout=5)
#                     time.sleep(1) # (Important) Adhere to Nominatim's API limit (1 req/sec)
                    
#                     if location:
#                         # 4. Add lat/lon to the original entry
#                         entry['lat'] = location.latitude
#                         entry['lon'] = location.longitude
#                         st.toast(f"âœ… Success: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
#                         success_count += 1
#                     else:
#                         st.toast(f"âš ï¸ Failed: Could not find coordinates for {query}.", icon="â“")
#                         fail_count += 1
                        
#                 except (GeocoderTimedOut, GeocoderUnavailable):
#                     st.toast(f"âŒ Error: Request for {query} timed out. Please try again later.", icon="ğŸ”¥")
#                     fail_count += 1
#                 except Exception as e:
#                     st.toast(f"âŒ Error: {query} ({e})", icon="ğŸ”¥")
#                     fail_count += 1
                
#                 progress_bar.progress((i + 1) / len(entries_to_update), text=f"Processing: {query}")

#         # 5. Save the entire file
#         set_target_city_entries(current_entries) # (Includes call to save_target_city_entries)
        
#         st.success(f"Coordinate auto-completion finished! (Success: {success_count} / Failed: {fail_count})")
#         st.rerun()
#     # --- [New 2] End ---


#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("Add New City")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["Other (Enter manually)"]
#             region_choice = st.selectbox("Region", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "Other (Enter manually)":
#                 new_region = st.text_input("New Region Name", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("Trip Duration", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("City", key="add_city")
#             neighborhood = st.text_input("Neighborhood (Optional)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("Country", key="add_country")
#             hotel_cluster = st.text_input("Recommended Hotel Cluster (Optional)", key="add_hotel_cluster")

#         with st.expander("UN-DSA Substitute City (Optional)"):
#             substitute_city = st.text_input("Substitute City", key="add_sub_city")
#             substitute_country = st.text_input("Substitute Country", key="add_sub_country")

#         add_submitted = st.form_submit_button("Add")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "Other (Enter manually)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("Region, Country, and City are required fields.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("An identical entry already exists.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"Added {region_value} - {city_name.strip()} entry.")
#                 st.rerun()

#     st.subheader("Edit/Delete Existing City")
    
#     if current_entries:
#         # Connect on_change callback to the Selectbox
#         selected_label = st.selectbox(
#             "Select city to edit", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection
#         )

#         # Initial fill for the form on page load
#         if "edit_region" not in st.session_state:
#             _sync_edit_form_from_selection()

#         # Remove 'value=' from form widgets and use only 'key='
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 region_edit = st.text_input("Region", key="edit_region")
#                 city_edit = st.text_input("City", key="edit_city")
#                 neighborhood_edit = st.text_input("Neighborhood (Optional)", key="edit_neighborhood")
#             with col_f:
#                 country_edit = st.text_input("Country", key="edit_country")
#                 hotel_cluster_edit = st.text_input("Recommended Hotel Cluster (Optional)", key="edit_hotel")

#             trip_lengths_edit = st.multiselect(
#                 "Trip Duration",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", 
#             )

#             with st.expander("UN-DSA Substitute City (Optional)"):
#                 sub_city_edit = st.text_input("Substitute City", key="edit_sub_city")
#                 sub_country_edit = st.text_input("Substitute Country", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("Save Changes")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("Delete", type="secondary")

#         # Modify save/delete logic to read values from session_state
#         if update_btn:
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("Region, Country, and City are required fields.")
#             else:
#                 selected_idx = options[st.session_state.edit_city_selector]
#                 current_entries[selected_idx] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[selected_idx]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[selected_idx].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("Update complete.")
#                 st.rerun()
        
#         if delete_btn:
#             selected_idx = options[st.session_state.edit_city_selector]
#             del current_entries[selected_idx]
#             set_target_city_entries(current_entries)
#             st.warning("Deleted the selected item.")
#             st.rerun()
#     else:
#         st.info("No target cities registered, so there is nothing to edit.")

#     # --- [New 3] Add 'Data Cache Management' UI ---
#     st.divider()
#     st.header("Data Cache Management (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("Failed to load `data_sources/menu_cache.py`. This feature is unavailable.")
#     else:
#         st.info("Manage actual menu/price data for AI to reference when estimating city prices. (Improves AI analysis accuracy)")

#         # 1. Add new cache item form
#         st.subheader("Add New Cache Item")
        
#         st.selectbox(
#             "Select City (Auto-fill):", 
#             sorted_labels, # Variable defined at the top of the tab
#             key="cache_city_selector",
#             on_change=_sync_cache_form_from_selection, # Newly created callback
#             index=None,
#             placeholder="Select a city to auto-fill Country, City, and Neighborhood."
#         )

#         # Initialize cache form on page load
#         if "new_cache_country" not in st.session_state:
#             _sync_cache_form_from_selection() # Initialize with empty values
        
#         # --- [v19.3 Hotfix] Remove clear_on_submit=True, use on_click callback ---
#         with st.form("add_menu_cache_form"):
#             st.write("Enter reference price information for AI analysis. (e.g., restaurant menu, taxi fare notice, etc.)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 new_cache_country = st.text_input("Country", key="new_cache_country", help="e.g.: Philippines")
#                 new_cache_city = st.text_input("City", key="new_cache_city", help="e.g.: Manila")
#                 new_cache_neighborhood = st.text_input("Neighborhood (Optional)", key="new_cache_neighborhood", help="e.g.: Makati (applies to whole city if left blank)")
#                 new_cache_vendor = st.text_input("Vendor/Item Name", key="new_cache_vendor", help="e.g.: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("Category", ["Food", "Transport", "Misc"], key="new_cache_category")
#                 new_cache_price = st.number_input("Price", min_value=0.0, step=0.01, key="new_cache_price")
#                 new_cache_currency = st.text_input("Currency", value="USD", key="new_cache_currency", help="e.g.: PHP, USD")
#                 new_cache_url = st.text_input("Source URL (Optional)", key="new_cache_url")
            
#             # [v19.3] Execute save/init logic via on_click callback
#             add_cache_submitted = st.form_submit_button(
#                 "Save New Cache Item",
#                 on_click=handle_cache_submit
#             )
#         # --- [v19.3 Hotfix] End ---

#         # 2. View and delete existing cache items
#         st.subheader("View/Delete Existing Cache Items")
#         all_cache_data = load_all_cache() # Function from menu_cache.py
        
#         if not all_cache_data:
#             st.info("No cached data is currently saved.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], width='stretch') # [v19.3] Fix warning

#             # Delete function
#             st.markdown("---")
#             st.write("##### Delete Cache Item")
            
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data))
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("Select cache item to delete:", delete_labels, index=None, placeholder="Select item to delete...")
            
#             if label_to_delete and st.button(f"Delete '{label_to_delete}' item", type="primary"):
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"Deleted '{entry_to_delete.get('vendor')}' item.")
#                     st.rerun()
#                 else:
#                     st.error("Failed to delete cache item.")
    

# 2025-11-11 ì „ì²´ ì˜ë¬¸ë²„ì „ ì—…ë°ì´íŠ¸ ì „
# # 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple

# import altair as alt  # [ì‹ ê·œ 2] ê³ ê¸‰ ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# import pydeck as pdk  # [ì‹ ê·œ 2] ê³ ê¸‰ 3D ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬

# # [ì‹ ê·œ 3] menu_cache ì„í¬íŠ¸ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ ê¸°ëŠ¥ì€ ì‘ë™í•˜ì§€ ì•ŠìŒ)
# from geopy.geocoders import Nominatim
# from geopy.exc import GeocoderTimedOut, GeocoderUnavailable
# try:
#     from data_sources.menu_cache import (
#         load_cached_menu_prices, 
#         load_all_cache, 
#         add_menu_cache_entry, 
#         save_cached_menu_prices
#     )
#     MENU_CACHE_ENABLED = True
# except ImportError:
#     st.warning("`data_sources/menu_cache.py` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
#     # (ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì„ì‹œë¡œ ì •ì˜)
#     def load_cached_menu_prices(city: str, country: str, neighborhood: Optional[str]) -> List[Dict[str, Any]]: return []
#     def load_all_cache() -> List[Dict[str, Any]]: return []
#     def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool: return False
#     def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool: return False
#     MENU_CACHE_ENABLED = False


# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# def save_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Persist weight configuration to disk and update the in-memory cache."""
#     normalized = _normalize_weight_config(config)
#     with open(weight_config_path(), "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)

#     global _WEIGHT_CONFIG_CACHE
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return normalized


# def load_weight_config(force: bool = False) -> Dict[str, float]:
#     """Load weight configuration from disk (or defaults when missing)."""
#     global _WEIGHT_CONFIG_CACHE
#     if _WEIGHT_CONFIG_CACHE and not force:
#         return dict(_WEIGHT_CONFIG_CACHE)

#     if not os.path.exists(weight_config_path()):
#         normalized = save_weight_config(DEFAULT_WEIGHT_CONFIG)
#         return dict(normalized)

#     try:
#         with open(weight_config_path(), "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("Weight config must be a JSON object")
#     except Exception:
#         data = DEFAULT_WEIGHT_CONFIG

#     normalized = _normalize_weight_config(data)
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return dict(normalized)


# def get_weight_config() -> Dict[str, float]:
#     """Return the active weight configuration, favouring session state if available."""
#     try:
#         session_config = st.session_state.get("weight_config")  # type: ignore[attr-defined]
#     except RuntimeError:
#         session_config = None

#     if session_config:
#         normalized = _normalize_weight_config(session_config)
#         try:
#             st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#         except RuntimeError:
#             pass
#         return normalized

#     config = load_weight_config()
#     try:
#         st.session_state["weight_config"] = config  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return config


# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """Update weights both in session and on disk."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = save_weight_config(config)
#     try:
#         st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return normalized


# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">{level_label} ê¸°ì¤€ ì˜ˆìƒ ì¼ë¹„ ì´ì•¡</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>ê³„ì‚°ì‹</span><br/>
#             $ {daily:,} Ã— {days}ì¼ ì¼ì • Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Persist UI settings to disk and update cache."""
#     normalized = _normalize_ui_settings(settings)
#     with open(UI_SETTINGS_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """Load UI settings, defaulting gracefully when missing or malformed."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
#     if not os.path.exists(UI_SETTINGS_FILE):
#         normalized = save_ui_settings(DEFAULT_UI_SETTINGS)
#         return dict(normalized)
#     try:
#         with open(UI_SETTINGS_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("UI settings must be a JSON object")
#     except Exception:
#         data = dict(DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(data)
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "ìˆí…€", "Long-term": "ë¡±í…€"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
#     return entry


# def load_target_city_entries() -> List[Dict[str, Any]]:
#     if not os.path.exists(TARGET_CONFIG_FILE):
#         save_target_city_entries(DEFAULT_TARGET_CITY_ENTRIES)
#     try:
#         with open(TARGET_CONFIG_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, list):
#             raise ValueError("Invalid target city config format")
#     except Exception:
#         data = DEFAULT_TARGET_CITY_ENTRIES
#     return [normalize_target_entry(item) for item in data]


# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     normalized = [normalize_target_entry(item) for item in entries]
#     with open(TARGET_CONFIG_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)


# TARGET_CITIES_ENTRIES = load_target_city_entries()


# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     return TARGET_CITIES_ENTRIES


# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
#     save_target_city_entries(st.session_state["target_cities_entries"])


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# def get_history_files():
#     if not os.path.exists(DATA_DIR):
#         return []
#     files = [f for f in os.listdir(DATA_DIR) if f.startswith("report_") and f.endswith(".json")]
#     return sorted(files, reverse=True)

# def save_report_data(data):
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = os.path.join(DATA_DIR, f"report_{timestamp}.json")
#     with open(filename, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data


# def load_report_data(filename):
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             try:
#                 data = json.load(f)
#                 return _sanitize_report_data(data)
#             except json.JSONDecodeError: return None
#     return None

# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None
#         search_target = target
#         is_substitute = "un_dsa_substitute" in target
#         if is_substitute: search_target = target["un_dsa_substitute"]
        
#         country_df = df[df['Country'].str.contains(search_target['country'], case=False, na=False)]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- ìŠ¤íŠ¸ë¦¼ë¦¿ UI êµ¬ì„± ---
# st.set_page_config(layout="wide")
# st.title("AICP: ì¶œì¥ ì¼ë¹„ ê³„ì‚° & ì¡°íšŒ ì‹œìŠ¤í…œ (v16.0 - Async & Dynamic)")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [ê°œì„  3 & ì‹ ê·œ 2] íƒ­ êµ¬ì¡° ë³€ê²½ (v18.0) ---
# tab_definitions = [
#     "ğŸ“Š Executive Dashboard", # [ì‹ ê·œ 2] ëŒ€ì‹œë³´ë“œ íƒ­ ì¶”ê°€
# ]

# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ ì¼ë¹„ ì¡°íšŒ (ì§ì›ìš©)")

# # ê´€ë¦¬ì íƒ­ì„ 2ê°œë¡œ ë¶„ë¦¬
# tab_definitions.append("ğŸ“ˆ ë³´ê³ ì„œ ë¶„ì„ (Admin)")
# tab_definitions.append("ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Admin)")

# tabs = st.tabs(tab_definitions)

# # íƒ­ ë³€ìˆ˜ í• ë‹¹
# dashboard_tab = tabs[0]
# tab_index_offset = 1

# if employee_tab_visible:
#     employee_tab = tabs[tab_index_offset]
#     admin_analysis_tab = tabs[tab_index_offset + 1]
#     admin_config_tab = tabs[tab_index_offset + 2]
#     tab_index_offset += 1
# else:
#     employee_tab = None
#     admin_analysis_tab = tabs[tab_index_offset]
#     admin_config_tab = tabs[tab_index_offset + 1]
# # --- [ìˆ˜ì •] ë ---


# # --- [ì‹ ê·œ 2] "Executive Dashboard" íƒ­ (ìƒˆë¡œ ì¶”ê°€) ---
# # with dashboard_tab:
# #     st.header("Global Cost Dashboard")
# #     st.info("ìµœì‹  ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ ì„¸ê³„ ì¶œì¥ ë¹„ìš© í˜„í™©ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.")

# #     # --- [v18.5 ìˆ˜ì •] Altair í…Œë§ˆê°€ Streamlit í…Œë§ˆë¥¼ ìë™ ê°ì§€í•˜ë„ë¡ ì„¤ì • ---
# #     try:
# #         alt.theme.enable("streamlit")
# #     except Exception:
# #          # (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ ì‹œ, ì´ì „ì²˜ëŸ¼ ìˆ˜ë™ í…Œë§ˆ ì ìš© - ì—¬ê¸°ì„œëŠ” ìƒëµ)
# #         pass 

# #     history_files = get_history_files()
# #     if not history_files:
# #         st.warning("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ AI ë¶„ì„ì„ 1íšŒ ì´ìƒ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
# #     else:
# #         latest_report_file = history_files[0]
# #         st.subheader(f"ê¸°ì¤€ ë³´ê³ ì„œ: `{latest_report_file}`")
        
# #         report_data = load_report_data(latest_report_file)
# #         config_entries = get_target_city_entries()
        
# #         if not report_data or 'cities' not in report_data or not config_entries:
# #             st.error("ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
# #         else:
# #             # 1. ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„ (ë³´ê³ ì„œ + ì¢Œí‘œ)
# #             df_report = pd.DataFrame(report_data['cities'])
# #             df_config = pd.DataFrame(config_entries)
            
# #             df_merged = pd.merge(
# #                 df_report,
# #                 df_config,
# #                 left_on=["city", "country_display"],
# #                 right_on=["city", "country"],
# #                 suffixes=("_report", "_config")
# #             )
            
# #             required_map_cols = ['city', 'country', 'lat', 'lon', 'final_allowance']
            
# #             if not all(col in df_merged.columns for col in ['lat', 'lon']):
# #                 st.warning(
# #                     "ì§€ë„ì— í‘œì‹œí•  ì¢Œí‘œ(lat/lon) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ—ºï¸\n\n"
# #                     "**í•´ê²° ë°©ë²•:** 'ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Admin)' íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ [ëª¨ë“  ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„±] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
# #                 )
# #                 map_data = pd.DataFrame(columns=required_map_cols)
# #             else:
# #                 map_data = df_merged.copy()
# #                 map_data = map_data[required_map_cols]
# #                 map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)
# #                 map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
# #                 map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
# #                 map_data.dropna(subset=['lat', 'lon'], inplace=True)

# #             if map_data.empty:
# #                 st.caption("ì§€ë„ì— í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì¢Œí‘œê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)")
# #             else:
# #                 # 2. ë¹„ìš©ì— ë”°ë¥¸ ìƒ‰ìƒ(R,G,B) ë° í¬ê¸°(Size) ê³„ì‚°
# #                 min_cost = map_data['final_allowance'].min()
# #                 max_cost = map_data['final_allowance'].max()
# #                 range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

# #                 def get_color_and_size(cost):
# #                     norm_cost = (cost - min_cost) / range_cost
# #                     r = int(255 * norm_cost)
# #                     g = int(255 * (1 - norm_cost))
# #                     b = 0
# #                     size = 50000 + (norm_cost * 450000)
# #                     return [r, g, b, 160], size

# #                 color_size = map_data['final_allowance'].apply(get_color_and_size)
# #                 map_data['color'] = [item[0] for item in color_size]
# #                 map_data['size'] = [item[1] for item in color_size]

# #                 # 3. Pydeck ì°¨íŠ¸ ìƒì„±
# #                 view_state = pdk.ViewState(
# #                     latitude=map_data['lat'].mean(),
# #                     longitude=map_data['lon'].mean(),
# #                     zoom=0.5,
# #                     pitch=0,
# #                     bearing=0
# #                 )

# #                 layer = pdk.Layer(
# #                     'ScatterplotLayer',
# #                     data=map_data,
# #                     get_position='[lon, lat]',
# #                     get_color='color',
# #                     get_radius='size',
# #                     pickable=True,
# #                     opacity=0.8,
# #                     stroked=True,
# #                     filled=True,
# #                     radius_scale=0.5,
# #                     get_line_color=[255, 255, 255, 100],
# #                     get_line_width=10000,
# #                 )

# #                 tooltip = {
# #                     "html": "<b>{city}, {country}</b><br/>"
# #                             "ìµœì¢… ìˆ˜ë‹¹: <b>${final_allowance}</b>",
# #                     "style": { "color": "white", "backgroundColor": "#1e3c72" }
# #                 }
                
# #                 r = pdk.Deck(
# #                     layers=[layer],
# #                     initial_view_state=view_state,
# #                     # --- [v18.5 ìˆ˜ì •] map_styleì„ ì‚­ì œí•˜ì—¬ Streamlit ê¸°ë³¸ê°’(í…Œë§ˆ ìë™ ê°ì§€) ì‚¬ìš© ---
# #                     tooltip=tooltip
# #                 )

# #                 map_col, legend_col = st.columns([4, 1])

# #                 with map_col:
# #                     st.pydeck_chart(r, use_container_width=True)

# #                 with legend_col:
# #                     st.write("##### Legend (ë¹„ìš©)")
# #                     st.markdown(f"""
# #                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
# #                         <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
# #                         <span style="margin-left: 10px;">ê³ ë¹„ìš© (~${max_cost:,.0f})</span>
# #                     </div>
# #                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
# #                         <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
# #                         <span style="margin-left: 10px;">ì¤‘ê°„ ë¹„ìš©</span>
# #                     </div>
# #                     <div style="display: flex; align-items: center;">
# #                         <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
# #                         <span style="margin-left: 10px;">ì €ë¹„ìš© (~${min_cost:,.0f})</span>
# #                     </div>
# #                     """, unsafe_allow_html=True)
# #                     st.caption("ì›ì˜ í¬ê¸°ì™€ ìƒ‰ìƒ(ë¶‰ì€ìƒ‰)ì´ í´ìˆ˜ë¡ ë¹„ìš©ì´ ë†’ì€ ë„ì‹œì…ë‹ˆë‹¤.")

# #             # 4. (ì•„ì´ë””ì–´ 1 ì ìš©) Top 10 ì°¨íŠ¸
# #             st.divider()
# #             col1, col2 = st.columns(2)
            
# #             if 'final_allowance' in df_merged.columns:
# #                 with col1:
# #                     st.write("##### ğŸ’° Top 10 ê³ ë¹„ìš© ë„ì‹œ (AI ìµœì¢…ì•ˆ)")
# #                     top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                    
# #                     # --- [v18.5 ìˆ˜ì •] Altair ì°¨íŠ¸ (í…Œë§ˆê°€ ê¸€ììƒ‰ì„ ìë™ ê´€ë¦¬) ---
# #                     chart_cost = alt.Chart(top_10_cost_df).mark_bar(
# #                         color="#0D6EFD"
# #                     ).encode(
# #                         x=alt.X('city', sort=None, title="ë„ì‹œ"),
# #                         y=alt.Y('final_allowance', title="ìµœì¢… ìˆ˜ë‹¹ ($)"),
# #                         tooltip=[
# #                             alt.Tooltip('city', title="ë„ì‹œ"),
# #                             alt.Tooltip('final_allowance', title="ìµœì¢… ìˆ˜ë‹¹ ($)", format='$,.0f')
# #                         ]
# #                     ).properties(
# #                         background='transparent' # ë°°ê²½ íˆ¬ëª…í™”
# #                     ).interactive()
# #                     st.altair_chart(chart_cost, use_container_width=True)
            
# #                 with col2:
# #                     st.write("##### âš ï¸ Top 10 ë³€ë™ì„± ë†’ì€ ë„ì‹œ (AI ì‹ ë¢°ë„)")
# #                     df_report_vc = pd.DataFrame(report_data['cities'])
# #                     df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
# #                     df_report_vc.dropna(subset=['vc'], inplace=True)
                    
# #                     if df_report_vc.empty:
# #                         st.info("ë³€ë™ì„±(VC) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì‹  ë²„ì „ìœ¼ë¡œ AI ë¶„ì„ í•„ìš”)")
# #                     else:
# #                         top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                        
# #                         # --- [v18.5 ìˆ˜ì •] Altair ì°¨íŠ¸ (í…Œë§ˆê°€ ê¸€ììƒ‰ì„ ìë™ ê´€ë¦¬) ---
# #                         chart_vc = alt.Chart(top_10_vc_df).mark_bar(
# #                             color="#DC3545"
# #                         ).encode(
# #                             x=alt.X('city', sort=None, title="ë„ì‹œ"),
# #                             y=alt.Y('vc', title="ë³€ë™ ê³„ìˆ˜ (VC)", axis=alt.Axis(format='%')),
# #                             tooltip=[
# #                                 alt.Tooltip('city', title="ë„ì‹œ"),
# #                                 alt.Tooltip('vc', title="ë³€ë™ ê³„ìˆ˜ (VC)", format='.2%')
# #                             ]
# #                         ).properties(
# #                             background='transparent' # ë°°ê²½ íˆ¬ëª…í™”
# #                         ).interactive()
# #                         st.altair_chart(chart_vc, use_container_width=True)
# #                         st.caption("ë³€ë™ì„±(VC)ì´ ë†’ì„ìˆ˜ë¡ AIê°€ ê°€ê²© ì¶”ì •ì„ í™•ì‹ í•˜ì§€ ëª»í•˜ëŠ” ë„ì‹œì…ë‹ˆë‹¤.")
# #             else:
# #                 st.warning("ì°¨íŠ¸ë¥¼ í‘œì‹œí•  'final_allowance' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
# with dashboard_tab:
#     st.header("Global Cost Dashboard")
#     st.info("ìµœì‹  ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ ì„¸ê³„ ì¶œì¥ ë¹„ìš© í˜„í™©ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.")

#     try:
#         alt.theme.enable("streamlit")
#     except Exception:
#         pass 

#     history_files = get_history_files()
#     if not history_files:
#         st.warning("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ AI ë¶„ì„ì„ 1íšŒ ì´ìƒ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
#     else:
#         latest_report_file = history_files[0]
#         st.subheader(f"ê¸°ì¤€ ë³´ê³ ì„œ: `{latest_report_file}`")
        
#         report_data = load_report_data(latest_report_file)
#         config_entries = get_target_city_entries()
        
#         if not report_data or 'cities' not in report_data or not config_entries:
#             st.error("ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#         else:
#             # 1. ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„ (ë³´ê³ ì„œ + ì¢Œí‘œ)
#             df_report = pd.DataFrame(report_data['cities'])
#             df_config = pd.DataFrame(config_entries)
            
#             df_merged = pd.merge(
#                 df_report,
#                 df_config,
#                 left_on=["city", "country_display"],
#                 right_on=["city", "country"],
#                 suffixes=("_report", "_config")
#             )
            
#             required_map_cols = ['city', 'country', 'lat', 'lon', 'final_allowance']
            
#             if not all(col in df_merged.columns for col in ['lat', 'lon']):
#                 st.warning(
#                     "ì§€ë„ì— í‘œì‹œí•  ì¢Œí‘œ(lat/lon) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ—ºï¸\n\n"
#                     "**í•´ê²° ë°©ë²•:** 'ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Admin)' íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ [ëª¨ë“  ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„±] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
#                 )
#                 map_data = pd.DataFrame(columns=required_map_cols)
#             else:
#                 map_data = df_merged.copy()
#                 map_data = map_data[required_map_cols]
#                 map_data.dropna(subset=['lat', 'lon', 'final_allowance'], inplace=True)
#                 map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
#                 map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
#                 map_data.dropna(subset=['lat', 'lon'], inplace=True)

#             if map_data.empty:
#                 st.caption("ì§€ë„ì— í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì¢Œí‘œê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)")
#             else:
#                 # 2. ë¹„ìš©ì— ë”°ë¥¸ ìƒ‰ìƒ(R,G,B) ë° í¬ê¸°(Size) ê³„ì‚°
#                 min_cost = map_data['final_allowance'].min()
#                 max_cost = map_data['final_allowance'].max()
#                 range_cost = max_cost - min_cost if max_cost > min_cost else 1.0

#                 def get_color_and_size(cost):
#                     norm_cost = (cost - min_cost) / range_cost
#                     r = int(255 * norm_cost)
#                     g = int(255 * (1 - norm_cost))
#                     b = 0
#                     size = 50000 + (norm_cost * 450000)
#                     return [r, g, b, 160], size

#                 color_size = map_data['final_allowance'].apply(get_color_and_size)
#                 map_data['color'] = [item[0] for item in color_size]
#                 map_data['size'] = [item[1] for item in color_size]

#                 # 3. Pydeck ì°¨íŠ¸ ìƒì„±
#                 view_state = pdk.ViewState(
#                     latitude=map_data['lat'].mean(),
#                     longitude=map_data['lon'].mean(),
#                     zoom=0.5,
#                     pitch=0,
#                     bearing=0
#                 )

#                 layer = pdk.Layer(
#                     'ScatterplotLayer',
#                     data=map_data,
#                     get_position='[lon, lat]',
#                     get_color='color',
#                     get_radius='size',
#                     pickable=True,
#                     opacity=0.8,
#                     stroked=True,
#                     filled=True,
#                     radius_scale=0.5,
#                     get_line_color=[255, 255, 255, 100],
#                     get_line_width=10000,
#                 )

#                 tooltip = {
#                     "html": "<b>{city}, {country}</b><br/>"
#                             "ìµœì¢… ìˆ˜ë‹¹: <b>${final_allowance}</b>",
#                     "style": { "color": "white", "backgroundColor": "#1e3c72" }
#                 }
                
#                 r = pdk.Deck(
#                     layers=[layer],
#                     initial_view_state=view_state,
#                     tooltip=tooltip
#                 )

#                 map_col, legend_col = st.columns([4, 1])

#                 with map_col:
#                     st.pydeck_chart(r, use_container_width=True)

#                 with legend_col:
#                     st.write("##### Legend (ë¹„ìš©)")
#                     st.markdown(f"""
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(255, 0, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">ê³ ë¹„ìš© (~${max_cost:,.0f})</span>
#                     </div>
#                     <div style="display: flex; align-items: center; margin-bottom: 5px;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(127, 127, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">ì¤‘ê°„ ë¹„ìš©</span>
#                     </div>
#                     <div style="display: flex; align-items: center;">
#                         <div style="width: 20px; height: 20px; background-color: rgb(0, 255, 0, 0.8); border-radius: 50%; border: 1px solid #FFF;"></div>
#                         <span style="margin-left: 10px;">ì €ë¹„ìš© (~${min_cost:,.0f})</span>
#                     </div>
#                     """, unsafe_allow_html=True)
#                     st.caption("ì›ì˜ í¬ê¸°ì™€ ìƒ‰ìƒ(ë¶‰ì€ìƒ‰)ì´ í´ìˆ˜ë¡ ë¹„ìš©ì´ ë†’ì€ ë„ì‹œì…ë‹ˆë‹¤.")

#             # 4. (ì•„ì´ë””ì–´ 1 ì ìš©) Top 10 ì°¨íŠ¸
#             st.divider()
#             col1, col2 = st.columns(2)
            
#             if 'final_allowance' in df_merged.columns:
#                 with col1:
#                     st.write("##### ğŸ’° Top 10 ê³ ë¹„ìš© ë„ì‹œ (AI ìµœì¢…ì•ˆ)")
#                     top_10_cost_df = df_merged.nlargest(10, 'final_allowance')[['city', 'final_allowance']].reset_index(drop=True)
                    
#                     average_cost = df_merged['final_allowance'].mean()
                    
#                     # --- [v19.1 í•«í”½ìŠ¤] íˆ´íŒìš© 'average' ì»¬ëŸ¼ ì¶”ê°€ ---
#                     top_10_cost_df['average'] = average_cost
                    
#                     base_cost = alt.Chart(top_10_cost_df).encode(
#                         x=alt.X('city', sort=None, title="ë„ì‹œ", axis=alt.Axis(labelAngle=-45)), 
#                         y=alt.Y('final_allowance', title="ìµœì¢… ìˆ˜ë‹¹ ($)", axis=alt.Axis(format='$,.0f')),
#                         tooltip=[
#                             alt.Tooltip('city', title="ë„ì‹œ"),
#                             alt.Tooltip('final_allowance', title="ìµœì¢… ìˆ˜ë‹¹ ($)", format='$,.0f'),
#                             alt.Tooltip('average', title="ì „ì²´ í‰ê· ", format='$,.0f') # <-- ìˆ˜ì •ë¨
#                         ]
#                     )
                    
#                     bars_cost = base_cost.mark_bar(color="#0D6EFD").encode()
                    
#                     rule_cost = alt.Chart(pd.DataFrame({'average_cost': [average_cost]})).mark_rule(
#                         color='gray', strokeDash=[3, 3] # [v19.1] ì„  ìƒ‰ìƒ ë³€ê²½
#                     ).encode(
#                         y=alt.Y('average_cost', title=''),
#                         tooltip=[alt.Tooltip('average_cost', title="ì „ì²´ í‰ê· ", format='$,.0f')] 
#                     )
                    
#                     chart_cost = (bars_cost + rule_cost).properties(
#                         background='transparent',
#                         title=f"ì „ì²´ í‰ê· : ${average_cost:,.0f}" 
#                     ).interactive()
#                     st.altair_chart(chart_cost, use_container_width=True)
            
#                 with col2:
#                     st.write("##### âš ï¸ Top 10 ë³€ë™ì„± ë†’ì€ ë„ì‹œ (AI ì‹ ë¢°ë„)")
#                     df_report_vc = pd.DataFrame(report_data['cities'])
#                     df_report_vc['vc'] = df_report_vc['ai_summary'].apply(lambda x: x.get('ai_consistency_vc') if isinstance(x, dict) else None)
#                     df_report_vc.dropna(subset=['vc'], inplace=True)
                    
#                     if df_report_vc.empty:
#                         st.info("ë³€ë™ì„±(VC) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì‹  ë²„ì „ìœ¼ë¡œ AI ë¶„ì„ í•„ìš”)")
#                     else:
#                         top_10_vc_df = df_report_vc.nlargest(10, 'vc')[['city', 'vc']].reset_index(drop=True)
                        
#                         average_vc = df_report_vc['vc'].mean()

#                         # --- [v19.1 í•«í”½ìŠ¤] íˆ´íŒìš© 'average' ì»¬ëŸ¼ ì¶”ê°€ ---
#                         top_10_vc_df['average'] = average_vc
                        
#                         base_vc = alt.Chart(top_10_vc_df).encode(
#                             x=alt.X('city', sort=None, title="ë„ì‹œ", axis=alt.Axis(labelAngle=-45)), 
#                             y=alt.Y('vc', title="ë³€ë™ ê³„ìˆ˜ (VC)", axis=alt.Axis(format='%')),
#                             tooltip=[
#                                 alt.Tooltip('city', title="ë„ì‹œ"),
#                                 alt.Tooltip('vc', title="ë³€ë™ ê³„ìˆ˜ (VC)", format='.2%'),
#                                 alt.Tooltip('average', title="ì „ì²´ í‰ê· ", format='.2%') # <-- ìˆ˜ì •ë¨
#                             ]
#                         )
                        
#                         bars_vc = base_vc.mark_bar(color="#DC3545").encode()
                        
#                         rule_vc = alt.Chart(pd.DataFrame({'average_vc': [average_vc]})).mark_rule(
#                             color='gray', strokeDash=[3, 3] # [v19.1] ì„  ìƒ‰ìƒ ë³€ê²½
#                         ).encode(
#                             y=alt.Y('average_vc', title=''),
#                             tooltip=[alt.Tooltip('average_vc', title="ì „ì²´ í‰ê· ", format='.2%')]
#                         )
                        
#                         chart_vc = (bars_vc + rule_vc).properties(
#                             background='transparent',
#                             title=f"ì „ì²´ í‰ê· : {average_vc:.2%}"
#                         ).interactive()
#                         st.altair_chart(chart_vc, use_container_width=True)
#                         st.caption("ë³€ë™ì„±(VC)ì´ ë†’ì„ìˆ˜ë¡ AIê°€ ê°€ê²© ì¶”ì •ì„ í™•ì‹ í•˜ì§€ ëª»í•˜ëŠ” ë„ì‹œì…ë‹ˆë‹¤.")
#             else:
#                 st.warning("ì°¨íŠ¸ë¥¼ í‘œì‹œí•  'final_allowance' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# if employee_tab is not None:
#     with employee_tab:
#         st.header("ë„ì‹œë³„ ì¶œì¥ ì¼ë¹„ ì¡°íšŒ")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ PDFë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("êµ­ê°€:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("ë„ì‹œ:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("ì„ íƒí•œ êµ­ê°€ì— ë“±ë¡ëœ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "ì¶œì¥ ì‹œì‘ì¼",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "ì¶œì¥ ì¢…ë£Œì¼",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("ì§ê¸‰:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
#                     trip_days = 0 # 0ìœ¼ë¡œ ì„¤ì •
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"ìë™ ë¶„ë¥˜ëœ ì¶œì¥ ìœ í˜•: {trip_term_label} Â· {trip_days}ì¼ ì¼ì •")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("ì´ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë„ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶œì¥ ìœ í˜•ì„ 'ìˆí…€'ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ë„ì‹œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} ê²°ê³¼")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [ì‹ ê·œ 2] ì§ì› íƒ­ ì´ì•¡ ì¹´ë“œ
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} ì¼ì¼ ê¶Œì¥ ì¼ë¹„", "ê¸ˆì•¡ ì—†ìŒ")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±° (ì¼ë¹„ ê¸°ì¤€)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"ì„±ê³µ {ai_runs}/{ai_attempts}íšŒ"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"ì œì™¸ê°’ {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"ì‹œì¦Œ {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "AI ë°ì´í„° ì—†ìŒ"
                        
#                         # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | ì‚¬ìœ : {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"ìˆí…€ ê¸°ì¤€ $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"ìˆí…€ $ {un_base:,} â†’ ë¡±í…€ $ {un_display:,}"
#                                         render_stat_card("UN-DSA ê¸°ì¤€", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"ìˆí…€ ê¸°ì¤€ $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"ìˆí…€ $ {ai_avg:,} â†’ ë¡±í…€ $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI ì‹œì¥ ì¶”ì • (ì‹œì¦Œ ë³´ì •)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"ìˆí…€ $ {final_allowance:,} â†’ ë¡±í…€ $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("ê°€ì¤‘ í‰ê·  ê²°ê³¼", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ (show_ai_market_detailê³¼ ë¡œì§ í†µí•©)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # ì¤„ ê°„ê²©
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # ë¡±í…€/ì‹œì¦Œ ìš”ìœ¨ ì ìš©
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI ì¶”ì • ìƒì„¸ ë‚´ì—­ (ì¼ë¹„ ê¸°ì¤€)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("ì˜ˆìƒ ì‹ë¹„ (Food)", f"$ {food_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("ì˜ˆìƒ êµí†µë¹„ (Transport)", f"$ {trans_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("ì˜ˆìƒ ê¸°íƒ€ (Misc)", f"$ {misc_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_misc)}", "muted")
                        
#                         # [ê°œì„  3] show_weighted_result ì¹´ë“œê°€ ì¤‘ë³µë˜ë¯€ë¡œ, ì•„ë˜ ë¸”ë¡ì€ ì œê±°
#                         # (ê¸°ì¡´ second_row_keys ë¡œì§ ì œê±°)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("ê´€ë¦¬ìê°€ ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±°ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")

# # --- [ê°œì„  2] admin_tab -> admin_analysis_tab ìœ¼ë¡œ ë³€ê²½ ---
# with admin_analysis_tab:
    
#     # [ê°œì„  2] ADMIN_ACCESS_CODE ë¡œë“œ ë° .env ì²´í¬
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # .envì—ì„œ ë¡œë“œ

#     if not ACCESS_CODE_VALUE:
#         st.error("ë³´ì•ˆ ì˜¤ë¥˜: .env íŒŒì¼ì— 'ADMIN_ACCESS_CODE'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¤‘ì§€í•˜ê³  .env íŒŒì¼ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [ê°œì„  3] ì„±ê³µ ì‹œ ìƒˆë¡œê³ ì¹¨
#             else:
#                 st.error("Access Codeê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#                 st.stop() # [ê°œì„  3] ì‹¤íŒ¨ ì‹œ ì¤‘ì§€
#         else:
#             st.stop() # [ê°œì„  3] í¼ ì œì¶œ ì „ ì¤‘ì§€

#     # --- [ê°œì„  3] "ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.subheader("ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("í™œì„± ë³´ê³ ì„œ ë²„ì „ì„ ì„ íƒí•˜ì„¸ìš”:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("ìƒì„±ëœ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 4] ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ")
#     if len(history_files) < 2:
#         st.info("ë¹„êµí•  ë³´ê³ ì„œê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("ê¸°ì¤€ ë³´ê³ ì„œ (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("ë¹„êµ ë³´ê³ ì„œ (B)", history_files, index=0, key="compare_b")
        
#         if st.button("ë³´ê³ ì„œ ë¹„êµí•˜ê¸°"):
#             if file_a == file_b:
#                 st.warning("ì„œë¡œ ë‹¤ë¥¸ ë³´ê³ ì„œë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
#             else:
#                 with st.spinner("ë³´ê³ ì„œ ë¹„êµ ì¤‘..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["ë³€ë™ì•¡ ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
#                         df_merged["ë³€ë™ë¥  (%)"] = (df_merged["ë³€ë™ì•¡ ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "ë³€ë™ì•¡ ($)", "ë³€ë™ë¥  (%)"
#                         ]].style.format({"ë³€ë™ë¥  (%)": "{:,.1f}%", "ë³€ë™ì•¡ ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("ë³´ê³ ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ê°œì„  3] "UN-DSA (PDF) ë¶„ì„" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) ë¶„ì„ ë° AI ì‹¤í–‰")
#     st.warning(f"AI í˜¸ì¶œì´ {NUM_AI_CALLS}íšŒ ì‹¤í–‰ë˜ë¯€ë¡œ ì‹œê°„ê³¼ ë¹„ìš©ì— ìœ ì˜í•´ ì£¼ì„¸ìš”. (ê°œì„  1: ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)")
#     uploaded_file = st.file_uploader("UN-DSA PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

#     # --- [ê°œì„  1] ë¹„ë™ê¸° AI ë¶„ì„ ì‹¤í–‰ ë¡œì§ ---
#     if uploaded_file and st.button("AI ë¶„ì„ ì‹¤í–‰", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV ë³€í™˜ ì¤‘... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("PDF->TSV ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("TSV ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 # ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

#                 # 1. ëª¨ë“  ë„ì‹œì— ëŒ€í•œ ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë¯¸ë¦¬ ìƒì„±
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [ë„ì‹œ1-10íšŒ], [ë„ì‹œ2-10íšŒ], ... ]

#                 # 2. ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"AI ì¶”ì •ì¹˜ ê³„ì‚° ì¤‘... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # í•´ë‹¹ ë„ì‹œì˜ 10ê°œ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"{city_name} ë¶„ì„ ì¤‘ ë¹„ë™ê¸° ì˜¤ë¥˜: {e}")
#                         market_results = [] # ì‹¤íŒ¨ ì²˜ë¦¬

#                     # 3. ê²°ê³¼ ì²˜ë¦¬
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [ì‹ ê·œ 2] ìƒì„¸ ë¹„ìš© ì¶”ê°€
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. ìµœì¢… ìˆ˜ë‹¹ ê³„ì‚°
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
#                     admin_weights = get_weight_config() # ê´€ë¦¬ì ì„¤ì • ë¡œë“œ
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # UN ë°ì´í„° ì—†ìœ¼ë©´ AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [ì‹ ê·œ 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [ì‹ ê·œ 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [ì‹ ê·œ 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [ì‹ ê·œ 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì €ì¥
#                         },
#                     }

#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ë¡œ ìµœì¢…ê°’ ê³„ì‚°
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # ë‹¤ìŒ ë„ì‹œë¡œ

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ ---
#             with st.spinner("PDF ì²˜ë¦¬ ë° AI ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì•½ 10~30ì´ˆ ì†Œìš”)"):
#                 progress_bar = st.progress(0, text="ë¶„ì„ ì‹œì‘...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [ê°œì„  3] "Latest Analysis Summary" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] ArrowInvalid Error ë°©ì§€ ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # ìˆ«ìë¥¼ "12%" í˜•íƒœì˜ ë¬¸ìì—´ë¡œ ë³€ê²½
#             else:
#                 delta_display = "N/A" # ì´ë¯¸ "N/A" ë¬¸ìì—´
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- ìˆ˜ì •ëœ ë¬¸ìì—´ ê°’ ì‚¬ìš©
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- use_container_width ì¶”ê°€ (í•„ìš”ì‹œ width='stretch'ë¡œ ë³€ê²½)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))

# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# with admin_config_tab:
#     # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤. 'ë³´ê³ ì„œ ë¶„ì„ (Admin)' íƒ­ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
#         st.stop()
        
#     # --- [v19.3] ë„ì‹œ í¸ì§‘/ìºì‹œ ê´€ë¦¬ê°€ ê³µìœ í•  ë„ì‹œ ëª©ë¡ì„ íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜ ---
#     current_entries = get_target_city_entries()
#     options = {
#         f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#         for idx, entry in enumerate(current_entries)
#     }
#     sorted_labels = list(options.keys())
    
#     # --- ì½œë°± í•¨ìˆ˜ 1: 'ë„ì‹œ í¸ì§‘' í¼ ë™ê¸°í™” ---
#     def _sync_edit_form_from_selection():
#         if "edit_city_selector" not in st.session_state or not st.session_state.edit_city_selector:
#              st.session_state.edit_city_selector = sorted_labels[0]
             
#         selected_idx = options[st.session_state.edit_city_selector]
#         selected_entry = current_entries[selected_idx]
        
#         st.session_state.edit_region = selected_entry.get("region", "")
#         st.session_state.edit_city = selected_entry.get("city", "")
#         st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#         st.session_state.edit_country = selected_entry.get("country", "")
#         st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
        
#         existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#         st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
        
#         sub_data = selected_entry.get("un_dsa_substitute") or {}
#         st.session_state.edit_sub_city = sub_data.get("city", "")
#         st.session_state.edit_sub_country = sub_data.get("country", "")

#     # --- [v19.3] ì½œë°± í•¨ìˆ˜ 2: 'ìºì‹œ ì¶”ê°€' í¼ ë™ê¸°í™” ---
#     def _sync_cache_form_from_selection():
#         selected_label = st.session_state.get("cache_city_selector") # get()ìœ¼ë¡œ ì˜¤ë¥˜ ë°©ì§€
        
#         if selected_label in options: # 'options' dictë¥¼ ê³µìœ 
#             selected_idx = options[selected_label]
#             selected_entry = current_entries[selected_idx]
#             st.session_state.new_cache_country = selected_entry.get("country", "")
#             st.session_state.new_cache_city = selected_entry.get("city", "")
#             st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
#         else: # (placeholder ì„ íƒ ì‹œ)
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
        
#         # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” í•­ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
#         st.session_state.new_cache_vendor = ""
#         st.session_state.new_cache_category = "Food"
#         st.session_state.new_cache_price = 0.0
#         st.session_state.new_cache_currency = "USD"
#         st.session_state.new_cache_url = ""

#     # --- [v19.3 í•«í”½ìŠ¤] ì½œë°± í•¨ìˆ˜ 3: 'ìºì‹œ ì €ì¥' ë¡œì§ ---
#     def handle_cache_submit():
#         # 1. ìœ íš¨ì„± ê²€ì‚¬
#         if (not st.session_state.new_cache_country or 
#             not st.session_state.new_cache_city or 
#             not st.session_state.new_cache_vendor):
#             st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#             return # ì—¬ê¸°ì„œ ì¤‘ë‹¨ (í¼ ê°’ ìœ ì§€ë¨)

#         # 2. ìƒˆ í•­ëª© ìƒì„±
#         new_entry = {
#             "country": st.session_state.new_cache_country.strip(),
#             "city": st.session_state.new_cache_city.strip(),
#             "neighborhood": st.session_state.new_cache_neighborhood.strip(),
#             "vendor": st.session_state.new_cache_vendor.strip(),
#             "category": st.session_state.new_cache_category,
#             "price": st.session_state.new_cache_price,
#             "currency": st.session_state.new_cache_currency.strip().upper(),
#             "url": st.session_state.new_cache_url.strip(),
#         }
        
#         # 3. íŒŒì¼ì— ì €ì¥
#         if add_menu_cache_entry(new_entry):
#             st.success(f"'{new_entry['vendor']}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            
#             # 4. (ì¤‘ìš”) í¼ ë¦¬ì…‹: session_state ê°’ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
#             st.session_state.new_cache_vendor = ""
#             st.session_state.new_cache_category = "Food"
#             st.session_state.new_cache_price = 0.0
#             st.session_state.new_cache_currency = "USD"
#             st.session_state.new_cache_url = ""
#             st.session_state.cache_city_selector = None # ë“œë¡­ë‹¤ìš´ë„ ë¦¬ì…‹
            
#             # st.rerun()ì€ on_click ì½œë°±ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•  í•„ìš” ì—†ìŒ
#         else:
#             st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#     # --- [v19.3 í•«í”½ìŠ¤] ë ---

#     st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")
#         time.sleep(1) # ìœ ì €ê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤Œ
#         st.rerun()

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
#         time.sleep(1)
#         st.rerun()

#     st.divider()
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬ (target_cities_config.json)")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch') # [v19.3] ê²½ê³  ìˆ˜ì •
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     # --- [ì‹ ê·œ 2] ë„ì‹œ ì¢Œí‘œ ìë™ ì™„ì„± ê¸°ëŠ¥ (ìƒˆë¡œ ì¶”ê°€) ---
#     st.divider()
#     st.subheader("ë„ì‹œ ì¢Œí‘œ ê´€ë¦¬")
    
#     if st.button("ëª¨ë“  ë„ì‹œ ì¢Œí‘œ(Lat/Lon) ìë™ ì™„ì„±", help="target_cities_config.jsonì˜ ëª¨ë“  ë„ì‹œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¢Œí‘œê°€ ì—†ëŠ” ë„ì‹œì— ëŒ€í•´ geopyë¥¼ í˜¸ì¶œí•´ ì¢Œí‘œë¥¼ ìë™ ì €ì¥í•©ë‹ˆë‹¤."):
        
#         # 1. ì§€ì˜¤ì½”ë” ì´ˆê¸°í™”
#         try:
#             geolocator = Nominatim(user_agent=f"aicp_app_{random.randint(1000,9999)}")
#         except Exception as e:
#             st.error(f"Geopy(Nominatim) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#             st.stop()

#         # 2. ë„ì‹œ ëª©ë¡ ë¡œë“œ
#         current_entries = get_target_city_entries()
#         entries_to_update = [e for e in current_entries if not e.get('lat') or not e.get('lon')]
        
#         if not entries_to_update:
#             st.success("ëª¨ë“  ë„ì‹œì— ì´ë¯¸ ì¢Œí‘œê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”)")
#             st.stop()
            
#         st.info(f"ì´ {len(current_entries)}ê°œ ë„ì‹œ ì¤‘, ì¢Œí‘œê°€ ì—†ëŠ” {len(entries_to_update)}ê°œ ë„ì‹œì— ëŒ€í•œ ì¢Œí‘œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
        
#         progress_bar = st.progress(0, text="ì¢Œí‘œ ìë™ ì™„ì„± ì‹œì‘...")
#         success_count = 0
#         fail_count = 0
        
#         with st.spinner("ë„ì‹œ ì¢Œí‘œë¥¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
#             for i, entry in enumerate(entries_to_update):
#                 city = entry['city']
#                 country = entry['country']
#                 query = f"{city}, {country}"
                
#                 try:
#                     # 3. API í˜¸ì¶œ
#                     location = geolocator.geocode(query, timeout=5)
#                     time.sleep(1) # (ì¤‘ìš”) Nominatimì˜ API ì œí•œ(ì´ˆë‹¹ 1íšŒ) ì¤€ìˆ˜
                    
#                     if location:
#                         # 4. ì›ë³¸ entryì— lat/lon ì¶”ê°€
#                         entry['lat'] = location.latitude
#                         entry['lon'] = location.longitude
#                         st.toast(f"âœ… ì„±ê³µ: {query} ({location.latitude:.4f}, {location.longitude:.4f})", icon="ğŸŒ")
#                         success_count += 1
#                     else:
#                         st.toast(f"âš ï¸ ì‹¤íŒ¨: {query}ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", icon="â“")
#                         fail_count += 1
                        
#                 except (GeocoderTimedOut, GeocoderUnavailable):
#                     st.toast(f"âŒ ì˜¤ë¥˜: {query} ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", icon="ğŸ”¥")
#                     fail_count += 1
#                 except Exception as e:
#                     st.toast(f"âŒ ì˜¤ë¥˜: {query} ({e})", icon="ğŸ”¥")
#                     fail_count += 1
                
#                 progress_bar.progress((i + 1) / len(entries_to_update), text=f"ì²˜ë¦¬ ì¤‘: {query}")

#         # 5. ì „ì²´ íŒŒì¼ ì €ì¥
#         set_target_city_entries(current_entries) # (save_target_city_entries í˜¸ì¶œ í¬í•¨)
        
#         st.success(f"ì¢Œí‘œ ìë™ ì™„ì„± ì™„ë£Œ! (ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count})")
#         st.rerun()
#     # --- [ì‹ ê·œ 2] ë ---


#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()

#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
    
#     if current_entries:
#         # ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state:
#             _sync_edit_form_from_selection()

#         # í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", 
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 selected_idx = options[st.session_state.edit_city_selector]
#                 current_entries[selected_idx] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[selected_idx]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[selected_idx].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()
        
#         if delete_btn:
#             selected_idx = options[st.session_state.edit_city_selector]
#             del current_entries[selected_idx]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun()
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#     st.divider()
#     st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#         # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#         st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
        
#         st.selectbox(
#             "ë„ì‹œ ì„ íƒ (ìë™ ì±„ìš°ê¸°):", 
#             sorted_labels,  # íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜
#             key="cache_city_selector",
#             on_change=_sync_cache_form_from_selection, # ìƒˆë¡œ ë§Œë“  ì½œë°±
#             index=None,
#             placeholder="ë„ì‹œë¥¼ ì„ íƒí•˜ë©´ êµ­ê°€, ë„ì‹œ, ì„¸ë¶€ ì§€ì—­ì´ ìë™ ì…ë ¥ë©ë‹ˆë‹¤."
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ìºì‹œ í¼ ì´ˆê¸°í™”
#         if "new_cache_country" not in st.session_state:
#             _sync_cache_form_from_selection() # ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        
#         # --- [v19.3 í•«í”½ìŠ¤] clear_on_submit=True ì œê±°, on_click ì½œë°± ì‚¬ìš© ---
#         with st.form("add_menu_cache_form"):
#             st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 new_cache_country = st.text_input("êµ­ê°€ (Country)", key="new_cache_country", help="ì˜ˆ: Philippines")
#                 new_cache_city = st.text_input("ë„ì‹œ (City)", key="new_cache_city", help="ì˜ˆ: Manila")
#                 new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", key="new_cache_neighborhood", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                 new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", key="new_cache_vendor", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"], key="new_cache_category")
#                 new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01, key="new_cache_price")
#                 new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", key="new_cache_currency", help="ì˜ˆ: PHP, USD")
#                 new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)", key="new_cache_url")
            
#             # [v19.3] on_click ì½œë°±ìœ¼ë¡œ ì €ì¥/ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
#             add_cache_submitted = st.form_submit_button(
#                 "ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥",
#                 on_click=handle_cache_submit
#             )
#         # --- [v19.3 í•«í”½ìŠ¤] ë ---

#         # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#         st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#         all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
        
#         if not all_cache_data:
#             st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], width='stretch') # [v19.3] ê²½ê³  ìˆ˜ì •

#             # ì‚­ì œ ê¸°ëŠ¥
#             st.markdown("---")
#             st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
            
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data))
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
            
#             if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
#                 else:
#                     st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # --- [ì‹ ê·œ 3] UI ë ---

# 2025-11-06 ì™€ìš°í¬ì¸íŠ¸(ë§µ) ê°œì„  ì „
#  # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple

# # [ì‹ ê·œ 3] menu_cache ì„í¬íŠ¸ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ ê¸°ëŠ¥ì€ ì‘ë™í•˜ì§€ ì•ŠìŒ)
# try:
#     from data_sources.menu_cache import (
#         load_cached_menu_prices, 
#         load_all_cache, 
#         add_menu_cache_entry, 
#         save_cached_menu_prices
#     )
#     MENU_CACHE_ENABLED = True
# except ImportError:
#     st.warning("`data_sources/menu_cache.py` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
#     # (ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì„ì‹œë¡œ ì •ì˜)
#     def load_cached_menu_prices(city: str, country: str, neighborhood: Optional[str]) -> List[Dict[str, Any]]: return []
#     def load_all_cache() -> List[Dict[str, Any]]: return []
#     def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool: return False
#     def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool: return False
#     MENU_CACHE_ENABLED = False


# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# def save_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Persist weight configuration to disk and update the in-memory cache."""
#     normalized = _normalize_weight_config(config)
#     with open(weight_config_path(), "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)

#     global _WEIGHT_CONFIG_CACHE
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return normalized


# def load_weight_config(force: bool = False) -> Dict[str, float]:
#     """Load weight configuration from disk (or defaults when missing)."""
#     global _WEIGHT_CONFIG_CACHE
#     if _WEIGHT_CONFIG_CACHE and not force:
#         return dict(_WEIGHT_CONFIG_CACHE)

#     if not os.path.exists(weight_config_path()):
#         normalized = save_weight_config(DEFAULT_WEIGHT_CONFIG)
#         return dict(normalized)

#     try:
#         with open(weight_config_path(), "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("Weight config must be a JSON object")
#     except Exception:
#         data = DEFAULT_WEIGHT_CONFIG

#     normalized = _normalize_weight_config(data)
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return dict(normalized)


# def get_weight_config() -> Dict[str, float]:
#     """Return the active weight configuration, favouring session state if available."""
#     try:
#         session_config = st.session_state.get("weight_config")  # type: ignore[attr-defined]
#     except RuntimeError:
#         session_config = None

#     if session_config:
#         normalized = _normalize_weight_config(session_config)
#         try:
#             st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#         except RuntimeError:
#             pass
#         return normalized

#     config = load_weight_config()
#     try:
#         st.session_state["weight_config"] = config  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return config


# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """Update weights both in session and on disk."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = save_weight_config(config)
#     try:
#         st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return normalized


# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">{level_label} ê¸°ì¤€ ì˜ˆìƒ ì¼ë¹„ ì´ì•¡</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>ê³„ì‚°ì‹</span><br/>
#             $ {daily:,} Ã— {days}ì¼ ì¼ì • Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Persist UI settings to disk and update cache."""
#     normalized = _normalize_ui_settings(settings)
#     with open(UI_SETTINGS_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """Load UI settings, defaulting gracefully when missing or malformed."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
#     if not os.path.exists(UI_SETTINGS_FILE):
#         normalized = save_ui_settings(DEFAULT_UI_SETTINGS)
#         return dict(normalized)
#     try:
#         with open(UI_SETTINGS_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("UI settings must be a JSON object")
#     except Exception:
#         data = dict(DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(data)
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6)": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term", "Long-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "ìˆí…€", "Long-term": "ë¡±í…€"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry.setdefault("trip_lengths", DEFAULT_TRIP_LENGTH.copy())
#     return entry


# def load_target_city_entries() -> List[Dict[str, Any]]:
#     if not os.path.exists(TARGET_CONFIG_FILE):
#         save_target_city_entries(DEFAULT_TARGET_CITY_ENTRIES)
#     try:
#         with open(TARGET_CONFIG_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, list):
#             raise ValueError("Invalid target city config format")
#     except Exception:
#         data = DEFAULT_TARGET_CITY_ENTRIES
#     return [normalize_target_entry(item) for item in data]


# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     normalized = [normalize_target_entry(item) for item in entries]
#     with open(TARGET_CONFIG_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)


# TARGET_CITIES_ENTRIES = load_target_city_entries()


# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     return TARGET_CITIES_ENTRIES


# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
#     save_target_city_entries(st.session_state["target_cities_entries"])


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# def get_history_files():
#     if not os.path.exists(DATA_DIR):
#         return []
#     files = [f for f in os.listdir(DATA_DIR) if f.startswith("report_") and f.endswith(".json")]
#     return sorted(files, reverse=True)

# def save_report_data(data):
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = os.path.join(DATA_DIR, f"report_{timestamp}.json")
#     with open(filename, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data


# def load_report_data(filename):
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             try:
#                 data = json.load(f)
#                 return _sanitize_report_data(data)
#             except json.JSONDecodeError: return None
#     return None

# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None
#         search_target = target
#         is_substitute = "un_dsa_substitute" in target
#         if is_substitute: search_target = target["un_dsa_substitute"]
        
#         country_df = df[df['Country'].str.contains(search_target['country'], case=False, na=False)]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- ìŠ¤íŠ¸ë¦¼ë¦¿ UI êµ¬ì„± ---
# st.set_page_config(layout="wide")
# st.title("AICP: ì¶œì¥ ì¼ë¹„ ê³„ì‚° & ì¡°íšŒ ì‹œìŠ¤í…œ (v16.0 - Async & Dynamic)")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [ê°œì„  3] íƒ­ êµ¬ì¡° ë³€ê²½ ---
# tab_definitions = []
# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ ì¼ë¹„ ì¡°íšŒ (ì§ì›ìš©)")

# # ê´€ë¦¬ì íƒ­ì„ 2ê°œë¡œ ë¶„ë¦¬
# tab_definitions.append("ğŸ“ˆ ë³´ê³ ì„œ ë¶„ì„ (Admin)")
# tab_definitions.append("ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Admin)")

# tabs = st.tabs(tab_definitions)

# employee_tab = None
# admin_analysis_tab = None
# admin_config_tab = None

# if employee_tab_visible:
#     employee_tab = tabs[0]
#     admin_analysis_tab = tabs[1]
#     admin_config_tab = tabs[2]
# else:
#     admin_analysis_tab = tabs[0]
#     admin_config_tab = tabs[1]
# # --- [ê°œì„  3] ë ---


# if employee_tab is not None:
#     with employee_tab:
#         st.header("ë„ì‹œë³„ ì¶œì¥ ì¼ë¹„ ì¡°íšŒ")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ PDFë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("êµ­ê°€:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("ë„ì‹œ:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("ì„ íƒí•œ êµ­ê°€ì— ë“±ë¡ëœ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "ì¶œì¥ ì‹œì‘ì¼",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "ì¶œì¥ ì¢…ë£Œì¼",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("ì§ê¸‰:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
#                     trip_days = 0 # 0ìœ¼ë¡œ ì„¤ì •
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"ìë™ ë¶„ë¥˜ëœ ì¶œì¥ ìœ í˜•: {trip_term_label} Â· {trip_days}ì¼ ì¼ì •")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("ì´ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë„ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶œì¥ ìœ í˜•ì„ 'ìˆí…€'ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ë„ì‹œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} ê²°ê³¼")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [ì‹ ê·œ 2] ì§ì› íƒ­ ì´ì•¡ ì¹´ë“œ
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} ì¼ì¼ ê¶Œì¥ ì¼ë¹„", "ê¸ˆì•¡ ì—†ìŒ")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±° (ì¼ë¹„ ê¸°ì¤€)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"ì„±ê³µ {ai_runs}/{ai_attempts}íšŒ"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"ì œì™¸ê°’ {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"ì‹œì¦Œ {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "AI ë°ì´í„° ì—†ìŒ"
                        
#                         # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | ì‚¬ìœ : {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"ìˆí…€ ê¸°ì¤€ $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"ìˆí…€ $ {un_base:,} â†’ ë¡±í…€ $ {un_display:,}"
#                                         render_stat_card("UN-DSA ê¸°ì¤€", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"ìˆí…€ ê¸°ì¤€ $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"ìˆí…€ $ {ai_avg:,} â†’ ë¡±í…€ $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI ì‹œì¥ ì¶”ì • (ì‹œì¦Œ ë³´ì •)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"ìˆí…€ $ {final_allowance:,} â†’ ë¡±í…€ $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("ê°€ì¤‘ í‰ê·  ê²°ê³¼", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ (show_ai_market_detailê³¼ ë¡œì§ í†µí•©)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # ì¤„ ê°„ê²©
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # ë¡±í…€/ì‹œì¦Œ ìš”ìœ¨ ì ìš©
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI ì¶”ì • ìƒì„¸ ë‚´ì—­ (ì¼ë¹„ ê¸°ì¤€)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("ì˜ˆìƒ ì‹ë¹„ (Food)", f"$ {food_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("ì˜ˆìƒ êµí†µë¹„ (Transport)", f"$ {trans_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("ì˜ˆìƒ ê¸°íƒ€ (Misc)", f"$ {misc_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_misc)}", "muted")
                        
#                         # [ê°œì„  3] show_weighted_result ì¹´ë“œê°€ ì¤‘ë³µë˜ë¯€ë¡œ, ì•„ë˜ ë¸”ë¡ì€ ì œê±°
#                         # (ê¸°ì¡´ second_row_keys ë¡œì§ ì œê±°)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("ê´€ë¦¬ìê°€ ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±°ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")

# # --- [ê°œì„  2] admin_tab -> admin_analysis_tab ìœ¼ë¡œ ë³€ê²½ ---
# with admin_analysis_tab:
    
#     # [ê°œì„  2] ADMIN_ACCESS_CODE ë¡œë“œ ë° .env ì²´í¬
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # .envì—ì„œ ë¡œë“œ

#     if not ACCESS_CODE_VALUE:
#         st.error("ë³´ì•ˆ ì˜¤ë¥˜: .env íŒŒì¼ì— 'ADMIN_ACCESS_CODE'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¤‘ì§€í•˜ê³  .env íŒŒì¼ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [ê°œì„  3] ì„±ê³µ ì‹œ ìƒˆë¡œê³ ì¹¨
#             else:
#                 st.error("Access Codeê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#                 st.stop() # [ê°œì„  3] ì‹¤íŒ¨ ì‹œ ì¤‘ì§€
#         else:
#             st.stop() # [ê°œì„  3] í¼ ì œì¶œ ì „ ì¤‘ì§€

#     # --- [ê°œì„  3] "ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.subheader("ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("í™œì„± ë³´ê³ ì„œ ë²„ì „ì„ ì„ íƒí•˜ì„¸ìš”:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("ìƒì„±ëœ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 4] ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ")
#     if len(history_files) < 2:
#         st.info("ë¹„êµí•  ë³´ê³ ì„œê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("ê¸°ì¤€ ë³´ê³ ì„œ (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("ë¹„êµ ë³´ê³ ì„œ (B)", history_files, index=0, key="compare_b")
        
#         if st.button("ë³´ê³ ì„œ ë¹„êµí•˜ê¸°"):
#             if file_a == file_b:
#                 st.warning("ì„œë¡œ ë‹¤ë¥¸ ë³´ê³ ì„œë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
#             else:
#                 with st.spinner("ë³´ê³ ì„œ ë¹„êµ ì¤‘..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["ë³€ë™ì•¡ ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
#                         df_merged["ë³€ë™ë¥  (%)"] = (df_merged["ë³€ë™ì•¡ ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "ë³€ë™ì•¡ ($)", "ë³€ë™ë¥  (%)"
#                         ]].style.format({"ë³€ë™ë¥  (%)": "{:,.1f}%", "ë³€ë™ì•¡ ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("ë³´ê³ ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ê°œì„  3] "UN-DSA (PDF) ë¶„ì„" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) ë¶„ì„ ë° AI ì‹¤í–‰")
#     st.warning(f"AI í˜¸ì¶œì´ {NUM_AI_CALLS}íšŒ ì‹¤í–‰ë˜ë¯€ë¡œ ì‹œê°„ê³¼ ë¹„ìš©ì— ìœ ì˜í•´ ì£¼ì„¸ìš”. (ê°œì„  1: ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)")
#     uploaded_file = st.file_uploader("UN-DSA PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

#     # --- [ê°œì„  1] ë¹„ë™ê¸° AI ë¶„ì„ ì‹¤í–‰ ë¡œì§ ---
#     if uploaded_file and st.button("AI ë¶„ì„ ì‹¤í–‰", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV ë³€í™˜ ì¤‘... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("PDF->TSV ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("TSV ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 # ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

#                 # 1. ëª¨ë“  ë„ì‹œì— ëŒ€í•œ ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë¯¸ë¦¬ ìƒì„±
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [ë„ì‹œ1-10íšŒ], [ë„ì‹œ2-10íšŒ], ... ]

#                 # 2. ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"AI ì¶”ì •ì¹˜ ê³„ì‚° ì¤‘... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # í•´ë‹¹ ë„ì‹œì˜ 10ê°œ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"{city_name} ë¶„ì„ ì¤‘ ë¹„ë™ê¸° ì˜¤ë¥˜: {e}")
#                         market_results = [] # ì‹¤íŒ¨ ì²˜ë¦¬

#                     # 3. ê²°ê³¼ ì²˜ë¦¬
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [ì‹ ê·œ 2] ìƒì„¸ ë¹„ìš© ì¶”ê°€
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. ìµœì¢… ìˆ˜ë‹¹ ê³„ì‚°
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
#                     admin_weights = get_weight_config() # ê´€ë¦¬ì ì„¤ì • ë¡œë“œ
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # UN ë°ì´í„° ì—†ìœ¼ë©´ AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [ì‹ ê·œ 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [ì‹ ê·œ 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [ì‹ ê·œ 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [ì‹ ê·œ 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì €ì¥
#                         },
#                     }

#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ë¡œ ìµœì¢…ê°’ ê³„ì‚°
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # ë‹¤ìŒ ë„ì‹œë¡œ

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ ---
#             with st.spinner("PDF ì²˜ë¦¬ ë° AI ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì•½ 10~30ì´ˆ ì†Œìš”)"):
#                 progress_bar = st.progress(0, text="ë¶„ì„ ì‹œì‘...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [ê°œì„  3] "Latest Analysis Summary" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] ArrowInvalid Error ë°©ì§€ ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # ìˆ«ìë¥¼ "12%" í˜•íƒœì˜ ë¬¸ìì—´ë¡œ ë³€ê²½
#             else:
#                 delta_display = "N/A" # ì´ë¯¸ "N/A" ë¬¸ìì—´
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- ìˆ˜ì •ëœ ë¬¸ìì—´ ê°’ ì‚¬ìš©
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- use_container_width ì¶”ê°€ (í•„ìš”ì‹œ width='stretch'ë¡œ ë³€ê²½)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))

# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# with admin_config_tab:
#     # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤. 'ë³´ê³ ì„œ ë¶„ì„ (Admin)' íƒ­ì—ì„œ ë¨¼ì € ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
#         st.stop()
        
#     # --- [íŒ¨ì¹˜ v17.1] ë„ì‹œ í¸ì§‘/ìºì‹œ ê´€ë¦¬ê°€ ê³µìœ í•  ë„ì‹œ ëª©ë¡ì„ íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜ ---
#     current_entries = get_target_city_entries()
#     options = {
#         f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#         for idx, entry in enumerate(current_entries)
#     }
#     sorted_labels = list(options.keys())
    
#     # --- ì½œë°± í•¨ìˆ˜ 1: 'ë„ì‹œ í¸ì§‘' í¼ ë™ê¸°í™” ---
#     def _sync_edit_form_from_selection():
#         if "edit_city_selector" not in st.session_state:
#              st.session_state.edit_city_selector = sorted_labels[0]
             
#         selected_idx = options[st.session_state.edit_city_selector]
#         selected_entry = current_entries[selected_idx]
        
#         st.session_state.edit_region = selected_entry.get("region", "")
#         st.session_state.edit_city = selected_entry.get("city", "")
#         st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#         st.session_state.edit_country = selected_entry.get("country", "")
#         st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
        
#         existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#         st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
        
#         sub_data = selected_entry.get("un_dsa_substitute") or {}
#         st.session_state.edit_sub_city = sub_data.get("city", "")
#         st.session_state.edit_sub_country = sub_data.get("country", "")

#     # --- [íŒ¨ì¹˜ v17.1] ì½œë°± í•¨ìˆ˜ 2: 'ìºì‹œ ì¶”ê°€' í¼ ë™ê¸°í™” ---
#     def _sync_cache_form_from_selection():
#         selected_label = st.session_state.get("cache_city_selector") # get()ìœ¼ë¡œ ì˜¤ë¥˜ ë°©ì§€
        
#         if selected_label in options: # 'options' dictë¥¼ ê³µìœ 
#             selected_idx = options[selected_label]
#             selected_entry = current_entries[selected_idx]
#             st.session_state.new_cache_country = selected_entry.get("country", "")
#             st.session_state.new_cache_city = selected_entry.get("city", "")
#             st.session_state.new_cache_neighborhood = selected_entry.get("neighborhood", "")
#         else: # (placeholder ì„ íƒ ì‹œ)
#             st.session_state.new_cache_country = ""
#             st.session_state.new_cache_city = ""
#             st.session_state.new_cache_neighborhood = ""
        
#         # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” í•­ìƒ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
#         st.session_state.new_cache_vendor = ""
#         st.session_state.new_cache_category = "Food"
#         st.session_state.new_cache_price = 0.0
#         st.session_state.new_cache_currency = "USD"
#         st.session_state.new_cache_url = ""
#     # --- [íŒ¨ì¹˜ v17.1] ë ---

#     st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")
#         time.sleep(1) # ìœ ì €ê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤Œ
#         st.rerun()

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
#         time.sleep(1)
#         st.rerun()

#     st.divider()
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬ (target_cities_config.json)")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width='stretch')
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()

#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
#     # current_entries = get_target_city_entries() # íƒ­ ìƒë‹¨ìœ¼ë¡œ ì´ë™
    
#     if current_entries:
#         # options = { ... } # íƒ­ ìƒë‹¨ìœ¼ë¡œ ì´ë™
#         # sorted_labels = list(options.keys()) # íƒ­ ìƒë‹¨ìœ¼ë¡œ ì´ë™
#         # def _sync_edit_form_from_selection(): # íƒ­ ìƒë‹¨ìœ¼ë¡œ ì´ë™

#         # ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state:
#             _sync_edit_form_from_selection()

#         # í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", 
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 selected_idx = options[st.session_state.edit_city_selector]
#                 current_entries[selected_idx] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[selected_idx]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[selected_idx].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()
        
#         if delete_btn:
#             selected_idx = options[st.session_state.edit_city_selector]
#             del current_entries[selected_idx]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun()
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#     st.divider()
#     st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#         # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#         st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
        
#         # --- [íŒ¨ì¹˜ v17.1] ë„ì‹œ ìë™ ì™„ì„±ì„ ìœ„í•œ ë“œë¡­ë‹¤ìš´ ì¶”ê°€ ---
#         st.selectbox(
#             "ë„ì‹œ ì„ íƒ (ìë™ ì±„ìš°ê¸°):", 
#             sorted_labels,  # íƒ­ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜
#             key="cache_city_selector",
#             on_change=_sync_cache_form_from_selection, # ìƒˆë¡œ ë§Œë“  ì½œë°±
#             index=None,
#             placeholder="ë„ì‹œë¥¼ ì„ íƒí•˜ë©´ êµ­ê°€, ë„ì‹œ, ì„¸ë¶€ ì§€ì—­ì´ ìë™ ì…ë ¥ë©ë‹ˆë‹¤."
#         )

#         # í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ ìºì‹œ í¼ ì´ˆê¸°í™”
#         if "new_cache_country" not in st.session_state:
#             _sync_cache_form_from_selection() # ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
#         # --- [íŒ¨ì¹˜ v17.1] ë ---
        
#         with st.form("add_menu_cache_form", clear_on_submit=True):
#             st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 # --- [íŒ¨ì¹˜ v17.1] ëª¨ë“  ì…ë ¥ ìœ„ì ¯ì— key=... ì¶”ê°€ ---
#                 new_cache_country = st.text_input("êµ­ê°€ (Country)", key="new_cache_country", help="ì˜ˆ: Philippines")
#                 new_cache_city = st.text_input("ë„ì‹œ (City)", key="new_cache_city", help="ì˜ˆ: Manila")
#                 new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", key="new_cache_neighborhood", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                 new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", key="new_cache_vendor", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"], key="new_cache_category")
#                 new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01, key="new_cache_price")
#                 new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", key="new_cache_currency", help="ì˜ˆ: PHP, USD")
#                 new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)", key="new_cache_url")
            
#             add_cache_submitted = st.form_submit_button("ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥")

#             if add_cache_submitted:
#                 # --- [íŒ¨ì¹˜ v17.1] ë¡œì»¬ ë³€ìˆ˜ ëŒ€ì‹  st.session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜´ ---
#                 if (not st.session_state.new_cache_country or 
#                     not st.session_state.new_cache_city or 
#                     not st.session_state.new_cache_vendor):
#                     st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#                 else:
#                     new_entry = {
#                         "country": st.session_state.new_cache_country.strip(),
#                         "city": st.session_state.new_cache_city.strip(),
#                         "neighborhood": st.session_state.new_cache_neighborhood.strip(),
#                         "vendor": st.session_state.new_cache_vendor.strip(),
#                         "category": st.session_state.new_cache_category,
#                         "price": st.session_state.new_cache_price,
#                         "currency": st.session_state.new_cache_currency.strip().upper(),
#                         "url": st.session_state.new_cache_url.strip(),
#                     }
                    
#                     # menu_cache.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•­ëª© ì¶”ê°€
#                     if add_menu_cache_entry(new_entry):
#                         st.success(f"'{new_entry['vendor']}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                         # í¼ì´ clear_on_submit=Trueì´ë¯€ë¡œ í¼ ê°’ì„ ë‹¤ì‹œ ì´ˆê¸°í™”
#                         _sync_cache_form_from_selection()
#                         st.rerun()
#                     else:
#                         st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

#         # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#         st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#         all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
        
#         if not all_cache_data:
#             st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], width='stretch') # [HOTFIX] width='stretch' ì ìš©

#             # ì‚­ì œ ê¸°ëŠ¥
#             st.markdown("---")
#             st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
            
#             # ì‚­ì œí•  í•­ëª©ì„ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ í•œ ë ˆì´ë¸” ìƒì„± (ìµœì‹  í•­ëª©ì´ ìœ„ë¡œ)
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data)) # reversed()ë¡œ ìµœì‹  í•­ëª©ì´ ë¨¼ì € ë³´ì´ê²Œ
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
            
#             if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                 # ê±°ê¾¸ë¡œ ë§¤í•‘ëœ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 # menu_cache.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²´ íŒŒì¼ ì €ì¥
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
#                 else:
#                     st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ì‹ ê·œ 3] UI ë ---

# 202511-05
# # 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple

# # [ì‹ ê·œ 3] menu_cache ì„í¬íŠ¸ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ ê¸°ëŠ¥ì€ ì‘ë™í•˜ì§€ ì•ŠìŒ)
# try:
#     from data_sources.menu_cache import (
#         load_cached_menu_prices, 
#         load_all_cache, 
#         add_menu_cache_entry, 
#         save_cached_menu_prices
#     )
#     MENU_CACHE_ENABLED = True
# except ImportError:
#     st.warning("`data_sources/menu_cache.py` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
#     # (ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì„ì‹œë¡œ ì •ì˜)
#     def load_cached_menu_prices(city: str, country: str, neighborhood: Optional[str]) -> List[Dict[str, Any]]: return []
#     def load_all_cache() -> List[Dict[str, Any]]: return []
#     def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool: return False
#     def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool: return False
#     MENU_CACHE_ENABLED = False


# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# def save_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Persist weight configuration to disk and update the in-memory cache."""
#     normalized = _normalize_weight_config(config)
#     with open(weight_config_path(), "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)

#     global _WEIGHT_CONFIG_CACHE
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return normalized


# def load_weight_config(force: bool = False) -> Dict[str, float]:
#     """Load weight configuration from disk (or defaults when missing)."""
#     global _WEIGHT_CONFIG_CACHE
#     if _WEIGHT_CONFIG_CACHE and not force:
#         return dict(_WEIGHT_CONFIG_CACHE)

#     if not os.path.exists(weight_config_path()):
#         normalized = save_weight_config(DEFAULT_WEIGHT_CONFIG)
#         return dict(normalized)

#     try:
#         with open(weight_config_path(), "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("Weight config must be a JSON object")
#     except Exception:
#         data = DEFAULT_WEIGHT_CONFIG

#     normalized = _normalize_weight_config(data)
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return dict(normalized)


# def get_weight_config() -> Dict[str, float]:
#     """Return the active weight configuration, favouring session state if available."""
#     try:
#         session_config = st.session_state.get("weight_config")  # type: ignore[attr-defined]
#     except RuntimeError:
#         session_config = None

#     if session_config:
#         normalized = _normalize_weight_config(session_config)
#         try:
#             st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#         except RuntimeError:
#             pass
#         return normalized

#     config = load_weight_config()
#     try:
#         st.session_state["weight_config"] = config  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return config


# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """Update weights both in session and on disk."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = save_weight_config(config)
#     try:
#         st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return normalized


# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">{level_label} ê¸°ì¤€ ì˜ˆìƒ ì¼ë¹„ ì´ì•¡</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>ê³„ì‚°ì‹</span><br/>
#             $ {daily:,} Ã— {days}ì¼ ì¼ì • Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Persist UI settings to disk and update cache."""
#     normalized = _normalize_ui_settings(settings)
#     with open(UI_SETTINGS_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """Load UI settings, defaulting gracefully when missing or malformed."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
#     if not os.path.exists(UI_SETTINGS_FILE):
#         normalized = save_ui_settings(DEFAULT_UI_SETTINGS)
#         return dict(normalized)
#     try:
#         with open(UI_SETTINGS_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("UI settings must be a JSON object")
#     except Exception:
#         data = dict(DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(data)
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6)": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "ìˆí…€", "Long-term": "ë¡±í…€"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return entry


# def load_target_city_entries() -> List[Dict[str, Any]]:
#     if not os.path.exists(TARGET_CONFIG_FILE):
#         save_target_city_entries(DEFAULT_TARGET_CITY_ENTRIES)
#     try:
#         with open(TARGET_CONFIG_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, list):
#             raise ValueError("Invalid target city config format")
#     except Exception:
#         data = DEFAULT_TARGET_CITY_ENTRIES
#     return [normalize_target_entry(item) for item in data]


# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     normalized = [normalize_target_entry(item) for item in entries]
#     with open(TARGET_CONFIG_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)


# TARGET_CITIES_ENTRIES = load_target_city_entries()


# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     return TARGET_CITIES_ENTRIES


# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
#     save_target_city_entries(st.session_state["target_cities_entries"])


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# def get_history_files():
#     if not os.path.exists(DATA_DIR):
#         return []
#     files = [f for f in os.listdir(DATA_DIR) if f.startswith("report_") and f.endswith(".json")]
#     return sorted(files, reverse=True)

# def save_report_data(data):
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = os.path.join(DATA_DIR, f"report_{timestamp}.json")
#     with open(filename, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data


# def load_report_data(filename):
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             try:
#                 data = json.load(f)
#                 return _sanitize_report_data(data)
#             except json.JSONDecodeError: return None
#     return None

# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None
#         search_target = target
#         is_substitute = "un_dsa_substitute" in target
#         if is_substitute: search_target = target["un_dsa_substitute"]
        
#         country_df = df[df['Country'].str.contains(search_target['country'], case=False, na=False)]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- ìŠ¤íŠ¸ë¦¼ë¦¿ UI êµ¬ì„± ---
# st.set_page_config(layout="wide")
# st.title("AICP: ì¶œì¥ ì¼ë¹„ ê³„ì‚° & ì¡°íšŒ ì‹œìŠ¤í…œ (v16.0 - Async & Dynamic)")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [ê°œì„  3] íƒ­ êµ¬ì¡° ë³€ê²½ ---
# tab_definitions = []
# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ ì¼ë¹„ ì¡°íšŒ (ì§ì›ìš©)")

# # ê´€ë¦¬ì íƒ­ì„ 2ê°œë¡œ ë¶„ë¦¬
# tab_definitions.append("ğŸ“ˆ ë³´ê³ ì„œ ë¶„ì„ (Admin)")
# tab_definitions.append("ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Admin)")

# tabs = st.tabs(tab_definitions)

# employee_tab = None
# admin_analysis_tab = None
# admin_config_tab = None

# if employee_tab_visible:
#     employee_tab = tabs[0]
#     admin_analysis_tab = tabs[1]
#     admin_config_tab = tabs[2]
# else:
#     admin_analysis_tab = tabs[0]
#     admin_config_tab = tabs[1]
# # --- [ê°œì„  3] ë ---


# if employee_tab is not None:
#     with employee_tab:
#         st.header("ë„ì‹œë³„ ì¶œì¥ ì¼ë¹„ ì¡°íšŒ")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ PDFë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("êµ­ê°€:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("ë„ì‹œ:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("ì„ íƒí•œ êµ­ê°€ì— ë“±ë¡ëœ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "ì¶œì¥ ì‹œì‘ì¼",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "ì¶œì¥ ì¢…ë£Œì¼",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("ì§ê¸‰:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
#                     trip_days = 0 # 0ìœ¼ë¡œ ì„¤ì •
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"ìë™ ë¶„ë¥˜ëœ ì¶œì¥ ìœ í˜•: {trip_term_label} Â· {trip_days}ì¼ ì¼ì •")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("ì´ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë„ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶œì¥ ìœ í˜•ì„ 'ìˆí…€'ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ë„ì‹œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} ê²°ê³¼")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [ì‹ ê·œ 2] ì§ì› íƒ­ ì´ì•¡ ì¹´ë“œ
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} ì¼ì¼ ê¶Œì¥ ì¼ë¹„", "ê¸ˆì•¡ ì—†ìŒ")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±° (ì¼ë¹„ ê¸°ì¤€)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"ì„±ê³µ {ai_runs}/{ai_attempts}íšŒ"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"ì œì™¸ê°’ {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"ì‹œì¦Œ {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "AI ë°ì´í„° ì—†ìŒ"
                        
#                         # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | ì‚¬ìœ : {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"ìˆí…€ ê¸°ì¤€ $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"ìˆí…€ $ {un_base:,} â†’ ë¡±í…€ $ {un_display:,}"
#                                         render_stat_card("UN-DSA ê¸°ì¤€", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"ìˆí…€ ê¸°ì¤€ $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"ìˆí…€ $ {ai_avg:,} â†’ ë¡±í…€ $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI ì‹œì¥ ì¶”ì • (ì‹œì¦Œ ë³´ì •)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"ìˆí…€ $ {final_allowance:,} â†’ ë¡±í…€ $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("ê°€ì¤‘ í‰ê·  ê²°ê³¼", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ (show_ai_market_detailê³¼ ë¡œì§ í†µí•©)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # ì¤„ ê°„ê²©
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # ë¡±í…€/ì‹œì¦Œ ìš”ìœ¨ ì ìš©
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI ì¶”ì • ìƒì„¸ ë‚´ì—­ (ì¼ë¹„ ê¸°ì¤€)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("ì˜ˆìƒ ì‹ë¹„ (Food)", f"$ {food_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("ì˜ˆìƒ êµí†µë¹„ (Transport)", f"$ {trans_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("ì˜ˆìƒ ê¸°íƒ€ (Misc)", f"$ {misc_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_misc)}", "muted")
                        
#                         # [ê°œì„  3] show_weighted_result ì¹´ë“œê°€ ì¤‘ë³µë˜ë¯€ë¡œ, ì•„ë˜ ë¸”ë¡ì€ ì œê±°
#                         # (ê¸°ì¡´ second_row_keys ë¡œì§ ì œê±°)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("ê´€ë¦¬ìê°€ ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±°ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")

# # --- [ê°œì„  2] admin_tab -> admin_analysis_tab ìœ¼ë¡œ ë³€ê²½ ---
# with admin_analysis_tab:
    
#     # [ê°œì„  2] ADMIN_ACCESS_CODE ë¡œë“œ ë° .env ì²´í¬
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # .envì—ì„œ ë¡œë“œ

#     if not ACCESS_CODE_VALUE:
#         st.error("ë³´ì•ˆ ì˜¤ë¥˜: .env íŒŒì¼ì— 'ADMIN_ACCESS_CODE'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¤‘ì§€í•˜ê³  .env íŒŒì¼ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [ê°œì„  3] ì„±ê³µ ì‹œ ìƒˆë¡œê³ ì¹¨
#             else:
#                 st.error("Access Codeê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#                 st.stop() # [ê°œì„  3] ì‹¤íŒ¨ ì‹œ ì¤‘ì§€
#         else:
#             st.stop() # [ê°œì„  3] í¼ ì œì¶œ ì „ ì¤‘ì§€

#     # --- [ê°œì„  3] "ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.subheader("ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("í™œì„± ë³´ê³ ì„œ ë²„ì „ì„ ì„ íƒí•˜ì„¸ìš”:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("ìƒì„±ëœ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 4] ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ")
#     if len(history_files) < 2:
#         st.info("ë¹„êµí•  ë³´ê³ ì„œê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("ê¸°ì¤€ ë³´ê³ ì„œ (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("ë¹„êµ ë³´ê³ ì„œ (B)", history_files, index=0, key="compare_b")
        
#         if st.button("ë³´ê³ ì„œ ë¹„êµí•˜ê¸°"):
#             if file_a == file_b:
#                 st.warning("ì„œë¡œ ë‹¤ë¥¸ ë³´ê³ ì„œë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
#             else:
#                 with st.spinner("ë³´ê³ ì„œ ë¹„êµ ì¤‘..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["ë³€ë™ì•¡ ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
#                         df_merged["ë³€ë™ë¥  (%)"] = (df_merged["ë³€ë™ì•¡ ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "ë³€ë™ì•¡ ($)", "ë³€ë™ë¥  (%)"
#                         ]].style.format({"ë³€ë™ë¥  (%)": "{:,.1f}%", "ë³€ë™ì•¡ ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("ë³´ê³ ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ê°œì„  3] "UN-DSA (PDF) ë¶„ì„" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) ë¶„ì„ ë° AI ì‹¤í–‰")
#     st.warning(f"AI í˜¸ì¶œì´ {NUM_AI_CALLS}íšŒ ì‹¤í–‰ë˜ë¯€ë¡œ ì‹œê°„ê³¼ ë¹„ìš©ì— ìœ ì˜í•´ ì£¼ì„¸ìš”. (ê°œì„  1: ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)")
#     uploaded_file = st.file_uploader("UN-DSA PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

#     # --- [ê°œì„  1] ë¹„ë™ê¸° AI ë¶„ì„ ì‹¤í–‰ ë¡œì§ ---
#     if uploaded_file and st.button("AI ë¶„ì„ ì‹¤í–‰", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV ë³€í™˜ ì¤‘... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("PDF->TSV ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("TSV ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 # ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

#                 # 1. ëª¨ë“  ë„ì‹œì— ëŒ€í•œ ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë¯¸ë¦¬ ìƒì„±
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [ë„ì‹œ1-10íšŒ], [ë„ì‹œ2-10íšŒ], ... ]

#                 # 2. ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"AI ì¶”ì •ì¹˜ ê³„ì‚° ì¤‘... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # í•´ë‹¹ ë„ì‹œì˜ 10ê°œ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"{city_name} ë¶„ì„ ì¤‘ ë¹„ë™ê¸° ì˜¤ë¥˜: {e}")
#                         market_results = [] # ì‹¤íŒ¨ ì²˜ë¦¬

#                     # 3. ê²°ê³¼ ì²˜ë¦¬
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [ì‹ ê·œ 2] ìƒì„¸ ë¹„ìš© ì¶”ê°€
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. ìµœì¢… ìˆ˜ë‹¹ ê³„ì‚°
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
#                     admin_weights = get_weight_config() # ê´€ë¦¬ì ì„¤ì • ë¡œë“œ
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # UN ë°ì´í„° ì—†ìœ¼ë©´ AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [ì‹ ê·œ 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [ì‹ ê·œ 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [ì‹ ê·œ 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [ì‹ ê·œ 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì €ì¥
#                         },
#                     }

#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ë¡œ ìµœì¢…ê°’ ê³„ì‚°
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # ë‹¤ìŒ ë„ì‹œë¡œ

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ ---
#             with st.spinner("PDF ì²˜ë¦¬ ë° AI ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì•½ 10~30ì´ˆ ì†Œìš”)"):
#                 progress_bar = st.progress(0, text="ë¶„ì„ ì‹œì‘...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [ê°œì„  3] "Latest Analysis Summary" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] ArrowInvalid Error ë°©ì§€ ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # ìˆ«ìë¥¼ "12%" í˜•íƒœì˜ ë¬¸ìì—´ë¡œ ë³€ê²½
#             else:
#                 delta_display = "N/A" # ì´ë¯¸ "N/A" ë¬¸ìì—´
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- ìˆ˜ì •ëœ ë¬¸ìì—´ ê°’ ì‚¬ìš©
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- use_container_width ì¶”ê°€ (í•„ìš”ì‹œ width='stretch'ë¡œ ë³€ê²½)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))

# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# with admin_config_tab:
#     # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
#         st.stop()
        
#     st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

#     st.divider()
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], use_container_width=True)
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()

#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
#     current_entries = get_target_city_entries()
    
#     if current_entries:
#         # 1. ë“œë¡­ë‹¤ìš´ ì˜µì…˜ êµ¬ì„±
#         options = {
#             f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#             for idx, entry in enumerate(current_entries)
#         }
#         sorted_labels = list(options.keys())

#         # 2. on_change ì½œë°± í•¨ìˆ˜ ì •ì˜ (ìœ„ì ¯ë³´ë‹¤ ë¨¼ì €)
#         def _sync_edit_form_from_selection():
#             # ë“œë¡­ë‹¤ìš´ì—ì„œ í˜„ì¬ ì„ íƒëœ ê°’ì„ ê°€ì ¸ì˜´
#             if "edit_city_selector" not in st.session_state:
#                  st.session_state.edit_city_selector = sorted_labels[0]
                 
#             selected_idx = options[st.session_state.edit_city_selector]
#             selected_entry = current_entries[selected_idx]
            
#             # session_stateì˜ ê°’ì„ ì„ íƒëœ ë„ì‹œì˜ ë°ì´í„°ë¡œ ê°•ì œ ì—…ë°ì´íŠ¸
#             st.session_state.edit_region = selected_entry.get("region", "")
#             st.session_state.edit_city = selected_entry.get("city", "")
#             st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#             st.session_state.edit_country = selected_entry.get("country", "")
#             st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
            
#             # ì¶œì¥ ê¸°ê°„ (trip_lengths) ì„¤ì •
#             existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#             st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
            
#             # UN-DSA ëŒ€ì²´ ë„ì‹œ ì„¤ì •
#             sub_data = selected_entry.get("un_dsa_substitute") or {}
#             st.session_state.edit_sub_city = sub_data.get("city", "")
#             st.session_state.edit_sub_country = sub_data.get("country", "")

#         # 3. ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection  # <-- [ìˆ˜ì •] ì½œë°± í•¨ìˆ˜ ì—°ê²°
#         )

#         # 4. í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state:
#             # ì²« ë¡œë“œ ì‹œ, selectboxì˜ ê¸°ë³¸ê°’(ì²« ë²ˆì§¸ í•­ëª©)ì— ë§ì¶° í¼ì„ ì±„ì›€
#             _sync_edit_form_from_selection()

#         # 5. í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             # [ìˆ˜ì •] default=... ëŒ€ì‹  key=... ì‚¬ìš©
#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", # 'default' ëŒ€ì‹  'key'ë¡œ ìƒíƒœ ê´€ë¦¬
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # 6. ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             # [ìˆ˜ì •] ìœ„ì ¯ ë³€ìˆ˜(region_edit) ëŒ€ì‹  st.session_stateì—ì„œ ì§ì ‘ ê°’ì„ ì½ìŒ
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 current_entries[options[selected_label]] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[options[selected_label]]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[options[selected_label]].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()  # <-- [í™•ì¸] ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë˜ì–´ ìˆìŒ
        
#         if delete_btn:
#             del current_entries[options[selected_label]]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun() # <-- [í™•ì¸] ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë˜ì–´ ìˆìŒ

#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#     st.divider()
#     st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#         # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#         st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
#         with st.form("add_menu_cache_form", clear_on_submit=True):
#             st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 new_cache_country = st.text_input("êµ­ê°€ (Country)", help="ì˜ˆ: Philippines")
#                 new_cache_city = st.text_input("ë„ì‹œ (City)", help="ì˜ˆ: Manila")
#                 new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                 new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"])
#                 new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01)
#                 new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", help="ì˜ˆ: PHP, USD")
#                 new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)")
            
#             add_cache_submitted = st.form_submit_button("ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥")

#             if add_cache_submitted:
#                 if not new_cache_country or not new_cache_city or not new_cache_vendor:
#                     st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#                 else:
#                     new_entry = {
#                         "country": new_cache_country.strip(),
#                         "city": new_cache_city.strip(),
#                         "neighborhood": new_cache_neighborhood.strip(),
#                         "vendor": new_cache_vendor.strip(),
#                         "category": new_cache_category,
#                         "price": new_cache_price,
#                         "currency": new_cache_currency.strip().upper(),
#                         "url": new_cache_url.strip(),
#                     }
#                     # menu_cache.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•­ëª© ì¶”ê°€
#                     if add_menu_cache_entry(new_entry):
#                         st.success(f"'{new_cache_vendor}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                         st.rerun()
#                     else:
#                         st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

#         # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#         st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#         all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
        
#         if not all_cache_data:
#             st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], use_container_width=True)

#             # ì‚­ì œ ê¸°ëŠ¥
#             st.markdown("---")
#             st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
            
#             # ì‚­ì œí•  í•­ëª©ì„ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ í•œ ë ˆì´ë¸” ìƒì„± (ìµœì‹  í•­ëª©ì´ ìœ„ë¡œ)
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data)) # reversed()ë¡œ ìµœì‹  í•­ëª©ì´ ë¨¼ì € ë³´ì´ê²Œ
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
            
#             if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                 # ê±°ê¾¸ë¡œ ë§¤í•‘ëœ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 # menu_cache.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²´ íŒŒì¼ ì €ì¥
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
#                 else:
#                     st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ì‹ ê·œ 3] UI ë ---

#     st.divider() # <-- ì´ê²ƒì´ 'ë¹„ì¤‘ ì„¤ì •' ì„¹ì…˜ê³¼ êµ¬ë¶„í•˜ëŠ” ì„ ì…ë‹ˆë‹¤.
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     # ... (ì´í›„ ë¹„ì¤‘ ì„¤ì • í¼ ì½”ë“œê°€ ì´ì–´ì§) ...




# # 2025-10-20-16 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (v16.0 - Async, Dynamic Weights, Full Admin)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv httpx
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.
# # 3. .env íŒŒì¼ì— ADMIN_ACCESS_CODE="<ë¹„ë°€ë²ˆí˜¸>"ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# import asyncio  # [ê°œì„  1] ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles, stdev  # [ì‹ ê·œ 1] stdev ì¶”ê°€
# from typing import Any, Dict, List, Optional, Set, Tuple

# # [ì‹ ê·œ 3] menu_cache ì„í¬íŠ¸ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ ê¸°ëŠ¥ì€ ì‘ë™í•˜ì§€ ì•ŠìŒ)
# try:
#     from data_sources.menu_cache import (
#         load_cached_menu_prices, 
#         load_all_cache, 
#         add_menu_cache_entry, 
#         save_cached_menu_prices
#     )
#     MENU_CACHE_ENABLED = True
# except ImportError:
#     st.warning("`data_sources/menu_cache.py` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
#     # (ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì„ì‹œë¡œ ì •ì˜)
#     def load_cached_menu_prices(city: str, country: str, neighborhood: Optional[str]) -> List[Dict[str, Any]]: return []
#     def load_all_cache() -> List[Dict[str, Any]]: return []
#     def add_menu_cache_entry(new_entry: Dict[str, Any]) -> bool: return False
#     def save_cached_menu_prices(all_samples: List[Dict[str, Any]]) -> bool: return False
#     MENU_CACHE_ENABLED = False


# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# def save_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Persist weight configuration to disk and update the in-memory cache."""
#     normalized = _normalize_weight_config(config)
#     with open(weight_config_path(), "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)

#     global _WEIGHT_CONFIG_CACHE
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return normalized


# def load_weight_config(force: bool = False) -> Dict[str, float]:
#     """Load weight configuration from disk (or defaults when missing)."""
#     global _WEIGHT_CONFIG_CACHE
#     if _WEIGHT_CONFIG_CACHE and not force:
#         return dict(_WEIGHT_CONFIG_CACHE)

#     if not os.path.exists(weight_config_path()):
#         normalized = save_weight_config(DEFAULT_WEIGHT_CONFIG)
#         return dict(normalized)

#     try:
#         with open(weight_config_path(), "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("Weight config must be a JSON object")
#     except Exception:
#         data = DEFAULT_WEIGHT_CONFIG

#     normalized = _normalize_weight_config(data)
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return dict(normalized)


# def get_weight_config() -> Dict[str, float]:
#     """Return the active weight configuration, favouring session state if available."""
#     try:
#         session_config = st.session_state.get("weight_config")  # type: ignore[attr-defined]
#     except RuntimeError:
#         session_config = None

#     if session_config:
#         normalized = _normalize_weight_config(session_config)
#         try:
#             st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#         except RuntimeError:
#             pass
#         return normalized

#     config = load_weight_config()
#     try:
#         st.session_state["weight_config"] = config  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return config


# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """Update weights both in session and on disk."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = save_weight_config(config)
#     try:
#         st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return normalized


# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ (ì¤‘ë³µ)"), # [ì‹ ê·œ 2] ì¤‘ë³µëœ ì¹´ë“œ
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         # ì´ ìŠ¤íƒ€ì¼ì€ ì»¤ìŠ¤í…€ ìƒ‰ìƒì„ ìœ ì§€í•©ë‹ˆë‹¤ (ì–‘ìª½ ëª¨ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë³´ì„)
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem; color: #ffffff;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem; color: #ffffff;",
#         "caption": "font-size:1.1rem;opacity:0.95; color: #ffffff;",
#     },
#     "secondary": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--secondary-background-color); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
#     "muted": {
#         # [ìˆ˜ì •] Streamlit í…Œë§ˆ ë³€ìˆ˜ ì‚¬ìš©
#         "container": "padding:1.1rem;border-radius:14px;background-color: var(--gray-100); border: 1px solid var(--gray-300);",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem; color: var(--text-color);",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem; color: var(--text-color);",
#         "caption": "font-size:0.85rem; color: var(--gray-600);", # ìº¡ì…˜ì€ íšŒìƒ‰ ê³„ì—´ ì‚¬ìš©
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
    
#     # [ìˆ˜ì •] ìº¡ì…˜ì— ìŠ¤íƒ€ì¼ ì ìš©
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
    
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">{level_label} ê¸°ì¤€ ì˜ˆìƒ ì¼ë¹„ ì´ì•¡</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>ê³„ì‚°ì‹</span><br/>
#             $ {daily:,} Ã— {days}ì¼ ì¼ì • Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Persist UI settings to disk and update cache."""
#     normalized = _normalize_ui_settings(settings)
#     with open(UI_SETTINGS_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """Load UI settings, defaulting gracefully when missing or malformed."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
#     if not os.path.exists(UI_SETTINGS_FILE):
#         normalized = save_ui_settings(DEFAULT_UI_SETTINGS)
#         return dict(normalized)
#     try:
#         with open(UI_SETTINGS_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("UI settings must be a JSON object")
#     except Exception:
#         data = dict(DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(data)
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6)": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "ìˆí…€", "Long-term": "ë¡±í…€"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return entry


# def load_target_city_entries() -> List[Dict[str, Any]]:
#     if not os.path.exists(TARGET_CONFIG_FILE):
#         save_target_city_entries(DEFAULT_TARGET_CITY_ENTRIES)
#     try:
#         with open(TARGET_CONFIG_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, list):
#             raise ValueError("Invalid target city config format")
#     except Exception:
#         data = DEFAULT_TARGET_CITY_ENTRIES
#     return [normalize_target_entry(item) for item in data]


# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     normalized = [normalize_target_entry(item) for item in entries]
#     with open(TARGET_CONFIG_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)


# TARGET_CITIES_ENTRIES = load_target_city_entries()


# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     return TARGET_CITIES_ENTRIES


# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
#     save_target_city_entries(st.session_state["target_cities_entries"])


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# # --- [ì‹ ê·œ 1] aggregate_ai_totals í•¨ìˆ˜ ìˆ˜ì • ---
# # (ì´ìƒì¹˜ ì œê±° + ë³€ë™ê³„ìˆ˜(VC) ê³„ì‚°)
# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê·  ë° ë³€ë™ ê³„ìˆ˜(VC)ë¥¼ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean_raw": None, "mean": None, "variation_coeff": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
    
#     # --- [ì‹ ê·œ 1] AI ì¼ê´€ì„± ì ìˆ˜ (ë³€ë™ ê³„ìˆ˜) ê³„ì‚° ---
#     variation_coeff = None
#     if filtered and computed_mean and computed_mean > 0:
#         if len(filtered) > 1:
#             try:
#                 computed_stdev = stdev(filtered)
#                 variation_coeff = computed_stdev / computed_mean # ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨ / í‰ê· 
#             except StatisticsError:
#                 variation_coeff = 0.0 # ëª¨ë“  ê°’ì´ ë™ì¼
#         else:
#             variation_coeff = 0.0 # ê°’ì´ í•˜ë‚˜ë¿ì´ë©´ ë³€ë™ ì—†ìŒ

#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#         "variation_coeff": variation_coeff # <-- AI ì¼ê´€ì„± ì ìˆ˜
#     }

# # --- [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
# def get_dynamic_weights(
#     variation_coeff: Optional[float], 
#     admin_weights: Dict[str, float]
# ) -> Dict[str, Any]:
#     """AI ì¼ê´€ì„±(VC)ì— ë”°ë¼ ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤."""
    
#     # ê´€ë¦¬ì ì„¤ì •ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
#     base_ai_weight = admin_weights.get("ai_weight", 0.5)
    
#     if variation_coeff is None:
#         # AI ë°ì´í„°ê°€ ì—†ìœ¼ë©´ UN 100%
#         return {"un_weight": 1.0, "ai_weight": 0.0, "source": "No AI Data"}
        
#     if variation_coeff <= 0.05: # 5% ì´í•˜: ë§¤ìš° ì¼ê´€ë¨
#         # AI ì‹ ë¢°ë„ ìƒí–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœëŒ€ 0.7ê¹Œì§€)
#         dynamic_ai_weight = min(base_ai_weight + 0.2, 0.7)
#         source = f"High AI Consistency (VC: {variation_coeff:.2f})"
#     elif variation_coeff >= 0.15: # 15% ì´ìƒ: ë§¤ìš° ë¶ˆì•ˆì •
#         # AI ì‹ ë¢°ë„ í•˜í–¥ (ê´€ë¦¬ì ì„¤ì •ì¹˜ì—ì„œ ìµœì†Œ 0.3ê¹Œì§€)
#         dynamic_ai_weight = max(base_ai_weight - 0.2, 0.3)
#         source = f"Low AI Consistency (VC: {variation_coeff:.2f})"
#     else:
#         # 5% ~ 15% ì‚¬ì´: ê´€ë¦¬ì ì„¤ì •ê°’ ìœ ì§€
#         dynamic_ai_weight = base_ai_weight
#         source = f"Standard (Admin Default) (VC: {variation_coeff:.2f})"

#     final_ai_weight = max(0.0, min(1.0, dynamic_ai_weight))
#     final_un_weight = 1.0 - final_ai_weight
    
#     return {"un_weight": final_un_weight, "ai_weight": final_ai_weight, "source": source}


# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# def get_history_files():
#     if not os.path.exists(DATA_DIR):
#         return []
#     files = [f for f in os.listdir(DATA_DIR) if f.startswith("report_") and f.endswith(".json")]
#     return sorted(files, reverse=True)

# def save_report_data(data):
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = os.path.join(DATA_DIR, f"report_{timestamp}.json")
#     with open(filename, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data


# def load_report_data(filename):
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             try:
#                 data = json.load(f)
#                 return _sanitize_report_data(data)
#             except json.JSONDecodeError: return None
#     return None

# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None
#         search_target = target
#         is_substitute = "un_dsa_substitute" in target
#         if is_substitute: search_target = target["un_dsa_substitute"]
        
#         country_df = df[df['Country'].str.contains(search_target['country'], case=False, na=False)]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# # --- [ê°œì„  1] AI í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°(async) ë²„ì „ìœ¼ë¡œ êµì²´ ---
# async def get_market_data_from_ai_async(
#     client: openai.AsyncOpenAI,  # <-- Async í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°›ìŒ
#     city: str,
#     country: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """[ë¹„ë™ê¸° ë²„ì „] AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     # --- (ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")
#     # --- (í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---
#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         # --- [ìˆ˜ì •] ë¹„ë™ê¸° API í˜¸ì¶œë¡œ ë³€ê²½ ---
#         response = await client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         # --- [ìˆ˜ì •] ë ---
        
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }
# # --- [ê°œì„  1] ë ---

# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
            
#             # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#             weight_source = ai_summary.get("weighted_average_components", {}).get("weights", {}).get("source", "N/A")

#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#                 'Weight Logic': weight_source, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìœ  ì¶”ê°€
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - [ì‹ ê·œ 1] **Dynamic Weighting**: AI-generated data consistency (Variation Coefficient) is measured. If AI results are highly consistent (VC <= 5%), AI weight is increased. If highly inconsistent (VC >= 15%), AI weight is decreased. Otherwise, admin-set defaults are used.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- ìŠ¤íŠ¸ë¦¼ë¦¿ UI êµ¬ì„± ---
# st.set_page_config(layout="wide")
# st.title("AICP: ì¶œì¥ ì¼ë¹„ ê³„ì‚° & ì¡°íšŒ ì‹œìŠ¤í…œ (v16.0 - Async & Dynamic)")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility


# # --- [ê°œì„  3] íƒ­ êµ¬ì¡° ë³€ê²½ ---
# tab_definitions = []
# if employee_tab_visible:
#     tab_definitions.append("ğŸ’µ ì¼ë¹„ ì¡°íšŒ (ì§ì›ìš©)")

# # ê´€ë¦¬ì íƒ­ì„ 2ê°œë¡œ ë¶„ë¦¬
# tab_definitions.append("ğŸ“ˆ ë³´ê³ ì„œ ë¶„ì„ (Admin)")
# tab_definitions.append("ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Admin)")

# tabs = st.tabs(tab_definitions)

# employee_tab = None
# admin_analysis_tab = None
# admin_config_tab = None

# if employee_tab_visible:
#     employee_tab = tabs[0]
#     admin_analysis_tab = tabs[1]
#     admin_config_tab = tabs[2]
# else:
#     admin_analysis_tab = tabs[0]
#     admin_config_tab = tabs[1]
# # --- [ê°œì„  3] ë ---


# if employee_tab is not None:
#     with employee_tab:
#         st.header("ë„ì‹œë³„ ì¶œì¥ ì¼ë¹„ ì¡°íšŒ")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ PDFë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("êµ­ê°€:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("ë„ì‹œ:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("ì„ íƒí•œ êµ­ê°€ì— ë“±ë¡ëœ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "ì¶œì¥ ì‹œì‘ì¼",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "ì¶œì¥ ì¢…ë£Œì¼",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("ì§ê¸‰:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
#                     trip_days = 0 # 0ìœ¼ë¡œ ì„¤ì •
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"ìë™ ë¶„ë¥˜ëœ ì¶œì¥ ìœ í˜•: {trip_term_label} Â· {trip_days}ì¼ ì¼ì •")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("ì´ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë„ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶œì¥ ìœ í˜•ì„ 'ìˆí…€'ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ë„ì‹œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level and trip_days is not None:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} ê²°ê³¼")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
                        
#                         # [ì‹ ê·œ 2] ì§ì› íƒ­ ì´ì•¡ ì¹´ë“œ
#                         render_primary_summary(
#                             f"{sel_level.split(' ')[0]}",
#                             trip_total_allowance,
#                             level_daily_allowance,
#                             trip_days,
#                             trip_term_label,
#                             trip_multiplier
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} ì¼ì¼ ê¶Œì¥ ì¼ë¹„", "ê¸ˆì•¡ ì—†ìŒ")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±° (ì¼ë¹„ ê¸°ì¤€)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"ì„±ê³µ {ai_runs}/{ai_attempts}íšŒ"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"ì œì™¸ê°’ {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"ì‹œì¦Œ {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "AI ë°ì´í„° ì—†ìŒ"
                        
#                         # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì‚¬ìœ 
#                         weights_info = ai_summary.get("weighted_average_components", {}).get("weights", {})
#                         weights_source = weights_info.get("source", "N/A")
#                         un_weight_pct = f"{weights_info.get('un_weight', 0.5):.0%}"
#                         ai_weight_pct = f"{weights_info.get('ai_weight', 0.5):.0%}"
#                         weight_caption = f"Blend: UN-DSA ({un_weight_pct}) + AI ({ai_weight_pct}) | ì‚¬ìœ : {weights_source}"

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         un_caption = f"ìˆí…€ ê¸°ì¤€ $ {un_base:,}" if un_base is not None else city_data.get("notes", "")
#                                         if trip_term == "Long-term" and un_base is not None:
#                                             un_caption = f"ìˆí…€ $ {un_base:,} â†’ ë¡±í…€ $ {un_display:,}"
#                                         render_stat_card("UN-DSA ê¸°ì¤€", f"$ {un_display:,}" if un_display is not None else "N/A", un_caption, "secondary")
                                    
#                                     elif key == "ai":
#                                         ai_caption_base = f"ìˆí…€ ê¸°ì¤€ $ {ai_avg:,}" if ai_avg is not None else ""
#                                         if trip_term == "Long-term" and ai_avg is not None:
#                                             ai_caption_base = f"ìˆí…€ $ {ai_avg:,} â†’ ë¡±í…€ $ {ai_display:,}"
#                                         ai_full_caption = f"{ai_notes} | {ai_caption_base}".strip(" | ")
#                                         render_stat_card("AI ì‹œì¥ ì¶”ì • (ì‹œì¦Œ ë³´ì •)", f"$ {ai_display:,}" if ai_display is not None else "N/A", ai_full_caption, "secondary")
                                    
#                                     else: # key == "weighted"
#                                         weighted_caption = weight_caption
#                                         if trip_term == "Long-term" and final_allowance is not None:
#                                             weighted_caption = f"ìˆí…€ $ {final_allowance:,} â†’ ë¡±í…€ $ {weighted_display:,} | {weight_caption}"
#                                         render_stat_card("ê°€ì¤‘ í‰ê·  ê²°ê³¼", f"$ {weighted_display:,}" if weighted_display is not None else "N/A", weighted_caption, "secondary")

#                         # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ (show_ai_market_detailê³¼ ë¡œì§ í†µí•©)
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             st.markdown("<br>", unsafe_allow_html=True) # ì¤„ ê°„ê²©
                            
#                             mean_food = ai_summary.get("mean_food", 0)
#                             mean_trans = ai_summary.get("mean_transport", 0)
#                             mean_misc = ai_summary.get("mean_misc", 0)
                            
#                             # ë¡±í…€/ì‹œì¦Œ ìš”ìœ¨ ì ìš©
#                             food_display = round(mean_food * season_factor * trip_multiplier)
#                             trans_display = round(mean_trans * season_factor * trip_multiplier)
#                             misc_display = round(mean_misc * season_factor * trip_multiplier)
                            
#                             st.write("###### AI ì¶”ì • ìƒì„¸ ë‚´ì—­ (ì¼ë¹„ ê¸°ì¤€)")
#                             col_f, col_t, col_m = st.columns(3)
#                             with col_f:
#                                 render_stat_card("ì˜ˆìƒ ì‹ë¹„ (Food)", f"$ {food_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_food)}", "muted")
#                             with col_t:
#                                 render_stat_card("ì˜ˆìƒ êµí†µë¹„ (Transport)", f"$ {trans_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_trans)}", "muted")
#                             with col_m:
#                                 render_stat_card("ì˜ˆìƒ ê¸°íƒ€ (Misc)", f"$ {misc_display:,}", f"ìˆí…€ ê¸°ì¤€: $ {round(mean_misc)}", "muted")
                        
#                         # [ê°œì„  3] show_weighted_result ì¹´ë“œê°€ ì¤‘ë³µë˜ë¯€ë¡œ, ì•„ë˜ ë¸”ë¡ì€ ì œê±°
#                         # (ê¸°ì¡´ second_row_keys ë¡œì§ ì œê±°)

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_info,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("ê´€ë¦¬ìê°€ ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±°ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")

# # --- [ê°œì„  2] admin_tab -> admin_analysis_tab ìœ¼ë¡œ ë³€ê²½ ---
# with admin_analysis_tab:
    
#     # [ê°œì„  2] ADMIN_ACCESS_CODE ë¡œë“œ ë° .env ì²´í¬
#     ACCESS_CODE_KEY = "admin_access_code_valid"
#     ACCESS_CODE_VALUE = os.getenv("ADMIN_ACCESS_CODE") # .envì—ì„œ ë¡œë“œ

#     if not ACCESS_CODE_VALUE:
#         st.error("ë³´ì•ˆ ì˜¤ë¥˜: .env íŒŒì¼ì— 'ADMIN_ACCESS_CODE'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¤‘ì§€í•˜ê³  .env íŒŒì¼ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
#         st.stop()
    
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#                 st.rerun() # [ê°œì„  3] ì„±ê³µ ì‹œ ìƒˆë¡œê³ ì¹¨
#             else:
#                 st.error("Access Codeê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#                 st.stop() # [ê°œì„  3] ì‹¤íŒ¨ ì‹œ ì¤‘ì§€
#         else:
#             st.stop() # [ê°œì„  3] í¼ ì œì¶œ ì „ ì¤‘ì§€

#     # --- [ê°œì„  3] "ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.subheader("ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("í™œì„± ë³´ê³ ì„œ ë²„ì „ì„ ì„ íƒí•˜ì„¸ìš”:", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("ìƒì„±ëœ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 4] ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("ê³¼ê±° ë³´ê³ ì„œ ë¹„êµ")
#     if len(history_files) < 2:
#         st.info("ë¹„êµí•  ë³´ê³ ì„œê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
#     else:
#         col_a, col_b = st.columns(2)
#         with col_a:
#             file_a = st.selectbox("ê¸°ì¤€ ë³´ê³ ì„œ (A)", history_files, index=1, key="compare_a")
#         with col_b:
#             file_b = st.selectbox("ë¹„êµ ë³´ê³ ì„œ (B)", history_files, index=0, key="compare_b")
        
#         if st.button("ë³´ê³ ì„œ ë¹„êµí•˜ê¸°"):
#             if file_a == file_b:
#                 st.warning("ì„œë¡œ ë‹¤ë¥¸ ë³´ê³ ì„œë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
#             else:
#                 with st.spinner("ë³´ê³ ì„œ ë¹„êµ ì¤‘..."):
#                     data_a = load_report_data(file_a)
#                     data_b = load_report_data(file_b)
                    
#                     if data_a and data_b and 'cities' in data_a and 'cities' in data_b:
#                         df_a = pd.DataFrame(data_a['cities'])[['city', 'country_display', 'final_allowance']]
#                         df_b = pd.DataFrame(data_b['cities'])[['city', 'country_display', 'final_allowance']]
                        
#                         df_merged = pd.merge(df_a, df_b, on=["city", "country_display"], suffixes=("_A", "_B"))
                        
#                         report_a_label = file_a.split('report_')[-1].split('.')[0]
#                         report_b_label = file_b.split('report_')[-1].split('.')[0]

#                         df_merged[f"A ({report_a_label})"] = df_merged["final_allowance_A"]
#                         df_merged[f"B ({report_b_label})"] = df_merged["final_allowance_B"]
                        
#                         df_merged["ë³€ë™ì•¡ ($)"] = df_merged["final_allowance_B"] - df_merged["final_allowance_A"]
                        
#                         # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
#                         df_merged["ë³€ë™ë¥  (%)"] = (df_merged["ë³€ë™ì•¡ ($)"] / df_merged["final_allowance_A"].replace(0, pd.NA)) * 100
                        
#                         st.dataframe(df_merged[[
#                             "city", "country_display", 
#                             f"A ({report_a_label})", 
#                             f"B ({report_b_label})", 
#                             "ë³€ë™ì•¡ ($)", "ë³€ë™ë¥  (%)"
#                         ]].style.format({"ë³€ë™ë¥  (%)": "{:,.1f}%", "ë³€ë™ì•¡ ($)": "{:,.0f}"}), width="stretch")
#                     else:
#                         st.error("ë³´ê³ ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ê°œì„  3] "UN-DSA (PDF) ë¶„ì„" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     st.divider()
#     st.subheader("UN-DSA (PDF) ë¶„ì„ ë° AI ì‹¤í–‰")
#     st.warning(f"AI í˜¸ì¶œì´ {NUM_AI_CALLS}íšŒ ì‹¤í–‰ë˜ë¯€ë¡œ ì‹œê°„ê³¼ ë¹„ìš©ì— ìœ ì˜í•´ ì£¼ì„¸ìš”. (ê°œì„  1: ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)")
#     uploaded_file = st.file_uploader("UN-DSA PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

#     # --- [ê°œì„  1] ë¹„ë™ê¸° AI ë¶„ì„ ì‹¤í–‰ ë¡œì§ ---
#     if uploaded_file and st.button("AI ë¶„ì„ ì‹¤í–‰", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
#         else:
#             st.session_state.latest_analysis_result = None
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ ---
#             async def run_analysis(progress_bar, openai_api_key):
#                 progress_bar.progress(0, text="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
#                 full_text = parse_pdf_to_text(uploaded_file)
                
#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []
#                 analysis_failed = False
                
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV ë³€í™˜ ì¤‘... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break
                
#                 if analysis_failed:
#                     st.error("PDF->TSV ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                 if not processed_data:
#                     st.error("TSV ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#                     progress_bar.empty()
#                     return

#                 # ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
#                 client = openai.AsyncOpenAI(api_key=openai_api_key)
                
#                 total_cities = len(processed_data["cities"])
#                 all_tasks = [] # ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

#                 # 1. ëª¨ë“  ë„ì‹œì— ëŒ€í•œ ëª¨ë“  AI í˜¸ì¶œ ì‘ì—…ì„ ë¯¸ë¦¬ ìƒì„±
#                 for city_data in processed_data["cities"]:
#                     city_name, country_name = city_data["city"], city_data["country_display"]
#                     city_context = {
#                         "neighborhood": city_data.get("neighborhood"),
#                         "hotel_cluster": city_data.get("hotel_cluster"),
#                     }
#                     season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                     menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
                    
#                     city_data["menu_samples"] = menu_samples
#                     city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
                    
#                     city_tasks = []
#                     for j in range(1, NUM_AI_CALLS + 1):
#                         task = get_market_data_from_ai_async(
#                             client, city_name, country_name, f"Run {j}",
#                             context=city_context, season_context=season_context, menu_samples=menu_samples
#                         )
#                         city_tasks.append(task)
                    
#                     all_tasks.append(city_tasks) # [ [ë„ì‹œ1-10íšŒ], [ë„ì‹œ2-10íšŒ], ... ]

#                 # 2. ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
#                 city_index = 0
#                 for city_tasks in all_tasks:
#                     city_data = processed_data["cities"][city_index]
#                     city_name = city_data["city"]
#                     progress_text = f"AI ì¶”ì •ì¹˜ ê³„ì‚° ì¤‘... ({city_index+1}/{total_cities}) {city_name}"
#                     progress_bar.progress((city_index + 1) / max(total_cities, 1), text=progress_text)
                    
#                     # í•´ë‹¹ ë„ì‹œì˜ 10ê°œ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
#                     try:
#                         market_results = await asyncio.gather(*city_tasks)
#                     except Exception as e:
#                         st.error(f"{city_name} ë¶„ì„ ì¤‘ ë¹„ë™ê¸° ì˜¤ë¥˜: {e}")
#                         market_results = [] # ì‹¤íŒ¨ ì²˜ë¦¬

#                     # 3. ê²°ê³¼ ì²˜ë¦¬
#                     ai_totals_source: List[int] = []
#                     ai_meta_runs: List[Dict[str, Any]] = []
                    
#                     # [ì‹ ê·œ 2] ë¹„ìš© í•­ëª©ë³„ ìƒì„¸ ë‚´ì—­ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
#                     ai_food: List[int] = []
#                     ai_transport: List[int] = []
#                     ai_misc: List[int] = []

#                     for j, market_result in enumerate(market_results, 1):
#                         city_data[f"market_data_{j}"] = market_result
#                         if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                             ai_totals_source.append(market_result["total"])
#                             # [ì‹ ê·œ 2] ìƒì„¸ ë¹„ìš© ì¶”ê°€
#                             ai_food.append(market_result.get("food", 0))
#                             ai_transport.append(market_result.get("transport", 0))
#                             ai_misc.append(market_result.get("misc", 0))
                        
#                         if "meta" in market_result:
#                             ai_meta_runs.append(market_result["meta"])
                    
#                     city_data["ai_provenance"] = ai_meta_runs

#                     # 4. ìµœì¢… ìˆ˜ë‹¹ ê³„ì‚°
#                     final_allowance = None
#                     un_per_diem_raw = city_data.get("un", {}).get("per_diem_excl_lodging")
#                     un_per_diem = float(un_per_diem_raw) if isinstance(un_per_diem_raw, (int, float)) else None

#                     ai_stats = aggregate_ai_totals(ai_totals_source)
#                     season_factor = (season_context or {}).get("factor", 1.0)
#                     ai_base_mean = ai_stats.get("mean_raw")
#                     ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
                    
#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
#                     admin_weights = get_weight_config() # ê´€ë¦¬ì ì„¤ì • ë¡œë“œ
#                     ai_vc_score = ai_stats.get("variation_coeff")
                    
#                     if un_per_diem is not None:
#                         weights_cfg = get_dynamic_weights(ai_vc_score, admin_weights)
#                     else:
#                         # UN ë°ì´í„° ì—†ìœ¼ë©´ AI 100%
#                         weights_cfg = {"un_weight": 0.0, "ai_weight": 1.0, "source": "AI Only (UN-DSA Missing)"}
                    
#                     city_data["ai_summary"] = {
#                         "raw_totals": ai_totals_source,
#                         "used_totals": ai_stats.get("used_values", []),
#                         "removed_totals": ai_stats.get("removed_values", []),
#                         "mean_base": ai_base_mean,
#                         "mean_base_rounded": ai_stats.get("mean"),
                        
#                         "ai_consistency_vc": ai_vc_score, # [ì‹ ê·œ 1]
                        
#                         "mean_food": mean(ai_food) if ai_food else 0, # [ì‹ ê·œ 2]
#                         "mean_transport": mean(ai_transport) if ai_transport else 0, # [ì‹ ê·œ 2]
#                         "mean_misc": mean(ai_misc) if ai_misc else 0, # [ì‹ ê·œ 2]

#                         "season_factor": season_factor,
#                         "season_label": (season_context or {}).get("label"),
#                         "season_adjusted_mean_raw": ai_season_adjusted,
#                         "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                         "successful_runs": len(ai_stats.get("used_values", [])),
#                         "attempted_runs": NUM_AI_CALLS,
#                         "reference_links": city_data.get("reference_links", []),
#                         "weighted_average_components": {
#                             "un_per_diem": un_per_diem,
#                             "ai_season_adjusted": ai_season_adjusted,
#                             "weights": weights_cfg, # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ ì €ì¥
#                         },
#                     }

#                     # [ì‹ ê·œ 1] ë™ì  ê°€ì¤‘ì¹˜ë¡œ ìµœì¢…ê°’ ê³„ì‚°
#                     if un_per_diem is not None and ai_season_adjusted is not None:
#                         weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                         final_allowance = round(weighted_average)
#                     elif un_per_diem is not None:
#                         final_allowance = round(un_per_diem)
#                     elif ai_season_adjusted is not None:
#                         final_allowance = round(ai_season_adjusted)

#                     city_data["final_allowance"] = final_allowance

#                     if final_allowance and un_per_diem and un_per_diem > 0:
#                         city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                     else:
#                         city_data["delta_vs_un_pct"] = "N/A"
                    
#                     city_index += 1 # ë‹¤ìŒ ë„ì‹œë¡œ

#                 save_report_data(processed_data)
#                 st.session_state.latest_analysis_result = processed_data
#                 st.success("AI analysis completed.")
#                 progress_bar.empty()
#                 st.rerun()
            
#             # --- ë¹„ë™ê¸° ì‹¤í–‰ ---
#             with st.spinner("PDF ì²˜ë¦¬ ë° AI ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì•½ 10~30ì´ˆ ì†Œìš”)"):
#                 progress_bar = st.progress(0, text="ë¶„ì„ ì‹œì‘...")
#                 asyncio.run(run_analysis(progress_bar, openai_api_key))

#     # --- [ê°œì„  3] "Latest Analysis Summary" ê¸°ëŠ¥ (analysis_sub_tab) ---
#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             # --- [HOTFIX] ArrowInvalid Error ë°©ì§€ ---
#             delta_val = city.get('delta_vs_un_pct')
#             if isinstance(delta_val, (int, float)):
#                 delta_display = f"{delta_val:.0f}%" # ìˆ«ìë¥¼ "12%" í˜•íƒœì˜ ë¬¸ìì—´ë¡œ ë³€ê²½
#             else:
#                 delta_display = "N/A" # ì´ë¯¸ "N/A" ë¬¸ìì—´
#             # --- [HOTFIX] End ---
                
#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': delta_display, # <-- ìˆ˜ì •ëœ ë¬¸ìì—´ ê°’ ì‚¬ìš©
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data), use_container_width=True) # <-- use_container_width ì¶”ê°€ (í•„ìš”ì‹œ width='stretch'ë¡œ ë³€ê²½)
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))

# # --- [ê°œì„  3] "ì‹œìŠ¤í…œ ì„¤ì •" íƒ­ (admin_config_tab) ---
# with admin_config_tab:
#     # ì•”í˜¸ í™•ì¸ (í•„ìˆ˜)
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         st.error("Access Codeê°€ í•„ìš”í•©ë‹ˆë‹¤.")
#         st.stop()
        
#     st.subheader("ì§ì›ìš© íƒ­ ë…¸ì¶œ")
#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility_toggle") # Key ì´ë¦„ ë³€ê²½
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.session_state.employee_tab_visibility = visibility_toggle # ì„¸ì…˜ ìƒíƒœì—ë„ ë°˜ì˜
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œê³ ì¹¨ ì‹œ ì ìš©)")

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS.get(section_key, True))
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

#     st.divider()
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     st.info("ì´ì œ ì´ ì„¤ì •ì€ 'ë™ì  ê°€ì¤‘ì¹˜' ë¡œì§ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. AI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ë©´ ìë™ìœ¼ë¡œ AI ë¹„ì¤‘ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
#     current_weights = get_weight_config()
#     st.caption(f"Current Admin Default -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")
#         st.rerun()

#     st.divider()
#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         # trip_lengthsë¥¼ ë³´ê¸° ì‰½ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
#         entries_display["trip_lengths"] = entries_display["trip_lengths"].apply(lambda x: ', '.join(x) if isinstance(x, list) else DEFAULT_TRIP_LENGTH[0])
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], use_container_width=True)
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()

#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
#     current_entries = get_target_city_entries()
    
#     if current_entries:
#         # 1. ë“œë¡­ë‹¤ìš´ ì˜µì…˜ êµ¬ì„±
#         options = {
#             f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#             for idx, entry in enumerate(current_entries)
#         }
#         sorted_labels = list(options.keys())

#         # 2. on_change ì½œë°± í•¨ìˆ˜ ì •ì˜ (ìœ„ì ¯ë³´ë‹¤ ë¨¼ì €)
#         def _sync_edit_form_from_selection():
#             # ë“œë¡­ë‹¤ìš´ì—ì„œ í˜„ì¬ ì„ íƒëœ ê°’ì„ ê°€ì ¸ì˜´
#             if "edit_city_selector" not in st.session_state:
#                  st.session_state.edit_city_selector = sorted_labels[0]
                 
#             selected_idx = options[st.session_state.edit_city_selector]
#             selected_entry = current_entries[selected_idx]
            
#             # session_stateì˜ ê°’ì„ ì„ íƒëœ ë„ì‹œì˜ ë°ì´í„°ë¡œ ê°•ì œ ì—…ë°ì´íŠ¸
#             st.session_state.edit_region = selected_entry.get("region", "")
#             st.session_state.edit_city = selected_entry.get("city", "")
#             st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#             st.session_state.edit_country = selected_entry.get("country", "")
#             st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
            
#             # ì¶œì¥ ê¸°ê°„ (trip_lengths) ì„¤ì •
#             existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#             st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
            
#             # UN-DSA ëŒ€ì²´ ë„ì‹œ ì„¤ì •
#             sub_data = selected_entry.get("un_dsa_substitute") or {}
#             st.session_state.edit_sub_city = sub_data.get("city", "")
#             st.session_state.edit_sub_country = sub_data.get("country", "")

#         # 3. ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection  # <-- [ìˆ˜ì •] ì½œë°± í•¨ìˆ˜ ì—°ê²°
#         )

#         # 4. í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state:
#             # ì²« ë¡œë“œ ì‹œ, selectboxì˜ ê¸°ë³¸ê°’(ì²« ë²ˆì§¸ í•­ëª©)ì— ë§ì¶° í¼ì„ ì±„ì›€
#             _sync_edit_form_from_selection()

#         # 5. í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             # [ìˆ˜ì •] default=... ëŒ€ì‹  key=... ì‚¬ìš©
#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", # 'default' ëŒ€ì‹  'key'ë¡œ ìƒíƒœ ê´€ë¦¬
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # 6. ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             # [ìˆ˜ì •] ìœ„ì ¯ ë³€ìˆ˜(region_edit) ëŒ€ì‹  st.session_stateì—ì„œ ì§ì ‘ ê°’ì„ ì½ìŒ
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 current_entries[options[selected_label]] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[options[selected_label]]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[options[selected_label]].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()  # <-- [í™•ì¸] ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë˜ì–´ ìˆìŒ
        
#         if delete_btn:
#             del current_entries[options[selected_label]]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun() # <-- [í™•ì¸] ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë˜ì–´ ìˆìŒ

#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     # --- [ì‹ ê·œ 3] 'ë°ì´í„° ìºì‹œ ê´€ë¦¬' UI ì¶”ê°€ ---
#     st.divider()
#     st.header("ë°ì´í„° ìºì‹œ ê´€ë¦¬ (Menu Cache)")

#     if not MENU_CACHE_ENABLED:
#         st.error("`data_sources/menu_cache.py` íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         st.info("AIê°€ ë„ì‹œ ë¬¼ê°€ ì¶”ì • ì‹œ ì°¸ê³ í•  ì‹¤ì œ ë©”ë‰´/ê°€ê²© ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (AI ë¶„ì„ ì •í™•ë„ í–¥ìƒ)")

#         # 1. ìƒˆ ìºì‹œ í•­ëª© ì¶”ê°€ í¼
#         st.subheader("ì‹ ê·œ ìºì‹œ í•­ëª© ì¶”ê°€")
#         with st.form("add_menu_cache_form", clear_on_submit=True):
#             st.write("AI ë¶„ì„ì— ì‚¬ìš©í•  ì°¸ê³  ê°€ê²© ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: ë ˆìŠ¤í† ë‘ ë©”ë‰´, íƒì‹œë¹„ ê³ ì§€ ë“±)")
#             c1, c2 = st.columns(2)
#             with c1:
#                 new_cache_country = st.text_input("êµ­ê°€ (Country)", help="ì˜ˆ: Philippines")
#                 new_cache_city = st.text_input("ë„ì‹œ (City)", help="ì˜ˆ: Manila")
#                 new_cache_neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (Neighborhood) (ì„ íƒ)", help="ì˜ˆ: Makati (ë¹„ì›Œë‘ë©´ ë„ì‹œ ì „ì²´ì— ì ìš©)")
#                 new_cache_vendor = st.text_input("ì¥ì†Œ/ìƒí’ˆëª… (Vendor)", help="ì˜ˆ: Jollibee (C3, Ayala Ave)")
#             with c2:
#                 new_cache_category = st.selectbox("ì¹´í…Œê³ ë¦¬ (Category)", ["Food", "Transport", "Misc"])
#                 new_cache_price = st.number_input("ê°€ê²© (Price)", min_value=0.0, step=0.01)
#                 new_cache_currency = st.text_input("í†µí™” (Currency)", value="USD", help="ì˜ˆ: PHP, USD")
#                 new_cache_url = st.text_input("ì¶œì²˜ URL (Source URL) (ì„ íƒ)")
            
#             add_cache_submitted = st.form_submit_button("ì‹ ê·œ ìºì‹œ í•­ëª© ì €ì¥")

#             if add_cache_submitted:
#                 if not new_cache_country or not new_cache_city or not new_cache_vendor:
#                     st.error("êµ­ê°€, ë„ì‹œ, ì¥ì†Œ/ìƒí’ˆëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
#                 else:
#                     new_entry = {
#                         "country": new_cache_country.strip(),
#                         "city": new_cache_city.strip(),
#                         "neighborhood": new_cache_neighborhood.strip(),
#                         "vendor": new_cache_vendor.strip(),
#                         "category": new_cache_category,
#                         "price": new_cache_price,
#                         "currency": new_cache_currency.strip().upper(),
#                         "url": new_cache_url.strip(),
#                     }
#                     # menu_cache.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•­ëª© ì¶”ê°€
#                     if add_menu_cache_entry(new_entry):
#                         st.success(f"'{new_cache_vendor}' í•­ëª©ì„ ìºì‹œì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                         st.rerun()
#                     else:
#                         st.error("ìºì‹œ í•­ëª© ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

#         # 2. ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ
#         st.subheader("ê¸°ì¡´ ìºì‹œ í•­ëª© ì¡°íšŒ ë° ì‚­ì œ")
#         all_cache_data = load_all_cache() # menu_cache.pyì˜ í•¨ìˆ˜
        
#         if not all_cache_data:
#             st.info("í˜„ì¬ ì €ì¥ëœ ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         else:
#             df_cache = pd.DataFrame(all_cache_data)
#             st.dataframe(df_cache[[
#                 "country", "city", "neighborhood", "vendor", 
#                 "category", "price", "currency", "last_updated", "url"
#             ]], use_container_width=True)

#             # ì‚­ì œ ê¸°ëŠ¥
#             st.markdown("---")
#             st.write("##### ìºì‹œ í•­ëª© ì‚­ì œ")
            
#             # ì‚­ì œí•  í•­ëª©ì„ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ í•œ ë ˆì´ë¸” ìƒì„± (ìµœì‹  í•­ëª©ì´ ìœ„ë¡œ)
#             delete_options_map = {
#                 f"[{entry.get('last_updated', '...')} / {entry.get('city', '...')}] {entry.get('vendor', '...')} ({entry.get('price', '...')})": idx
#                 for idx, entry in enumerate(reversed(all_cache_data)) # reversed()ë¡œ ìµœì‹  í•­ëª©ì´ ë¨¼ì € ë³´ì´ê²Œ
#             }
#             delete_labels = list(delete_options_map.keys())
            
#             label_to_delete = st.selectbox("ì‚­ì œí•  ìºì‹œ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", delete_labels, index=None, placeholder="ì‚­ì œí•  í•­ëª© ì„ íƒ...")
            
#             if label_to_delete and st.button(f"'{label_to_delete}' í•­ëª© ì‚­ì œ", type="primary"):
#                 # ê±°ê¾¸ë¡œ ë§¤í•‘ëœ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
#                 original_list_index = (len(all_cache_data) - 1) - delete_options_map[label_to_delete]
                
#                 entry_to_delete = all_cache_data.pop(original_list_index)
                
#                 # menu_cache.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²´ íŒŒì¼ ì €ì¥
#                 if save_cached_menu_prices(all_cache_data):
#                     st.success(f"'{entry_to_delete.get('vendor')}' í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#                     st.rerun()
#                 else:
#                     st.error("ìºì‹œ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
#     # --- [ì‹ ê·œ 3] UI ë ---

#     st.divider() # <-- ì´ê²ƒì´ 'ë¹„ì¤‘ ì„¤ì •' ì„¹ì…˜ê³¼ êµ¬ë¶„í•˜ëŠ” ì„ ì…ë‹ˆë‹¤.
#     st.subheader("ë¹„ì¤‘ ì„¤ì • (ê¸°ë³¸ê°’)")
#     # ... (ì´í›„ ë¹„ì¤‘ ì„¤ì • í¼ ì½”ë“œê°€ ì´ì–´ì§) ...


# # 2025-10-20-14 AI ê¸°ë°˜ ì¶œì¥ë¹„ ê³„ì‚° ë„êµ¬ (ê³ ê¸‰ ë¶„ì„ ëª¨ë¸ ì ìš©)
# # --- ì„¤ì¹˜ ì•ˆë‚´ ---
# # 1. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
# #    pip install streamlit pandas PyMuPDF tabulate openai python-dotenv
# #
# # 2. .env íŒŒì¼ì— OPENAI_API_KEY ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.

# import streamlit as st
# import pandas as pd
# import json
# import os
# import re
# import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
# import openai
# from dotenv import load_dotenv
# import io
# from datetime import datetime, timedelta
# import time
# import random
# from collections import Counter
# from statistics import StatisticsError, mean, quantiles
# from typing import Any, Dict, List, Optional, Set, Tuple

# from data_sources.menu_cache import load_cached_menu_prices

# # --- ì´ˆê¸° í™˜ê²½ ì„¤ì • ---

# # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # Maximum number of AI calls per analysis
# NUM_AI_CALLS = 10
# # --- Weight configuration (sum should remain 1.0) ---
# DEFAULT_WEIGHT_CONFIG = {"un_weight": 0.5, "ai_weight": 0.5}
# _WEIGHT_CONFIG_CACHE: Dict[str, float] = {}


# def weight_config_path() -> str:
#     return os.path.join(DATA_DIR, "weight_config.json")



# def _normalize_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Ensure weights are floats that sum to 1.0 (defaults fall back to 0.5 / 0.5)."""
#     try:
#         un_raw = float(config.get("un_weight", DEFAULT_WEIGHT_CONFIG["un_weight"]))
#     except (TypeError, ValueError):
#         un_raw = DEFAULT_WEIGHT_CONFIG["un_weight"]
#     try:
#         ai_raw = float(config.get("ai_weight", DEFAULT_WEIGHT_CONFIG["ai_weight"]))
#     except (TypeError, ValueError):
#         ai_raw = DEFAULT_WEIGHT_CONFIG["ai_weight"]

#     total = un_raw + ai_raw
#     if total <= 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)

#     un_norm = max(0.0, min(1.0, un_raw / total))
#     ai_norm = max(0.0, min(1.0, ai_raw / total))

#     total_norm = un_norm + ai_norm
#     if total_norm == 0:
#         return dict(DEFAULT_WEIGHT_CONFIG)
#     return {"un_weight": un_norm / total_norm, "ai_weight": ai_norm / total_norm}


# def save_weight_config(config: Dict[str, Any]) -> Dict[str, float]:
#     """Persist weight configuration to disk and update the in-memory cache."""
#     normalized = _normalize_weight_config(config)
#     with open(weight_config_path(), "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)

#     global _WEIGHT_CONFIG_CACHE
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return normalized


# def load_weight_config(force: bool = False) -> Dict[str, float]:
#     """Load weight configuration from disk (or defaults when missing)."""
#     global _WEIGHT_CONFIG_CACHE
#     if _WEIGHT_CONFIG_CACHE and not force:
#         return dict(_WEIGHT_CONFIG_CACHE)

#     if not os.path.exists(weight_config_path()):
#         normalized = save_weight_config(DEFAULT_WEIGHT_CONFIG)
#         return dict(normalized)

#     try:
#         with open(weight_config_path(), "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("Weight config must be a JSON object")
#     except Exception:
#         data = DEFAULT_WEIGHT_CONFIG

#     normalized = _normalize_weight_config(data)
#     _WEIGHT_CONFIG_CACHE = dict(normalized)
#     return dict(normalized)


# def get_weight_config() -> Dict[str, float]:
#     """Return the active weight configuration, favouring session state if available."""
#     try:
#         session_config = st.session_state.get("weight_config")  # type: ignore[attr-defined]
#     except RuntimeError:
#         session_config = None

#     if session_config:
#         normalized = _normalize_weight_config(session_config)
#         try:
#             st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#         except RuntimeError:
#             pass
#         return normalized

#     config = load_weight_config()
#     try:
#         st.session_state["weight_config"] = config  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return config


# def update_weight_config(un_weight: float, ai_weight: float) -> Dict[str, float]:
#     """Update weights both in session and on disk."""
#     config = {"un_weight": un_weight, "ai_weight": ai_weight}
#     normalized = save_weight_config(config)
#     try:
#         st.session_state["weight_config"] = normalized  # type: ignore[attr-defined]
#     except RuntimeError:
#         pass
#     return normalized


# # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ


# def build_reference_link_lines(menu_samples: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
#     """Return markdown-friendly bullets for cached menu/reference entries."""
#     lines_out: List[str] = []
#     if not menu_samples:
#         return lines_out

#     for sample in menu_samples[:max_items]:
#         if not isinstance(sample, dict):
#             continue

#         name = str(sample.get("vendor") or sample.get("name") or sample.get("title") or sample.get("source") or "Reference")

#         url = None
#         for key in ("url", "link", "source_url", "href"):
#             value = sample.get(key)
#             if isinstance(value, str) and value.lower().startswith(("http://", "https://")):
#                 url = value
#                 break

#         details: List[str] = []
#         price = sample.get("price")
#         if isinstance(price, (int, float)):
#             currency = sample.get("currency") or "USD"
#             details.append(f"{currency} {price}")
#         elif isinstance(price, str) and price.strip():
#             details.append(price.strip())

#         category = sample.get("category")
#         if category:
#             details.append(str(category))

#         last_updated = sample.get("last_updated")
#         if last_updated:
#             details.append(f"updated {last_updated}")

#         detail_text = ", ".join(details)
#         label = f"[{name}]({url})" if url else name

#         if detail_text:
#             lines_out.append(f"{label} - {detail_text}")
#         else:
#             lines_out.append(label)

#     return lines_out


# _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = os.path.join(_SCRIPT_DIR, "analysis_history")
# if not os.path.exists(DATA_DIR):
#     os.makedirs(DATA_DIR)

# UI_SETTINGS_FILE = os.path.join(DATA_DIR, "ui_settings.json")
# DEFAULT_UI_SETTINGS = {"show_employee_tab": True}
# EMPLOYEE_SECTION_DEFAULTS: Dict[str, bool] = {
#     "show_un_basis": True,
#     "show_ai_estimate": True,
#     "show_weighted_result": True,
#     "show_ai_market_detail": True,
#     "show_provenance": True,
#     "show_menu_samples": True,
# }
# EMPLOYEE_SECTION_LABELS = [
#     ("show_un_basis", "UN-DSA ê¸°ì¤€ ì¹´ë“œ"),
#     ("show_ai_estimate", "AI ì‹œì¥ ì¶”ì • ì¹´ë“œ"),
#     ("show_weighted_result", "ê°€ì¤‘ í‰ê·  ê²°ê³¼ ì¹´ë“œ"),
#     ("show_ai_market_detail", "AI Market Estimate ì¹´ë“œ"),
#     ("show_provenance", "AI ì‚°ì¶œ ê·¼ê±°(JSON)"),
#     ("show_menu_samples", "ë ˆí¼ëŸ°ìŠ¤ ë©”ë‰´ í‘œ"),
# ]
# _UI_SETTINGS_CACHE: Dict[str, Any] = {}


# CARD_STYLES = {
#     "primary": {
#         "container": "margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;",
#         "title": "font-size:1rem;opacity:0.85;margin-bottom:0.4rem;",
#         "value": "font-size:2.6rem;font-weight:800;letter-spacing:0.02em;margin-bottom:0.5rem;",
#         "caption": "font-size:1.1rem;opacity:0.95;",
#     },
#     "secondary": {
#         "container": "padding:1.1rem;border-radius:14px;background:rgba(30,60,114,0.08);border:1px solid rgba(30,60,114,0.12);color:#0f1e3d;",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem;",
#         "value": "font-size:1.55rem;font-weight:700;margin-bottom:0.3rem;",
#         "caption": "font-size:0.85rem;opacity:0.75;",
#     },
#     "muted": {
#         "container": "padding:1.1rem;border-radius:14px;background:#f5f7fb;border:1px solid #d8deec;color:#495063;",
#         "title": "font-size:0.95rem;font-weight:600;margin-bottom:0.35rem;",
#         "value": "font-size:1.45rem;font-weight:700;margin-bottom:0.3rem;",
#         "caption": "font-size:0.85rem;opacity:0.7;",
#     },
# }


# def render_stat_card(title: str, value: str, caption: str = "", variant: str = "secondary") -> None:
#     style = CARD_STYLES.get(variant, CARD_STYLES["secondary"])
#     caption_html = f"<div style='{style['caption']}'>{caption}</div>" if caption else ""
#     card_html = f"""
#     <div style="{style['container']}">
#         <div style="{style['title']}">{title}</div>
#         <div style="{style['value']}">{value}</div>
#         {caption_html}
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def render_primary_summary(level_label: str, total: int, daily: int, days: int, term_label: str, multiplier: float) -> None:
#     style = CARD_STYLES["primary"]
#     card_html = f"""
#     <div style="{style['container'].replace('text-align:center;', 'text-align:left;')}">
#         <div style="{style['title']}">{level_label} ê¸°ì¤€ ì˜ˆìƒ ì¼ë¹„ ì´ì•¡</div>
#         <div style="{style['value']}">$ {total:,}</div>
#         <div style="{style['caption']}">
#             <span style='font-size:0.95rem;opacity:0.8;'>ê³„ì‚°ì‹</span><br/>
#             $ {daily:,} Ã— {days}ì¼ ì¼ì • Ã— {term_label} (Ã—{multiplier:.2f})
#         </div>
#     </div>
#     """
#     st.markdown(card_html, unsafe_allow_html=True)


# def _normalize_employee_sections(sections: Any) -> Dict[str, bool]:
#     normalized = dict(EMPLOYEE_SECTION_DEFAULTS)
#     if isinstance(sections, dict):
#         for key in normalized:
#             normalized[key] = bool(sections.get(key, normalized[key]))
#     return normalized

# def _normalize_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Ensure UI settings include expected keys with correct types."""
#     normalized = dict(DEFAULT_UI_SETTINGS)
#     raw_visibility = settings.get("show_employee_tab", DEFAULT_UI_SETTINGS["show_employee_tab"])
#     normalized["show_employee_tab"] = bool(raw_visibility)
#     normalized["employee_sections"] = _normalize_employee_sections(settings.get("employee_sections"))
#     return normalized

# def save_ui_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
#     """Persist UI settings to disk and update cache."""
#     normalized = _normalize_ui_settings(settings)
#     with open(UI_SETTINGS_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)
#     global _UI_SETTINGS_CACHE
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return normalized

# def load_ui_settings(force: bool = False) -> Dict[str, Any]:
#     """Load UI settings, defaulting gracefully when missing or malformed."""
#     global _UI_SETTINGS_CACHE
#     if _UI_SETTINGS_CACHE and not force:
#         return dict(_UI_SETTINGS_CACHE)
#     if not os.path.exists(UI_SETTINGS_FILE):
#         normalized = save_ui_settings(DEFAULT_UI_SETTINGS)
#         return dict(normalized)
#     try:
#         with open(UI_SETTINGS_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, dict):
#             raise ValueError("UI settings must be a JSON object")
#     except Exception:
#         data = dict(DEFAULT_UI_SETTINGS)
#     normalized = _normalize_ui_settings(data)
#     _UI_SETTINGS_CACHE = dict(normalized)
#     return dict(normalized)

# JOB_LEVEL_RATIOS = {
#     "L3": 0.60, "L4": 0.60, "L5": 0.80, "L6)": 1.00,
#     "L7": 1.00, "L8": 1.20, "L9": 1.50, "L10": 1.50,
# }

# TARGET_CONFIG_FILE = os.path.join(DATA_DIR, "target_cities_config.json")
# TRIP_LENGTH_OPTIONS = ["Short-term", "Long-term"]
# DEFAULT_TRIP_LENGTH = ["Short-term"]
# LONG_TERM_THRESHOLD_DAYS = 30
# SHORT_TERM_MULTIPLIER = 1.0
# LONG_TERM_MULTIPLIER = 1.05
# TRIP_TERM_LABELS = {"Short-term": "ìˆí…€", "Long-term": "ë¡±í…€"}


# def classify_trip_duration(days: int) -> Tuple[str, float]:
#     """Return trip term classification and multiplier based on duration in days."""
#     if days >= LONG_TERM_THRESHOLD_DAYS:
#         return "Long-term", LONG_TERM_MULTIPLIER
#     return "Short-term", SHORT_TERM_MULTIPLIER

# DEFAULT_TARGET_CITY_ENTRIES: List[Dict[str, Any]] = [
#     {"region": "North America", "city": "Nassau", "country": "Bahamas"},
#     {"region": "North America", "city": "Los Angeles", "country": "USA", "neighborhood": "Downtown & Convention Center", "hotel_cluster": "JW Marriott / Ritz-Carlton L.A. LIVE"},
#     {"region": "North America", "city": "Las Vegas", "country": "USA", "neighborhood": "The Strip (Paradise)", "hotel_cluster": "MGM Grand & Mandalay Bay"},
#     {"region": "North America", "city": "Seattle", "country": "USA"},
#     {"region": "North America", "city": "Florida", "country": "USA"},
#     {"region": "North America", "city": "San Francisco", "country": "USA", "neighborhood": "SoMa & Financial District", "hotel_cluster": "Hilton Union Square / Marriott Marquis"},
#     {"region": "North America", "city": "Toronto", "country": "Canada"},
#     {"region": "Europe", "city": "Valletta", "country": "Malta"},
#     {"region": "Europe", "city": "London", "country": "United Kingdom", "neighborhood": "City & Canary Wharf", "hotel_cluster": "Hilton Bankside / Novotel Canary Wharf"},
#     {"region": "Europe", "city": "Dublin", "country": "Ireland"},
#     {"region": "Europe", "city": "Lisbon", "country": "Portugal"},
#     {"region": "Europe", "city": "Karlovy Vary", "country": "Czech Republic"},
#     {"region": "Europe", "city": "Amsterdam", "country": "Netherlands"},
#     {"region": "Europe", "city": "San Remo", "country": "Italy"},
#     {"region": "Europe", "city": "Barcelona", "country": "Spain", "neighborhood": "Eixample & Fira Gran Via", "hotel_cluster": "AC Hotel Barcelona / Hyatt Regency Tower"},
#     {"region": "Europe", "city": "Nicosia", "country": "Cyprus"},
#     {"region": "Europe", "city": "Paris", "country": "France"},
#     {"region": "Europe", "city": "Provence", "country": "France"},
#     {"region": "Asia", "city": "Taipei", "country": "Taiwan", "un_dsa_substitute": {"city": "Kuala Lumpur", "country": "Malaysia"}},
#     {"region": "Asia", "city": "Tokyo", "country": "Japan", "neighborhood": "Shinjuku & Roppongi", "hotel_cluster": "Hilton Tokyo / ANA InterContinental"},
#     {"region": "Asia", "city": "Manila", "country": "Philippines"},
#     {"region": "Asia", "city": "Seoul", "country": "Korea, Republic of", "neighborhood": "Gangnam Business District", "hotel_cluster": "Grand InterContinental / Josun Palace"},
#     {"region": "Asia", "city": "Busan", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Jeju Island", "country": "Korea, Republic of"},
#     {"region": "Asia", "city": "Incheon", "country": "Korea, Republic of"},
#     {"region": "Others", "city": "Sydney", "country": "Australia"},
#     {"region": "Others", "city": "Rosario", "country": "Argentina"},
#     {"region": "Others", "city": "Marrakech", "country": "Morocco"},
#     {"region": "Others", "city": "Rio de Janeiro", "country": "Brazil"},
# ]


# def normalize_target_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
#     """ëŒ€ìƒ ë„ì‹œ í•­ëª©ì— ê¸°ë³¸ê°’ì„ ì±„ì›Œ ë„£ëŠ”ë‹¤."""
#     entry = dict(entry)
#     entry.setdefault("region", "Others")
#     entry.setdefault("neighborhood", "")
#     entry.setdefault("hotel_cluster", "")
#     entry["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return entry


# def load_target_city_entries() -> List[Dict[str, Any]]:
#     if not os.path.exists(TARGET_CONFIG_FILE):
#         save_target_city_entries(DEFAULT_TARGET_CITY_ENTRIES)
#     try:
#         with open(TARGET_CONFIG_FILE, "r", encoding="utf-8") as handle:
#             data = json.load(handle)
#         if not isinstance(data, list):
#             raise ValueError("Invalid target city config format")
#     except Exception:
#         data = DEFAULT_TARGET_CITY_ENTRIES
#     return [normalize_target_entry(item) for item in data]


# def save_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     normalized = [normalize_target_entry(item) for item in entries]
#     with open(TARGET_CONFIG_FILE, "w", encoding="utf-8") as handle:
#         json.dump(normalized, handle, ensure_ascii=False, indent=2)


# TARGET_CITIES_ENTRIES = load_target_city_entries()


# def get_target_city_entries() -> List[Dict[str, Any]]:
#     if "target_cities_entries" in st.session_state:
#         return st.session_state["target_cities_entries"]
#     return TARGET_CITIES_ENTRIES


# def set_target_city_entries(entries: List[Dict[str, Any]]) -> None:
#     st.session_state["target_cities_entries"] = [normalize_target_entry(item) for item in entries]
#     save_target_city_entries(st.session_state["target_cities_entries"])


# def get_target_cities_grouped(entries: Optional[List[Dict[str, Any]]] = None) -> Dict[str, List[Dict[str, Any]]]:
#     entries = entries or get_target_city_entries()
#     grouped: Dict[str, List[Dict[str, Any]]] = {}
#     for entry in entries:
#         grouped.setdefault(entry.get("region", "Others"), []).append(entry)
#     return grouped


# def get_all_target_cities(entries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
#     entries = entries or get_target_city_entries()
#     return [normalize_target_entry(entry) for entry in entries]

# # ë„ì‹œ ì´ë¦„ ë³„ì¹­ ë§¤í•‘
# CITY_ALIASES = {
#     "jeju island": "cheju island", "busan": "pusan", "incheon": "incheon", "marrakech": "marrakesh",
#     "san remo": "san remo", "karlovy vary": "karlovy vary", "lisbon": "lisbon", "valletta": "malta island",
#     "kuala lumpur": "kuala lumpur"
# }

# # --- ë„ì‹œ ë©”íƒ€ë°ì´í„° ë° ì‹œì¦Œ ì„¤ì • ---

# SEASON_BANDS = [
#     {"months": (12, 1, 2), "label": "Peak-Holiday", "factor": 1.06},
#     {"months": (3, 4, 5), "label": "Spring-Shoulder", "factor": 1.02},
#     {"months": (6, 7, 8), "label": "Summer-Peak", "factor": 1.05},
#     {"months": (9, 10, 11), "label": "Autumn-Business", "factor": 1.03},
# ]

# CITY_SEASON_OVERRIDES: Dict[tuple, List[Dict[str, Any]]] = {
#     ("las vegas", "usa"): [
#         {"months": (1, 2), "label": "Winter Convention Peak", "factor": 1.07},
#         {"months": (6, 7, 8), "label": "Summer Off-Peak", "factor": 0.96},
#     ],
#     ("seoul", "korea, republic of"): [
#         {"months": (4, 5, 10), "label": "Cherry Blossom & Fall Peak", "factor": 1.05},
#         {"months": (1, 2), "label": "Winter Off-Peak", "factor": 0.97},
#     ],
#     ("barcelona", "spain"): [
#         {"months": (6, 7, 8), "label": "Summer Tourism Peak", "factor": 1.08},
#     ],
# }


# def get_city_context(city: str, country: str) -> Dict[str, Optional[str]]:
#     key = (city.lower(), country.lower())
#     for entry in get_target_city_entries():
#         if entry["city"].lower() == key[0] and entry["country"].lower() == key[1]:
#             return {
#                 "neighborhood": entry.get("neighborhood"),
#                 "hotel_cluster": entry.get("hotel_cluster"),
#             }
#     return {"neighborhood": None, "hotel_cluster": None}


# def get_current_season_info(city: str, country: str) -> Dict[str, Any]:
#     """í•´ë‹¹ ì›”ê³¼ ë„ì‹œ ì„¤ì •ì— ë”°ë¼ ê³„ì ˆ ë¼ë²¨ê³¼ ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
#     month = datetime.now().month
#     city_key = (city.lower(), country.lower())
#     overrides = CITY_SEASON_OVERRIDES.get(city_key, [])
#     for override in overrides:
#         if month in override["months"]:
#             return {
#                 "label": override["label"],
#                 "factor": override["factor"],
#                 "source": "city_override",
#             }

#     for band in SEASON_BANDS:
#         if month in band["months"]:
#             return {
#                 "label": band["label"],
#                 "factor": band["factor"],
#                 "source": "global_profile",
#             }

#     return {"label": "Standard", "factor": 1.0, "source": "default"}


# def aggregate_ai_totals(totals: List[int]) -> Dict[str, Any]:
#     """ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  í‰ê· ê°’ì„ ê³„ì‚°í•´ íˆ¬ëª…í•˜ê²Œ ì œê³µí•œë‹¤."""
#     if not totals:
#         return {"used_values": [], "removed_values": [], "mean": None}

#     sorted_totals = sorted(totals)
#     if len(sorted_totals) >= 4:
#         try:
#             q1, _, q3 = quantiles(sorted_totals, n=4, method="inclusive")
#             iqr = q3 - q1
#             lower_bound = q1 - 1.5 * iqr
#             upper_bound = q3 + 1.5 * iqr
#             filtered = [v for v in sorted_totals if lower_bound <= v <= upper_bound]
#         except (ValueError, StatisticsError):  # type: ignore[name-defined]
#             filtered = sorted_totals
#     else:
#         filtered = sorted_totals

#     if not filtered:
#         filtered = sorted_totals

#     removed_values: List[int] = []
#     filtered_counter = Counter(filtered)
#     for value in sorted_totals:
#         if filtered_counter[value]:
#             filtered_counter[value] -= 1
#         else:
#             removed_values.append(value)

#     computed_mean = mean(filtered) if filtered else None
#     return {
#         "used_values": filtered,
#         "removed_values": removed_values,
#         "mean_raw": computed_mean,
#         "mean": round(computed_mean) if computed_mean is not None else None,
#     }

# # --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

# def parse_pdf_to_text(uploaded_file):
#     uploaded_file.seek(0)
#     doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     full_text = ""
#     for page_num in range(4, len(doc)):
#         full_text += doc[page_num].get_text("text") + "\n\n"
#     return full_text

# def get_history_files():
#     if not os.path.exists(DATA_DIR):
#         return []
#     files = [f for f in os.listdir(DATA_DIR) if f.startswith("report_") and f.endswith(".json")]
#     return sorted(files, reverse=True)

# def save_report_data(data):
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = os.path.join(DATA_DIR, f"report_{timestamp}.json")
#     with open(filename, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)


# def _sanitize_report_data(data: Dict[str, Any]) -> Dict[str, Any]:
#     if not isinstance(data, dict):
#         return data
#     cities = data.get("cities")
#     if isinstance(cities, list):
#         for city in cities:
#             if isinstance(city, dict):
#                 city["trip_lengths"] = DEFAULT_TRIP_LENGTH.copy()
#     return data


# def load_report_data(filename):
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             try:
#                 data = json.load(f)
#                 return _sanitize_report_data(data)
#             except json.JSONDecodeError: return None
#     return None

# def build_tsv_conversion_prompt():
#     return """
# [Task]
# Convert noisy UN-DSA PDF text snippets into a clean TSV (Tab-Separated Values) table.
# [Guidelines]
# 1. Identify the country (Country) and the area/city (Area) entries inside the extracted text.
# 2. If a country header (for example "USA (US Dollar)") appears once and multiple areas follow, repeat the same country name for every subsequent row until a new country header is encountered.
# 3. Keep only four columns: `Country`, `Area`, `First 60 Days US$`, `Room as % of DSA`. Discard every other column.
# [Output Format]
# Return only the TSV content (one header row plus data rows) with tab separators, no explanations.
# Country	Area	First 60 Days US$	Room as % of DSA
# USA (US Dollar)	Washington D.C.	403	57
# """


# def call_openai_for_tsv_conversion(pdf_chunk, api_key):
#     client = openai.OpenAI(api_key=api_key)
#     system_prompt = build_tsv_conversion_prompt()
#     user_prompt = f"Here is a chunk of text extracted from a UN-DSA PDF. Convert it into TSV following the instructions.\n\n---\n\n{pdf_chunk}"
#     try:
#         response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0)
#         tsv_content = response.choices[0].message.content
#         if "```" in tsv_content:
#             tsv_content = tsv_content.split('```')[1].strip()
#             if tsv_content.startswith('tsv'): tsv_content = tsv_content[3:].strip()
#         return tsv_content
#     except Exception as e:
#         st.error(f"OpenAI API request failed: {e}")
#         return None

# def process_tsv_data(tsv_content):
#     try:
#         df = pd.read_csv(io.StringIO(tsv_content), sep='\t', on_bad_lines='skip', header=0)
#         df['Country'] = df['Country'].ffill()
#         df.rename(columns={'First 60 Days US$': 'TotalDSA', 'Room as % of DSA': 'RoomPct'}, inplace=True)
#         df = df[['Country', 'Area', 'TotalDSA', 'RoomPct']]
#         df['TotalDSA'] = pd.to_numeric(df['TotalDSA'], errors='coerce')
#         df['RoomPct'] = pd.to_numeric(df['RoomPct'], errors='coerce')
#         df.dropna(subset=['TotalDSA', 'RoomPct', 'Country', 'Area'], inplace=True)
#         df = df.astype({'TotalDSA': int, 'RoomPct': int})
#     except Exception as e:
#         st.error(f"TSV processing error: {e}")
#         return None

#     all_target_cities = get_all_target_cities()
#     final_cities_data = []
#     for target in all_target_cities:
#         city_data = {
#             "city": target["city"],
#             "country_display": target["country"],
#             "notes": "",
#             "neighborhood": target.get("neighborhood"),
#             "hotel_cluster": target.get("hotel_cluster"),
#             "trip_lengths": DEFAULT_TRIP_LENGTH.copy(),
#         }
#         found_row = None
#         search_target = target
#         is_substitute = "un_dsa_substitute" in target
#         if is_substitute: search_target = target["un_dsa_substitute"]
        
#         country_df = df[df['Country'].str.contains(search_target['country'], case=False, na=False)]
#         if not country_df.empty:
#             target_city_lower = search_target["city"].lower()
#             target_alias = CITY_ALIASES.get(target_city_lower, target_city_lower)
#             exact_match = country_df[country_df['Area'].str.lower().str.contains(target_alias, na=False)]
#             non_special_rate = exact_match[~exact_match['Area'].str.contains(r'\(', na=False)]
#             if not non_special_rate.empty:
#                 found_row = non_special_rate.iloc[0]
#                 city_data["notes"] = "Exact city match"
#             elif not exact_match.empty:
#                 found_row = exact_match.iloc[0]
#                 city_data["notes"] = "Exact city match (special rate possible)"
#             if found_row is None:
#                 elsewhere_match = country_df[country_df['Area'].str.lower().str.contains('elsewhere|all areas', na=False, regex=True)]
#                 if not elsewhere_match.empty:
#                     found_row = elsewhere_match.iloc[0]
#                     city_data["notes"] = "Applied 'Elsewhere' or 'All Areas' rate"
        
#         if is_substitute and found_row is not None:
#             city_data["notes"] = f"UN-DSA substitute city: {search_target['city']}"
#         if found_row is not None:
#             total_dsa, room_pct = found_row['TotalDSA'], found_row['RoomPct']
#             if 0 < total_dsa and 0 <= room_pct <= 100:
#                 per_diem = round(total_dsa * (1 - room_pct / 100))
#                 city_data["un"] = {"source_row": {"Country": found_row['Country'], "Area": found_row['Area']}, "total_dsa": int(total_dsa), "room_pct": int(room_pct), "per_diem_excl_lodging": per_diem, "status": "ok"}
#             else: city_data["un"] = {"status": "not_found"}
#         else:
#             city_data["un"] = {"status": "not_found"}
#             if not is_substitute: city_data["notes"] = "Could not find matching city in UN-DSA table"
#         city_data["season_context"] = get_current_season_info(city_data["city"], city_data["country_display"])
#         final_cities_data.append(city_data)
#     return {"as_of": datetime.now().strftime("%Y-%m-%d"), "currency": "USD", "cities": final_cities_data}

# def get_market_data_from_ai(
#     city: str,
#     country: str,
#     api_key: str,
#     source_name: str = "",
#     context: Optional[Dict[str, Optional[str]]] = None,
#     season_context: Optional[Dict[str, Any]] = None,
#     menu_samples: Optional[List[Dict[str, Any]]] = None,
# ) -> Dict[str, Any]:
#     """AI ëª¨ë¸ì„ í˜¸ì¶œí•´ ì¼ì¼ ì²´ë¥˜ë¹„ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤."""
#     client = openai.OpenAI(api_key=api_key)
#     context = context or {}
#     season_context = season_context or {}
#     menu_samples = menu_samples or []

#     request_id = random.randint(10000, 99999)
#     called_at = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

#     def _build_location_block() -> str:
#         lines: List[str] = []
#         if context.get("neighborhood"):
#             lines.append(f"- Primary neighborhood of stay: {context['neighborhood']}")
#         if context.get("hotel_cluster"):
#             lines.append(f"- Typical hotel cluster: {context['hotel_cluster']}")
#         return "\n".join(lines) if lines else "- No specific neighborhood context provided; rely on city-wide business areas."

#     def _build_menu_block() -> str:
#         if not menu_samples:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         snippets = []
#         for sample in menu_samples[:5]:
#             vendor = sample.get("vendor") or sample.get("name") or "Venue"
#             category = sample.get("category") or "General"
#             price = sample.get("price")
#             currency = sample.get("currency", "USD")
#             last_updated = sample.get("last_updated")
#             if price is None:
#                 continue
#             tail = f" (last updated {last_updated})" if last_updated else ""
#             snippets.append(f"- {vendor} ({category}): {currency} {price}{tail}")
#         if not snippets:
#             return "- No direct venue menu data available; use standard mid-range venues."
#         return "Menu price signals:\n" + "\n".join(snippets)

#     location_block = _build_location_block()
#     menu_block = _build_menu_block()
#     season_label = season_context.get("label", "Standard")
#     season_factor = season_context.get("factor", 1.0)
#     season_source = season_context.get("source", "global_profile")

#     prompt = f"""
# You are a corporate travel cost analyst. Request ID: {request_id}.
# Location context:
# {location_block}
# Season context: {season_label} (target multiplier {season_factor}) - source: {season_source}.
# {menu_block}

# For the city of {city}, {country}, provide a realistic, estimated daily cost of living for a business traveler in USD.
# Your response MUST be a JSON object with the following structure and nothing else. Do not add any explanation.

# IMPORTANT: If precise local data for {city} is unavailable, provide a reasonable estimate based on the national or regional average for {country}. It is crucial to provide a numerical estimate rather than returning null for all values.
# Interview insights to respect: breakfast is a simple meal with coffee, lunch is usually at a franchise or the hotel restaurant, dinner is at a local or franchise restaurant with tips included, daily transport is typically one 8km taxi ride mainly for evening meals, and miscellaneous costs cover water, drinks, snacks, toiletries, over-the-counter medicine, and laundry or hair grooming services (hotel laundry for short stays).

# {{
#   "food": {{
#     "description": "Average cost covering a simple breakfast with coffee, a franchise or hotel lunch, and a local or franchise dinner with tips included.",
#     "value": <integer>
#   }},
#   "transport": {{
#     "description": "Estimated cost for one 8km taxi ride used mainly for the evening meal commute, including tip.",
#     "value": <integer>
#   }},
#   "misc": {{
#     "description": "Estimated daily spend on essentials (water, drinks, snacks, toiletries), over-the-counter medication, and laundry or hair grooming services (hotel laundry for short stays).",
#     "value": <integer>
#   }}
# }}
# """

#     try:
#         response = client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": "You are an expert cost-of-living data analyst. You provide data only in the requested JSON format."},
#                 {"role": "user", "content": prompt},
#             ],
#             response_format={"type": "json_object"},
#             temperature=0.4,
#         )
#         raw_content = response.choices[0].message.content
#         data = json.loads(raw_content)

#         food = data.get("food", {}).get("value")
#         transport = data.get("transport", {}).get("value")
#         misc = data.get("misc", {}).get("value")

#         food_val = food if isinstance(food, int) else 0
#         transport_val = transport if isinstance(transport, int) else 0
#         misc_val = misc if isinstance(misc, int) else 0

#         meta = {
#             "source_name": source_name,
#             "request_id": request_id,
#             "prompt": prompt.strip(),
#             "response_raw": raw_content,
#             "called_at": called_at,
#             "season_context": season_context,
#             "location_context": context,
#             "menu_samples_used": menu_samples[:5],
#         }

#         if food_val == 0 and transport_val == 0 and misc_val == 0:
#             return {
#                 "status": "na",
#                 "notes": f"{source_name}: AIê°€ ìœ íš¨í•œ ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#                 "meta": meta,
#             }

#         total = food_val + transport_val + misc_val
#         notes = f"ì´ì•¡ ${total} (Food ${food_val}, Transport ${transport_val}, Misc ${misc_val})"
#         return {
#             "food": food_val,
#             "transport": transport_val,
#             "misc": misc_val,
#             "total": total,
#             "status": "ok",
#             "notes": notes,
#             "meta": meta,
#         }

#     except Exception as e:
#         return {
#             "status": "na",
#             "notes": f"{source_name} AI data extraction failed: {e}",
#             "meta": {
#                 "source_name": source_name,
#                 "request_id": request_id,
#                 "prompt": prompt.strip(),
#                 "called_at": called_at,
#                 "season_context": season_context,
#                 "location_context": context,
#                 "menu_samples_used": menu_samples[:5],
#                 "error": str(e),
#             },
#         }


# def generate_markdown_report(report_data):
#     md = f"# Business Travel Daily Allowance Report\n\n"
#     md += f"**As of:** {report_data.get('as_of', 'N/A')}\n\n"
#     weights_cfg = load_weight_config()
#     md += f"**Weight mix:** UN {weights_cfg.get('un_weight', 0.5):.0%} / AI {weights_cfg.get('ai_weight', 0.5):.0%}\n\n"

#     valid_allowances = [c['final_allowance'] for c in report_data['cities'] if c.get('final_allowance') is not None]
#     if valid_allowances:
#         md += "## 1. Summary\n\n"
#         md += (
#             f"- Recommended range: ${min(valid_allowances)} ~ ${max(valid_allowances)}\n"
#             f"- Average recommended allowance: ${round(sum(valid_allowances) / len(valid_allowances))}\n\n"
#         )

#     md += "## 2. City Details\n\n"
#     table_data = []
#     all_reference_links: Set[str] = set()
#     all_target_cities = get_all_target_cities()
#     report_cities_map = {(c.get('city', '').lower(), c.get('country_display', '').lower()): c for c in report_data.get('cities', [])}
#     for target in all_target_cities:
#         city_data = report_cities_map.get((target['city'].lower(), target['country'].lower()))
#         if city_data:
#             un_data = city_data.get('un', {})
#             ai_summary = city_data.get('ai_summary', {})
#             season_context = city_data.get('season_context', {})

#             un_val = f"$ {un_data.get('per_diem_excl_lodging')}" if un_data.get('status') == 'ok' else "N/A"
#             final_val = f"$ {city_data.get('final_allowance')}" if city_data.get('final_allowance') is not None else "N/A"
#             delta = f"{city_data.get('delta_vs_un_pct')}%" if city_data.get('delta_vs_un_pct') != 'N/A' else 'N/A'
#             ai_season_avg = ai_summary.get('season_adjusted_mean_rounded')
#             ai_runs_used = ai_summary.get('successful_runs', 0)
#             ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#             removed_totals = ai_summary.get('removed_totals') or []
#             reference_links = city_data.get('reference_links') or ai_summary.get('reference_links') or []
#             for link in reference_links:
#                 if isinstance(link, str) and link.strip():
#                     all_reference_links.add(link.strip())

#             row = {
#                 'City': city_data.get('city', 'N/A'),
#                 'Country': city_data.get('country_display', 'N/A'),
#                 'UN-DSA (1 day)': un_val,
#                 'AI (season adjusted)': f"$ {ai_season_avg}" if ai_season_avg is not None else 'N/A',
#                 'AI runs used': f"{ai_runs_used}/{ai_attempts}",
#                 'Season label': season_context.get('label', 'Standard'),
#                 'Removed outliers': ", ".join(map(str, removed_totals)) if removed_totals else '-',
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 market_data = city_data.get(f"market_data_{j}", {})
#                 md_val = f"$ {market_data.get('total')}" if market_data.get('status') == 'ok' else 'N/A'
#                 row[f"AI run {j}"] = md_val

#             row.update({
#                 'Final allowance': final_val,
#                 'Delta vs UN (%)': delta,
#                 'Trip types': ', '.join(city_data.get('trip_lengths', [])) if city_data.get('trip_lengths') else '-',
#                 'Notes': city_data.get('notes', ''),
#             })
#             table_data.append(row)

#     df = pd.DataFrame(table_data)
#     md += df.to_markdown(index=False)
#     md += "\n\n*AI provenance, prompts, and menu references are stored with each run and visible in the app detail panels.*\n\n"

#     md += (
#         "---\n"
#         "## 3. Methodology\n\n"
#         "1. **Baseline (UN-DSA)**\n"
#         "   - Extract 'Per Diem Excl. Lodging' from the official UN PDF tables.\n"
#         "   - Normalize the data as TSV to align city/country names.\n\n"
#         "2. **Market data (AI)**\n"
#         "   - Query OpenAI GPT-4o, GPT-4o-mini ten times per city with local context, hotel clusters, and season tags.\n"
#         "   - Store prompts, request IDs, season info, and menu samples with the responses.\n\n"
#         "3. **Post-processing**\n"
#         "   - Remove outliers via the IQR rule and compute averages.\n"
#         "   - Apply season factors and blend with UN-DSA using configured weights.\n"
#         "   - Multiply by grade ratios to produce per-level allowances.\n\n"
#         "---\n"
#         "## 4. Sources\n\n"
#         "- UN-DSA Circular (International Civil Service Commission)\n"
#         "- Mercer Cost of Living (2025 edition)\n"
#         "- Numbeo Cost of Living Index (2025 snapshot)\n"
#         "- Expatistan Cost of Living Guide\n"
#     )

#     return md




# # --- ìŠ¤íŠ¸ë¦¼ë¦¿ UI êµ¬ì„± ---
# st.set_page_config(layout="wide")
# st.title("AICP: ì¶œì¥ ì¼ë¹„ ê³„ì‚° & ì¡°íšŒ ì‹œìŠ¤í…œ (v14.0 - ê³ ê¸‰ ë¶„ì„ ëª¨ë¸)")

# if 'latest_analysis_result' not in st.session_state:
#     st.session_state.latest_analysis_result = None
# if 'target_cities_entries' not in st.session_state:
#     st.session_state.target_cities_entries = [normalize_target_entry(entry) for entry in TARGET_CITIES_ENTRIES]
# if 'weight_config' not in st.session_state:
#     st.session_state.weight_config = load_weight_config()
# else:
#     st.session_state.weight_config = _normalize_weight_config(st.session_state.weight_config)

# ui_settings = load_ui_settings()
# stored_employee_tab_visible = bool(ui_settings.get("show_employee_tab", True))
# if "employee_tab_visibility" not in st.session_state:
#     st.session_state.employee_tab_visibility = stored_employee_tab_visible
# employee_tab_visible = bool(st.session_state.get("employee_tab_visibility", stored_employee_tab_visible))
# section_visibility_default = _normalize_employee_sections(ui_settings.get("employee_sections"))
# if "employee_sections_visibility" not in st.session_state:
#     st.session_state.employee_sections_visibility = section_visibility_default
# else:
#     st.session_state.employee_sections_visibility = _normalize_employee_sections(st.session_state.employee_sections_visibility)
# employee_sections_visibility = st.session_state.employee_sections_visibility




# if employee_tab_visible:
#     employee_tab, admin_tab = st.tabs(["ì¼ë¹„ ì¡°íšŒ (ì§ì›ìš©)", "ë³´ê³ ì„œ ë¶„ì„ (ê´€ë¦¬ì)"])
# else:
#     (admin_tab,) = st.tabs(["ë³´ê³ ì„œ ë¶„ì„ (ê´€ë¦¬ì)"])
#     employee_tab = None

# if employee_tab is not None:
#     with employee_tab:
#         st.header("ë„ì‹œë³„ ì¶œì¥ ì¼ë¹„ ì¡°íšŒ")
#         history_files = get_history_files()
#         if not history_files:
#             st.info("ë¨¼ì € 'ë³´ê³ ì„œ ë¶„ì„' íƒ­ì—ì„œ PDFë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.")
#         else:
#             if "selected_report_file" not in st.session_state:
#                 st.session_state["selected_report_file"] = history_files[0]
#             if st.session_state["selected_report_file"] not in history_files:
#                 st.session_state["selected_report_file"] = history_files[0]
#             selected_file = st.session_state["selected_report_file"]
#             report_data = load_report_data(selected_file)
#             if report_data and 'cities' in report_data and report_data['cities']:
#                 cities_df = pd.DataFrame(report_data['cities'])
#                 target_entries = get_target_city_entries()
#                 countries = sorted({entry['country'] for entry in target_entries})

                
#                 col_country, col_city = st.columns(2)
#                 with col_country:
#                     selectable_countries = [c for c in countries if c in cities_df['country_display'].unique()]
#                     sel_country = st.selectbox("êµ­ê°€:", selectable_countries, key=f"country_{selected_file}")
#                 filtered_cities_all = sorted({
#                     entry['city'] for entry in target_entries if entry['country'] == sel_country
#                 })
#                 with col_city:
#                     if filtered_cities_all:
#                         sel_city = st.selectbox("ë„ì‹œ:", filtered_cities_all, key=f"city_{selected_file}")
#                     else:
#                         sel_city = None
#                         st.warning("ì„ íƒí•œ êµ­ê°€ì— ë“±ë¡ëœ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

#                 col_start, col_end, col_level = st.columns([1, 1, 1])
#                 with col_start:
#                     trip_start = st.date_input(
#                         "ì¶œì¥ ì‹œì‘ì¼",
#                         value=datetime.today().date(),
#                         key=f"trip_start_{selected_file}",
#                     )
#                 with col_end:
#                     trip_end = st.date_input(
#                         "ì¶œì¥ ì¢…ë£Œì¼",
#                         value=datetime.today().date() + timedelta(days=4),
#                         key=f"trip_end_{selected_file}",
#                     )
#                 with col_level:
#                     sel_level = st.selectbox("ì§ê¸‰:", list(JOB_LEVEL_RATIOS.keys()), key=f"l_{selected_file}")

#                 if isinstance(trip_start, datetime):
#                     trip_start = trip_start.date()
#                 if isinstance(trip_end, datetime):
#                     trip_end = trip_end.date()

#                 trip_valid = trip_end >= trip_start
#                 if not trip_valid:
#                     st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
#                     trip_days = None
#                     trip_term = "Short-term"
#                     trip_multiplier = SHORT_TERM_MULTIPLIER
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                 else:
#                     trip_days = (trip_end - trip_start).days + 1
#                     trip_term, trip_multiplier = classify_trip_duration(trip_days)
#                     trip_term_label = TRIP_TERM_LABELS.get(trip_term, trip_term)
#                     st.caption(f"ìë™ ë¶„ë¥˜ëœ ì¶œì¥ ìœ í˜•: {trip_term_label} Â· {trip_days}ì¼ ì¼ì •")

#                 if sel_city:
#                     filtered_trip_cities = []
#                     for entry in target_entries:
#                         if entry['country'] != sel_country or entry['city'] != sel_city:
#                             continue
#                         if trip_valid and trip_term not in entry.get('trip_lengths', TRIP_LENGTH_OPTIONS):
#                             continue
#                         filtered_trip_cities.append(entry['city'])
#                     if trip_valid and not filtered_trip_cities:
#                         st.warning("ì´ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë„ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶œì¥ ìœ í˜•ì„ 'ìˆí…€'ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ë„ì‹œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
#                         sel_city = None

#                 if trip_valid and sel_city and sel_level:
#                     city_data = cities_df[cities_df['city'] == sel_city].iloc[0].to_dict()
#                     final_allowance = city_data.get('final_allowance')
#                     st.subheader(f"{sel_country} - {sel_city} ê²°ê³¼")
#                     if final_allowance:
#                         level_ratio = JOB_LEVEL_RATIOS[sel_level]
#                         adjusted_daily_allowance = round(final_allowance * trip_multiplier)
#                         level_daily_allowance = round(adjusted_daily_allowance * level_ratio)
#                         trip_total_allowance = level_daily_allowance * trip_days
#                         st.markdown(
#                             f"""
#                             <div style='margin-top:0.8rem;padding:1.8rem;border-radius:18px;background:linear-gradient(135deg,#1e3c72 0%,#2a5298 100%);color:#fff;box-shadow:0 12px 28px rgba(30,60,114,0.35);text-align:center;'>
#                                 <div style='font-size:2.4rem;font-weight:800;letter-spacing:0.02em;'>
#                                     {sel_level.split(' ')[0]} ê¸°ì¤€ ì˜ˆìƒ ì¼ë¹„ ì´ì•¡ $ {trip_total_allowance:,} = $ {level_daily_allowance:,} Ã— {trip_days}ì¼ ì¼ì • Ã— {trip_term_label}(Ã—{trip_multiplier:.2f})
#                                 </div>
#                             </div>
#                             """,
#                             unsafe_allow_html=True,
#                         )
#                     else:
#                         st.metric(f"{sel_level.split(' ')[0]} ì¼ì¼ ê¶Œì¥ ì¼ë¹„", "ê¸ˆì•¡ ì—†ìŒ")

#                     #st.markdown("---")
#                     #st.write("**ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±° (ì¼ë¹„ ê¸°ì¤€)**")

#                     menu_samples = city_data.get('menu_samples') or []

#                     detail_cards_visible = any([
#                         employee_sections_visibility["show_un_basis"],
#                         employee_sections_visibility["show_ai_estimate"],
#                         employee_sections_visibility["show_weighted_result"],
#                         employee_sections_visibility["show_ai_market_detail"],
#                     ])
#                     extra_content_visible = (
#                         employee_sections_visibility["show_provenance"]
#                         or (employee_sections_visibility["show_menu_samples"] and menu_samples)
#                     )

#                     if detail_cards_visible or extra_content_visible:
#                         st.markdown("---")
#                         st.write("**ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±° (ì¼ë¹„ ê¸°ì¤€)**")
#                         un_data = city_data.get('un', {})
#                         ai_summary = city_data.get('ai_summary', {})
#                         season_context = city_data.get('season_context', {})

#                         ai_avg = ai_summary.get('season_adjusted_mean_rounded')
#                         ai_runs = ai_summary.get('successful_runs', len(ai_summary.get('used_totals', [])))
#                         ai_attempts = ai_summary.get('attempted_runs', NUM_AI_CALLS)
#                         removed_totals = ai_summary.get('removed_totals') or []
#                         season_label = season_context.get('label') or ai_summary.get('season_label', 'Standard')
#                         season_factor = season_context.get('factor', ai_summary.get('season_factor', 1.0))

#                         ai_notes_parts = [f"ì„±ê³µ {ai_runs}/{ai_attempts}íšŒ"]
#                         if removed_totals:
#                             ai_notes_parts.append(f"ì œì™¸ê°’ {removed_totals}")
#                         if season_label:
#                             ai_notes_parts.append(f"ì‹œì¦Œ {season_label} Ã—{season_factor}")
#                         ai_notes = " | ".join(ai_notes_parts) if ai_notes_parts else "AI ë°ì´í„° ì—†ìŒ"

#                         weights_cfg = get_weight_config()

#                         un_base = None
#                         un_display = None
#                         if un_data.get('status') == 'ok' and isinstance(un_data.get('per_diem_excl_lodging'), (int, float)):
#                             un_base = un_data['per_diem_excl_lodging']
#                             un_display = round(un_base * trip_multiplier)

#                         ai_display = round(ai_avg * trip_multiplier) if ai_avg is not None else None
#                         weighted_display = round(final_allowance * trip_multiplier) if final_allowance is not None else None

#                         first_row_keys = []
#                         if employee_sections_visibility["show_un_basis"]:
#                             first_row_keys.append("un")
#                         if employee_sections_visibility["show_ai_estimate"]:
#                             first_row_keys.append("ai")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             first_row_keys.append("weighted")

#                         if first_row_keys:
#                             first_row_cols = st.columns(len(first_row_keys))
#                             for key, col in zip(first_row_keys, first_row_cols):
#                                 with col:
#                                     if key == "un":
#                                         st.info("UN-DSA ê¸°ì¤€")
#                                         if un_display is not None:
#                                             st.metric("ì¼ë¹„", f"$ {un_display:,}")
#                                             if trip_term == "Long-term":
#                                                 st.caption(f"ìˆí…€ ê¸°ì¤€ $ {un_base:,} â†’ ë¡±í…€ $ {un_display:,}")
#                                             else:
#                                                 st.caption(f"ìˆí…€ ê¸°ì¤€ $ {un_base:,}")
#                                         else:
#                                             st.metric("ì¼ë¹„", "N/A")
#                                             st.caption(city_data.get("notes", ""))
#                                     elif key == "ai":
#                                         st.info("AI ì‹œì¥ ì¶”ì • (ì‹œì¦Œ ë³´ì •)")
#                                         if ai_display is not None:
#                                             st.metric("ì¼ë¹„", f"$ {ai_display:,}")
#                                             caption_parts = [ai_notes]
#                                             caption_parts.append(f"ìˆí…€ ê¸°ì¤€ $ {ai_avg:,}")
#                                             if trip_term == "Long-term":
#                                                 caption_parts.append(f"ë¡±í…€ ì ìš© $ {ai_display:,}")
#                                             st.caption(" | ".join([part for part in caption_parts if part]))
#                                         else:
#                                             st.metric("ì¼ë¹„", "N/A")
#                                             st.caption(ai_notes)
#                                     else:
#                                         st.info("ê°€ì¤‘ í‰ê·  ê²°ê³¼")
#                                         if weighted_display is not None:
#                                             st.metric("ì¼ë¹„", f"$ {weighted_display:,}")
#                                             caption_parts = [
#                                                 f"Blend of UN-DSA ({weights_cfg['un_weight']:.0%}) and AI estimate ({weights_cfg['ai_weight']:.0%})",
#                                                 f"ìˆí…€ ê¸°ì¤€ $ {final_allowance:,}",
#                                             ]
#                                             if trip_term == "Long-term":
#                                                 caption_parts.append(f"ë¡±í…€ ì ìš© $ {weighted_display:,}")
#                                             st.caption(" | ".join(caption_parts))
#                                         else:
#                                             st.metric("ì¼ë¹„", "N/A")
#                                             st.caption(city_data.get("notes", ""))

#                         second_row_keys = []
#                         if employee_sections_visibility["show_ai_market_detail"]:
#                             second_row_keys.append("ai_market")
#                         if employee_sections_visibility["show_weighted_result"]:
#                             second_row_keys.append("weighted_detail")

#                         if second_row_keys:
#                             second_row_cols = st.columns(len(second_row_keys))
#                             for key, col in zip(second_row_keys, second_row_cols):
#                                 with col:
#                                     if key == "ai_market":
#                                         st.info("AI Market Estimate (Season-Adjusted)")
#                                         if ai_display is not None:
#                                             st.metric("Daily Allowance", f"$ {ai_display:,}")
#                                             st.caption(ai_notes)
#                                         else:
#                                             st.metric("Daily Allowance", "N/A")
#                                             st.caption(ai_notes)
#                                     else:
#                                         st.info("Weighted Combined Result")
#                                         if weighted_display is not None:
#                                             st.metric("Daily Allowance", f"$ {weighted_display:,}")
#                                             st.caption(f"Blend of UN-DSA ({weights_cfg['un_weight']:.0%}) and AI estimate ({weights_cfg['ai_weight']:.0%})")
#                                         else:
#                                             st.metric("Daily Allowance", "N/A")

#                         if employee_sections_visibility["show_provenance"]:
#                             with st.expander("AI provenance & prompts"):
#                                 provenance_payload = {
#                                     "season_context": season_context,
#                                     "ai_summary": ai_summary,
#                                     "ai_runs": city_data.get('ai_provenance', []),
#                                     "reference_links": build_reference_link_lines(menu_samples, max_items=8),
#                                     "weights": weights_cfg,
#                                 }
#                                 st.json(provenance_payload)

#                         if employee_sections_visibility["show_menu_samples"] and menu_samples:
#                             with st.expander("Reference menu samples"):
#                                 link_lines = build_reference_link_lines(menu_samples, max_items=8)
#                                 if link_lines:
#                                     st.markdown("**Direct links**")
#                                     for link_line in link_lines:
#                                         st.markdown(f"- {link_line}")
#                                     st.markdown("---")
#                                 st.table(pd.DataFrame(menu_samples))
#                     else:
#                         st.info("ê´€ë¦¬ìê°€ ì„¸ë¶€ ì‚°ì¶œ ê·¼ê±°ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")

# ACCESS_CODE_KEY = "admin_access_code_valid"
# ACCESS_CODE_VALUE = "VUCA0207"

# with admin_tab:
#     if not st.session_state.get(ACCESS_CODE_KEY, False):
#         with st.form("admin_access_form"):
#             input_code = st.text_input("Access Code", type="password")
#             submitted = st.form_submit_button("Enter")
#         if submitted:
#             if input_code == ACCESS_CODE_VALUE:
#                 st.session_state[ACCESS_CODE_KEY] = True
#                 st.success("Access granted.")
#             else:
#                 st.error("Access Codeê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#         st.stop()

#     visibility_toggle = st.toggle("ì§ì›ìš© íƒ­ ë…¸ì¶œ", value=employee_tab_visible, key="employee_tab_visibility")
#     if visibility_toggle != stored_employee_tab_visible:
#         updated_settings = dict(ui_settings)
#         updated_settings["show_employee_tab"] = visibility_toggle
#         updated_settings["employee_sections"] = employee_sections_visibility
#         save_ui_settings(updated_settings)
#         ui_settings = updated_settings
#         st.success("ì§ì›ìš© íƒ­ ë…¸ì¶œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

#     st.subheader("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •")
#     section_toggle_values: Dict[str, bool] = {}
#     for section_key, label in EMPLOYEE_SECTION_LABELS:
#         current_value = employee_sections_visibility.get(section_key, EMPLOYEE_SECTION_DEFAULTS[section_key])
#         section_toggle_values[section_key] = st.toggle(
#             label,
#             value=current_value,
#             key=f"employee_section_toggle_{section_key}",
#         )
#     if section_toggle_values != employee_sections_visibility:
#         updated_settings = dict(ui_settings)
#         updated_settings["employee_sections"] = section_toggle_values
#         save_ui_settings(updated_settings)
#         ui_settings["employee_sections"] = section_toggle_values
#         st.session_state.employee_sections_visibility = section_toggle_values
#         employee_sections_visibility = section_toggle_values
#         st.success("ì§ì› í™”ë©´ ë…¸ì¶œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

#     st.subheader("ë³´ê³ ì„œ ë²„ì „ ê´€ë¦¬")
#     history_files = get_history_files()
#     if history_files:
#         if "selected_report_file" not in st.session_state:
#             st.session_state["selected_report_file"] = history_files[0]
#         if st.session_state["selected_report_file"] not in history_files:
#             st.session_state["selected_report_file"] = history_files[0]
#         default_index = history_files.index(st.session_state["selected_report_file"])
#         selected_file = st.selectbox("ë³´ê³ ì„œ ë²„ì „ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.", history_files, index=default_index, key="admin_report_file_select")
#         st.session_state["selected_report_file"] = selected_file
#     else:
#         st.info("ì„ íƒí•˜ì‹  ë³´ê³ ì„œ ë²„ì „ì´ ì—†ìŠµë‹ˆë‹¤.")

#     st.header("ëª©í‘œ ë„ì‹œ ê´€ë¦¬")
#     entries_df = pd.DataFrame(get_target_city_entries())
#     if not entries_df.empty:
#         entries_display = entries_df.copy()
#         entries_display["trip_lengths"] = DEFAULT_TRIP_LENGTH[0]
#         st.dataframe(entries_display[["region", "country", "city", "neighborhood", "hotel_cluster", "trip_lengths"]], width="stretch")
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ í•­ëª©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

#     st.subheader("ë¹„ì¤‘ ì„¤ì •")
#     current_weights = get_weight_config()
#     st.caption(f"Current weights -> UN {current_weights.get('un_weight', 0.5):.0%} / AI {current_weights.get('ai_weight', 0.5):.0%}")
#     with st.form("weight_config_form"):
#         un_weight_input = st.slider("UN-DSA weight", min_value=0.0, max_value=1.0, value=float(current_weights.get("un_weight", 0.5)), step=0.05, format="%.2f")
#         ai_weight_preview = max(0.0, 1.0 - un_weight_input)
#         st.write(f"AI market estimate weight: **{ai_weight_preview:.2f}**")
#         st.caption("Weights are normalised to sum to 1.0 when saved.")
#         weight_submit = st.form_submit_button("Save weights")
#     if weight_submit:
#         updated = update_weight_config(un_weight_input, ai_weight_preview)
#         st.success(f"Weights saved (UN {updated['un_weight']:.2f} / AI {updated['ai_weight']:.2f})")

#     existing_regions = sorted({entry["region"] for entry in get_target_city_entries()})
#     st.subheader("ì‹ ê·œ ë„ì‹œ ì¶”ê°€")
#     with st.form("add_target_city_form", clear_on_submit=True):
#         col_a, col_b = st.columns(2)
#         with col_a:
#             region_options = existing_regions + ["ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"]
#             region_choice = st.selectbox("ì§€ì—­", region_options, key="add_region_choice")
#             new_region = ""
#             if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
#                 new_region = st.text_input("ìƒˆ ì§€ì—­ ì´ë¦„", key="add_region_text")
#         with col_b:
#             trip_lengths_selected = st.multiselect("ì¶œì¥ ê¸°ê°„", TRIP_LENGTH_OPTIONS, default=DEFAULT_TRIP_LENGTH, key="add_trip_lengths")

#         col_c, col_d = st.columns(2)
#         with col_c:
#             city_name = st.text_input("ë„ì‹œ", key="add_city")
#             neighborhood = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="add_neighborhood")
#         with col_d:
#             country_name = st.text_input("êµ­ê°€", key="add_country")
#             hotel_cluster = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="add_hotel_cluster")

#         with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#             substitute_city = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="add_sub_city")
#             substitute_country = st.text_input("ëŒ€ì²´ êµ­ê°€", key="add_sub_country")

#         add_submitted = st.form_submit_button("ì¶”ê°€")

#     if add_submitted:
#         region_value = new_region.strip() if region_choice == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else region_choice
#         if not region_value or not city_name.strip() or not country_name.strip():
#             st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#         else:
#             current_entries = get_target_city_entries()
#             canonical_key = (region_value.lower(), country_name.strip().lower(), city_name.strip().lower())
#             duplicate_exists = any(
#                 (entry.get("region", "").lower(), entry.get("country", "").lower(), entry.get("city", "").lower()) == canonical_key
#                 for entry in current_entries
#             )
#             if duplicate_exists:
#                 st.warning("ë™ì¼í•œ í•­ëª©ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
#             else:
#                 new_entry = {
#                     "region": region_value,
#                     "country": country_name.strip(),
#                     "city": city_name.strip(),
#                     "neighborhood": neighborhood.strip(),
#                     "hotel_cluster": hotel_cluster.strip(),
#                     "trip_lengths": trip_lengths_selected or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if substitute_city.strip() and substitute_country.strip():
#                     new_entry["un_dsa_substitute"] = {
#                         "city": substitute_city.strip(),
#                         "country": substitute_country.strip(),
#                     }
#                 current_entries.append(new_entry)
#                 set_target_city_entries(current_entries)
#                 st.success(f"{region_value} - {city_name.strip()} í•­ëª©ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()
#     st.subheader("ê¸°ì¡´ ë„ì‹œ í¸ì§‘/ì‚­ì œ")
#     current_entries = get_target_city_entries()
    
#     if current_entries:
#         # 1. ë“œë¡­ë‹¤ìš´ ì˜µì…˜ êµ¬ì„±
#         options = {
#             f"{entry['region']} | {entry['country']} | {entry['city']}": idx
#             for idx, entry in enumerate(current_entries)
#         }
#         sorted_labels = list(options.keys())

#         # 2. on_change ì½œë°± í•¨ìˆ˜ ì •ì˜ (ìœ„ì ¯ë³´ë‹¤ ë¨¼ì €)
#         def _sync_edit_form_from_selection():
#             # ë“œë¡­ë‹¤ìš´ì—ì„œ í˜„ì¬ ì„ íƒëœ ê°’ì„ ê°€ì ¸ì˜´
#             selected_idx = options[st.session_state.edit_city_selector]
#             selected_entry = current_entries[selected_idx]
            
#             # session_stateì˜ ê°’ì„ ì„ íƒëœ ë„ì‹œì˜ ë°ì´í„°ë¡œ ê°•ì œ ì—…ë°ì´íŠ¸
#             st.session_state.edit_region = selected_entry.get("region", "")
#             st.session_state.edit_city = selected_entry.get("city", "")
#             st.session_state.edit_neighborhood = selected_entry.get("neighborhood", "")
#             st.session_state.edit_country = selected_entry.get("country", "")
#             st.session_state.edit_hotel = selected_entry.get("hotel_cluster", "")
            
#             # ì¶œì¥ ê¸°ê°„ (trip_lengths) ì„¤ì •
#             existing_trip_lengths = [t for t in selected_entry.get("trip_lengths", []) if t in TRIP_LENGTH_OPTIONS]
#             st.session_state.edit_trip_lengths = existing_trip_lengths or DEFAULT_TRIP_LENGTH.copy()
            
#             # UN-DSA ëŒ€ì²´ ë„ì‹œ ì„¤ì •
#             sub_data = selected_entry.get("un_dsa_substitute") or {}
#             st.session_state.edit_sub_city = sub_data.get("city", "")
#             st.session_state.edit_sub_country = sub_data.get("country", "")

#         # 3. ë“œë¡­ë‹¤ìš´(Selectbox)ì— on_change ì½œë°± ì—°ê²°
#         selected_label = st.selectbox(
#             "í¸ì§‘í•  ë„ì‹œë¥¼ ì„ íƒí•˜ì„¸ìš”", 
#             sorted_labels, 
#             key="edit_city_selector",
#             on_change=_sync_edit_form_from_selection  # <-- [ìˆ˜ì •] ì½œë°± í•¨ìˆ˜ ì—°ê²°
#         )

#         # 4. í˜ì´ì§€ ì²« ë¡œë“œ ì‹œ í¼ì„ ì±„ìš°ê¸° ìœ„í•œ ì´ˆê¸°í™”
#         if "edit_region" not in st.session_state and sorted_labels:
#             # ì²« ë¡œë“œ ì‹œ, selectboxì˜ ê¸°ë³¸ê°’(ì²« ë²ˆì§¸ í•­ëª©)ì— ë§ì¶° í¼ì„ ì±„ì›€
#             _sync_edit_form_from_selection()

#         # 5. í¼ ë‚´ë¶€ ìœ„ì ¯ì—ì„œ 'value=' ì œê±°í•˜ê³  'key='ë§Œ ì‚¬ìš©
#         with st.form("edit_target_city_form"):
#             col_e, col_f = st.columns(2)
#             with col_e:
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 region_edit = st.text_input("ì§€ì—­", key="edit_region")
#                 city_edit = st.text_input("ë„ì‹œ", key="edit_city")
#                 neighborhood_edit = st.text_input("ì„¸ë¶€ ì§€ì—­ (ì„ íƒ)", key="edit_neighborhood")
#             with col_f:
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 country_edit = st.text_input("êµ­ê°€", key="edit_country")
#                 hotel_cluster_edit = st.text_input("ì¶”ì²œ í˜¸í…” í´ëŸ¬ìŠ¤í„° (ì„ íƒ)", key="edit_hotel")

#             # [ìˆ˜ì •] default=... ëŒ€ì‹  key=... ì‚¬ìš©
#             trip_lengths_edit = st.multiselect(
#                 "ì¶œì¥ ê¸°ê°„",
#                 TRIP_LENGTH_OPTIONS,
#                 key="edit_trip_lengths", # 'default' ëŒ€ì‹  'key'ë¡œ ìƒíƒœ ê´€ë¦¬
#             )

#             with st.expander("UN-DSA ëŒ€ì²´ ë„ì‹œ (ì„ íƒ)"):
#                 # [ìˆ˜ì •] value=... ì œê±°
#                 sub_city_edit = st.text_input("ëŒ€ì²´ ë„ì‹œ", key="edit_sub_city")
#                 sub_country_edit = st.text_input("ëŒ€ì²´ êµ­ê°€", key="edit_sub_country")

#             col_btn1, col_btn2 = st.columns(2)
#             with col_btn1:
#                 update_btn = st.form_submit_button("ë³€ê²½ì‚¬í•­ ì €ì¥")
#             with col_btn2:
#                 delete_btn = st.form_submit_button("ì‚­ì œ", type="secondary")

#         # 6. ì €ì¥/ì‚­ì œ ë¡œì§ì€ session_stateì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë„ë¡ ìˆ˜ì •
#         if update_btn:
#             # [ìˆ˜ì •] ìœ„ì ¯ ë³€ìˆ˜(region_edit) ëŒ€ì‹  st.session_stateì—ì„œ ì§ì ‘ ê°’ì„ ì½ìŒ
#             if (not st.session_state.edit_region.strip() or 
#                 not st.session_state.edit_city.strip() or 
#                 not st.session_state.edit_country.strip()):
#                 st.error("ì§€ì—­, êµ­ê°€, ë„ì‹œëŠ” í•„ìˆ˜ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
#             else:
#                 current_entries[options[selected_label]] = {
#                     "region": st.session_state.edit_region.strip(),
#                     "country": st.session_state.edit_country.strip(),
#                     "city": st.session_state.edit_city.strip(),
#                     "neighborhood": st.session_state.edit_neighborhood.strip(),
#                     "hotel_cluster": st.session_state.edit_hotel.strip(),
#                     "trip_lengths": st.session_state.edit_trip_lengths or DEFAULT_TRIP_LENGTH.copy(),
#                 }
#                 if st.session_state.edit_sub_city.strip() and st.session_state.edit_sub_country.strip():
#                     current_entries[options[selected_label]]["un_dsa_substitute"] = {
#                         "city": st.session_state.edit_sub_city.strip(),
#                         "country": st.session_state.edit_sub_country.strip(),
#                     }
#                 else:
#                     current_entries[options[selected_label]].pop("un_dsa_substitute", None)

#                 set_target_city_entries(current_entries)
#                 st.success("ìˆ˜ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
#                 st.rerun()  # <-- [í™•ì¸] ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë˜ì–´ ìˆìŒ
        
#         if delete_btn:
#             del current_entries[options[selected_label]]
#             set_target_city_entries(current_entries)
#             st.warning("ì„ íƒí•œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
#             st.rerun() # <-- [í™•ì¸] ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •ë˜ì–´ ìˆìŒ
#     else:
#         st.info("ë“±ë¡ëœ ëª©í‘œ ë„ì‹œê°€ ì—†ì–´ í¸ì§‘í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

#     st.divider()
#     st.subheader("UN-DSA (PDF) ë¶„ì„")
#     st.warning(f"AI í˜¸ì¶œì´ {NUM_AI_CALLS}íšŒ ì‹¤í–‰ë˜ë¯€ë¡œ ì‹œê°„ê³¼ ë¹„ìš©ì— ìœ ì˜í•´ ì£¼ì„¸ìš”.")
#     uploaded_file = st.file_uploader("UN-DSA PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
#     if uploaded_file and st.button("AI ë¶„ì„ ì‹¤í–‰", type="primary"):
#         openai_api_key = os.getenv("OPENAI_API_KEY")
#         if not openai_api_key:
#             st.error(".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
#         else:
#             st.session_state.latest_analysis_result = None
#             with st.spinner("PDFë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
#                 progress_bar = st.progress(0, text="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
#                 full_text = parse_pdf_to_text(uploaded_file)

#                 CHUNK_SIZE = 15000
#                 text_chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
#                 all_tsv_lines = []

#                 analysis_failed = False
#                 for i, chunk in enumerate(text_chunks):
#                     progress_bar.progress(i / (len(text_chunks) + 1), text=f"AI PDF->TSV ë³€í™˜ ì¤‘... ({i+1}/{len(text_chunks)})")
#                     chunk_tsv = call_openai_for_tsv_conversion(chunk, openai_api_key)
#                     if chunk_tsv:
#                         lines = chunk_tsv.strip().split('\n')
#                         if not all_tsv_lines:
#                             all_tsv_lines.extend(lines)
#                         else:
#                             all_tsv_lines.extend(lines[1:])
#                     else:
#                         analysis_failed = True
#                         break

#                 if not analysis_failed:
#                     processed_data = process_tsv_data("\n".join(all_tsv_lines))
#                     if processed_data:
#                         total_cities = len(processed_data["cities"])
#                         for i, city_data in enumerate(processed_data["cities"]):
#                             city_name, country_name = city_data["city"], city_data["country_display"]
#                             progress_text = f"AI ì¶”ì •ì¹˜ ê³„ì‚° ì¤‘... ({i+1}/{total_cities}) {city_name}"
#                             progress_bar.progress((i + 1) / max(total_cities, 1), text=progress_text)

#                             city_context = {
#                                 "neighborhood": city_data.get("neighborhood"),
#                                 "hotel_cluster": city_data.get("hotel_cluster"),
#                             }
#                             season_context = city_data.get("season_context") or get_current_season_info(city_name, country_name)
#                             menu_samples = load_cached_menu_prices(city_name, country_name, city_context.get("neighborhood"))
#                             city_data["menu_samples"] = menu_samples
#                             city_data["reference_links"] = build_reference_link_lines(menu_samples, max_items=8)
#                             ai_totals_source: List[int] = []
#                             ai_meta_runs: List[Dict[str, Any]] = []

#                             for j in range(1, NUM_AI_CALLS + 1):
#                                 source_name = f"  {j}"
#                                 market_result = get_market_data_from_ai(
#                                     city_name,
#                                     country_name,
#                                     openai_api_key,
#                                     source_name,
#                                     context=city_context,
#                                     season_context=season_context,
#                                     menu_samples=menu_samples,
#                                 )
#                                 city_data[f"market_data_{j}"] = market_result
#                                 if market_result.get("status") == 'ok' and market_result.get("total") is not None:
#                                     ai_totals_source.append(market_result["total"])
#                                 if "meta" in market_result:
#                                     ai_meta_runs.append(market_result["meta"])
#                                 if j < NUM_AI_CALLS:
#                                     time.sleep(1)

#                             city_data["ai_provenance"] = ai_meta_runs

#                             final_allowance = None
#                             un_per_diem = city_data.get("un", {}).get("per_diem_excl_lodging")

#                             ai_stats = aggregate_ai_totals(ai_totals_source)
#                             season_factor = (season_context or {}).get("factor", 1.0)
#                             ai_base_mean = ai_stats.get("mean_raw")
#                             ai_season_adjusted = ai_base_mean * season_factor if ai_base_mean is not None else None
#                             weights_cfg = get_weight_config()

#                             city_data["ai_summary"] = {
#                                 "raw_totals": ai_totals_source,
#                                 "used_totals": ai_stats.get("used_values", []),
#                                 "removed_totals": ai_stats.get("removed_values", []),
#                                 "mean_base": ai_base_mean,
#                                 "mean_base_rounded": ai_stats.get("mean"),
#                                 "season_factor": season_factor,
#                                 "season_label": (season_context or {}).get("label"),
#                                 "season_adjusted_mean_raw": ai_season_adjusted,
#                                 "season_adjusted_mean_rounded": round(ai_season_adjusted) if ai_season_adjusted is not None else None,
#                                 "successful_runs": len(ai_stats.get("used_values", [])),
#                                 "attempted_runs": NUM_AI_CALLS,
#                                 "reference_links": city_data.get("reference_links", []),
#                                 "weighted_average_components": {
#                                     "un_per_diem": un_per_diem,
#                                     "ai_season_adjusted": ai_season_adjusted,
#                                     "weights": {"UN": weights_cfg["un_weight"], "AI": weights_cfg["ai_weight"]},
#                                 },
#                             }

#                             if un_per_diem and ai_season_adjusted is not None:
#                                 weighted_average = (un_per_diem * weights_cfg["un_weight"]) + (ai_season_adjusted * weights_cfg["ai_weight"])
#                                 final_allowance = round(weighted_average)
#                             elif un_per_diem:
#                                 final_allowance = round(un_per_diem)
#                             elif ai_season_adjusted is not None:
#                                 final_allowance = round(ai_season_adjusted)

#                             city_data["final_allowance"] = final_allowance

#                             if final_allowance and un_per_diem and un_per_diem > 0:
#                                 city_data["delta_vs_un_pct"] = round(((final_allowance - un_per_diem) / un_per_diem) * 100)
#                             else:
#                                 city_data["delta_vs_un_pct"] = "N/A"

#                         save_report_data(processed_data)
#                         st.session_state.latest_analysis_result = processed_data
#                         st.success("AI analysis completed.")

#     if st.session_state.latest_analysis_result:
#         st.markdown("---")
#         st.subheader("Latest Analysis Summary")
#         df_data = []
#         for city in st.session_state.latest_analysis_result['cities']:
#             row = {
#                 'City': city.get('city', 'N/A'),
#                 'Country': city.get('country_display', 'N/A'),
#                 'UN-DSA': city.get('un', {}).get('per_diem_excl_lodging'),
#             }
#             for j in range(1, NUM_AI_CALLS + 1):
#                 row[f"AI {j}"] = city.get(f'market_data_{j}', {}).get('total')

#             row.update({
#                 'Final Allowance': city.get('final_allowance'),
#                 'Delta (%)': city.get('delta_vs_un_pct'),
#                 'Trip Lengths': DEFAULT_TRIP_LENGTH[0],
#                 'Notes': city.get('notes', ''),
#             })
#             df_data.append(row)

#         st.dataframe(pd.DataFrame(df_data))
#         with st.expander("View generated markdown report"):
#             st.markdown(generate_markdown_report(st.session_state.latest_analysis_result))
